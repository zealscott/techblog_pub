<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Analysis of Learning from Positive and Unlabeled Data | Scott&#39;Log</title>
<meta name="keywords" content="deep learning, paper" />
<meta name="description" content="本文从基本的分类损失出发，推导了PU的分类问题其实就是Cost-sensitive classiﬁcation的形式，同时，通过实验证明了如果使用凸函数作为loss function，例如hinge loss会导致错误的分类边界（有bias），因此需要使用例如ramp loss之类的凹函数。同时，论文还对先验$\pi$存在偏差的情况进行了讨论，说明了如果样本中大部分都是正样本，那么就算先验差距比较大，但对总体的分类效果没有太大影响。最后对分类边界进行讨论，证明了使用PU进行分类的误差小于监督学习误差的$2\sqrt{2}$倍。
基本概念和定义  Ordinary classification  Bayes optimal classiﬁer的目标是最小化misclassiﬁcation rate，这在Introduction to Statistical Machine Learning By Masashi Sugiyama 书里有定义，直观理解就是最小化期望错分率： $R(f) = \pi R_1 (f) &#43; (1 - \pi) R_{-1}(f)$ 这里的$R_1$表示false negative rate，也就是分错正类的概率，乘以先验正类的概率$\pi$ $R_{-1}$表示false positive rate，也就是分错负类的概率，乘以先验负类的概率$1-\pi$ 这样，对分错样本的概率分别乘以其先验概率，就是其错分概率的期望。   Cost-sensitive classiﬁcation  如果对于某种错误我们的敏感程度不一样，那么就乘以不同的权重，重新定义为： $R(f) = \pi c_1 R_1(f) &#43; (1-\pi) c_{-1}R_{-1}(f)$ 这里用$c_1$和$c_{-1}$分别表示对两种错分的代价   PU classification   定义在未标记数据集$X$ 中的分布：
  $P_X = \pi P_1 &#43; (1-\pi) P_{-1}$">
<meta name="author" content="Scott Du">
<link rel="canonical" href="https://tech.zealscott.com/deeplearning/pulearning/pu-learning-non-convex/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.fd126033c3455a688b2479431fed44510433c36ce61093336d94a5681fad5866.css" integrity="sha256-/RJgM8NFWmiLJHlDH&#43;1EUQQzw2zmEJMzbZSlaB&#43;tWGY=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://tech.zealscott.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tech.zealscott.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tech.zealscott.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tech.zealscott.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://tech.zealscott.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body, 
        {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '\\[', right: '\\]', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false}
                ]
            }
        );"></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-116370175-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<meta property="og:title" content="Analysis of Learning from Positive and Unlabeled Data" />
<meta property="og:description" content="本文从基本的分类损失出发，推导了PU的分类问题其实就是Cost-sensitive classiﬁcation的形式，同时，通过实验证明了如果使用凸函数作为loss function，例如hinge loss会导致错误的分类边界（有bias），因此需要使用例如ramp loss之类的凹函数。同时，论文还对先验$\pi$存在偏差的情况进行了讨论，说明了如果样本中大部分都是正样本，那么就算先验差距比较大，但对总体的分类效果没有太大影响。最后对分类边界进行讨论，证明了使用PU进行分类的误差小于监督学习误差的$2\sqrt{2}$倍。
基本概念和定义  Ordinary classification  Bayes optimal classiﬁer的目标是最小化misclassiﬁcation rate，这在Introduction to Statistical Machine Learning By Masashi Sugiyama 书里有定义，直观理解就是最小化期望错分率： $R(f) = \pi R_1 (f) &#43; (1 - \pi) R_{-1}(f)$ 这里的$R_1$表示false negative rate，也就是分错正类的概率，乘以先验正类的概率$\pi$ $R_{-1}$表示false positive rate，也就是分错负类的概率，乘以先验负类的概率$1-\pi$ 这样，对分错样本的概率分别乘以其先验概率，就是其错分概率的期望。   Cost-sensitive classiﬁcation  如果对于某种错误我们的敏感程度不一样，那么就乘以不同的权重，重新定义为： $R(f) = \pi c_1 R_1(f) &#43; (1-\pi) c_{-1}R_{-1}(f)$ 这里用$c_1$和$c_{-1}$分别表示对两种错分的代价   PU classification   定义在未标记数据集$X$ 中的分布：
  $P_X = \pi P_1 &#43; (1-\pi) P_{-1}$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tech.zealscott.com/deeplearning/pulearning/pu-learning-non-convex/" /><meta property="og:image" content="https://tech.zealscott.com/papermod-cover.png"/><meta property="article:section" content="deeplearning" />
<meta property="article:published_time" content="2019-07-29T11:23:30&#43;08:00" />
<meta property="article:modified_time" content="2019-07-29T11:23:30&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://tech.zealscott.com/papermod-cover.png"/>

<meta name="twitter:title" content="Analysis of Learning from Positive and Unlabeled Data"/>
<meta name="twitter:description" content="本文从基本的分类损失出发，推导了PU的分类问题其实就是Cost-sensitive classiﬁcation的形式，同时，通过实验证明了如果使用凸函数作为loss function，例如hinge loss会导致错误的分类边界（有bias），因此需要使用例如ramp loss之类的凹函数。同时，论文还对先验$\pi$存在偏差的情况进行了讨论，说明了如果样本中大部分都是正样本，那么就算先验差距比较大，但对总体的分类效果没有太大影响。最后对分类边界进行讨论，证明了使用PU进行分类的误差小于监督学习误差的$2\sqrt{2}$倍。
基本概念和定义  Ordinary classification  Bayes optimal classiﬁer的目标是最小化misclassiﬁcation rate，这在Introduction to Statistical Machine Learning By Masashi Sugiyama 书里有定义，直观理解就是最小化期望错分率： $R(f) = \pi R_1 (f) &#43; (1 - \pi) R_{-1}(f)$ 这里的$R_1$表示false negative rate，也就是分错正类的概率，乘以先验正类的概率$\pi$ $R_{-1}$表示false positive rate，也就是分错负类的概率，乘以先验负类的概率$1-\pi$ 这样，对分错样本的概率分别乘以其先验概率，就是其错分概率的期望。   Cost-sensitive classiﬁcation  如果对于某种错误我们的敏感程度不一样，那么就乘以不同的权重，重新定义为： $R(f) = \pi c_1 R_1(f) &#43; (1-\pi) c_{-1}R_{-1}(f)$ 这里用$c_1$和$c_{-1}$分别表示对两种错分的代价   PU classification   定义在未标记数据集$X$ 中的分布：
  $P_X = \pi P_1 &#43; (1-\pi) P_{-1}$"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Analysis of Learning from Positive and Unlabeled Data",
      "item": "https://tech.zealscott.com/deeplearning/pulearning/pu-learning-non-convex/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Analysis of Learning from Positive and Unlabeled Data",
  "name": "Analysis of Learning from Positive and Unlabeled Data",
  "description": "本文从基本的分类损失出发，推导了PU的分类问题其实就是Cost-sensitive classiﬁcation的形式，同时，通过实验证明了如果使用凸函数作为loss function，例如hinge loss会导致错误的分类边界（有bias），因此需要使用例如ramp loss之类的凹函数。同时，论文还对先验$\\pi$存在偏差的情况进行了讨论，说明了如果样本中大部分都是正样本，那么就算先验差距比较大，但对总体的分类效果没有太大影响。最后对分类边界进行讨论，证明了使用PU进行分类的误差小于监督学习误差的$2\\sqrt{2}$倍。\n基本概念和定义  Ordinary classification  Bayes optimal classiﬁer的目标是最小化misclassiﬁcation rate，这在Introduction to Statistical Machine Learning By Masashi Sugiyama 书里有定义，直观理解就是最小化期望错分率： $R(f) = \\pi R_1 (f) + (1 - \\pi) R_{-1}(f)$ 这里的$R_1$表示false negative rate，也就是分错正类的概率，乘以先验正类的概率$\\pi$ $R_{-1}$表示false positive rate，也就是分错负类的概率，乘以先验负类的概率$1-\\pi$ 这样，对分错样本的概率分别乘以其先验概率，就是其错分概率的期望。   Cost-sensitive classiﬁcation  如果对于某种错误我们的敏感程度不一样，那么就乘以不同的权重，重新定义为： $R(f) = \\pi c_1 R_1(f) + (1-\\pi) c_{-1}R_{-1}(f)$ 这里用$c_1$和$c_{-1}$分别表示对两种错分的代价   PU classification   定义在未标记数据集$X$ 中的分布：\n  $P_X = \\pi P_1 + (1-\\pi) P_{-1}$",
  "keywords": [
    "deep learning", "paper"
  ],
  "articleBody": "本文从基本的分类损失出发，推导了PU的分类问题其实就是Cost-sensitive classiﬁcation的形式，同时，通过实验证明了如果使用凸函数作为loss function，例如hinge loss会导致错误的分类边界（有bias），因此需要使用例如ramp loss之类的凹函数。同时，论文还对先验$\\pi$存在偏差的情况进行了讨论，说明了如果样本中大部分都是正样本，那么就算先验差距比较大，但对总体的分类效果没有太大影响。最后对分类边界进行讨论，证明了使用PU进行分类的误差小于监督学习误差的$2\\sqrt{2}$倍。\n基本概念和定义  Ordinary classification  Bayes optimal classiﬁer的目标是最小化misclassiﬁcation rate，这在Introduction to Statistical Machine Learning By Masashi Sugiyama 书里有定义，直观理解就是最小化期望错分率： $R(f) = \\pi R_1 (f) + (1 - \\pi) R_{-1}(f)$ 这里的$R_1$表示false negative rate，也就是分错正类的概率，乘以先验正类的概率$\\pi$ $R_{-1}$表示false positive rate，也就是分错负类的概率，乘以先验负类的概率$1-\\pi$ 这样，对分错样本的概率分别乘以其先验概率，就是其错分概率的期望。   Cost-sensitive classiﬁcation  如果对于某种错误我们的敏感程度不一样，那么就乘以不同的权重，重新定义为： $R(f) = \\pi c_1 R_1(f) + (1-\\pi) c_{-1}R_{-1}(f)$ 这里用$c_1$和$c_{-1}$分别表示对两种错分的代价   PU classification   定义在未标记数据集$X$ 中的分布：\n  $P_X = \\pi P_1 + (1-\\pi) P_{-1}$\n  注意，这里的$P_X$可以理解为样本的分布：\n$$P(x) = P(y=1)P(x|y=1) + P(y=-1)P(x|y=-1)$$\n  也就是说，$P_1 = P(x|y = 1), P_{-1} = P(x|y=-1)$\n  论文认为两个数据集的分布不同：\n 对于positive sample：$x \\sim P(x|y=1)$ 对于unlabel sample：$x\\sim P_X$      对于PU问题，我们没有办法直接得到负类的信息，因此我们想要把目标函数中的$R_{-1}(f)$去掉。定义$R_X(f)$表示在$P_X$分布下预测为正类的风险risk：\n$$\\begin{equation}\\begin{split}R_X(f) \u0026= P_X(f(X = 1)) \\\u0026= \\pi P_1(f(X) = 1) + (1-\\pi) P_{-1}(f(X) = 1) \\\u0026= \\pi(1-R_1(f)) + (1-\\pi) R_{-1}(f) \\end{split}\\end{equation}$$\n  这样，我们就可以将$R_{-1}$替换为$R_X(f)$：\n$$\\begin{equation}\\begin{split}R(f) \u0026= \\pi R_1(f) + (1-\\pi)R_{-1}(f) \\\u0026= \\pi R_1(f) - \\pi(1-R_1(f)) + R_X(f) \\\u0026= 2\\pi R_1(f) + R_X(f) - \\pi \\end{split}\\end{equation}$$\n  我们可以定义$\\eta \\sim \\frac{n}{n + n’}$是$P_1$与$P_X$的占比，也就是在我们正类数据集样本数占所有样本数的比例，因此可进一步写成：\n $R(f) = c_1\\eta R_1(f) + c_X(1-\\eta)R_X(f)- \\pi$ 其中$c_1 = \\frac{2\\pi}{\\eta},c_X = \\frac{1}{1-\\eta}$    这样，我们就把PU分类问题转换为了Cost-sensitive classiﬁcation问题。通过设置不同的阈值并最小化分类错误率，就可以使用SVM等分类器进行训练。\n    Necessity of non-convex loss functions 论文认为，如果在PU分类问题中使用常见的凸函数作为loss function，可能导致结果有biased，因此需要选择非凸函数。\n Loss functions in ordinary classiﬁcation  在分类器上定义一个符号函数：$sign (g(x)) = f(x)$ 使用01损失，仿照之前的期望错分率定义损失函数：  $J_{0-1}(g) = \\pi E_1[l_{0-1}(g(X))] + (1-\\pi )E_{-1}[l_{0-1}(-g(X))] $ $l_{0-1}$在大于0的时候取0，小于0时取1   由于01损失在实际中很难优化，因此用ramp loss代替：  $l_R(z) = \\frac{1}{2}\\max(0,\\min(2,1-z))$   而为了保证凸性，因此一般使用hinge loss：  $l_H(z) = \\frac{1}{2}\\max(1-z,0)$     可以发现，ramp loss 在大于1时没有损失，在-1到1之间为线性损失，而大于1以后损失恒定为1 而hinge loss在小于1时也依然为线性损失（在SVM中使用）   Ramp loss function in PU classiﬁcation  将ramp loss带入之前定义的PU目标函数中，同时根据ramp loss的特殊性质：$l_R(-z) +l_R(z) = 1$，我们可以得到 $J_{PU-R} (g) = \\pi E_1[l_R(g(X))] + (1-\\pi)E_{-1}[l_R(-g(X))]$ 这个形式和最初的分类损失相同，也就是说，它们会有相同的分类边界   Hinge loss function in PU classiﬁcation  如果用hinge loss，同样的道理我们可以得到：     除了最初分类损失的项，还有另外的一项惩罚 作者认为，这个惩罚会导致分类边界的改变，及时$g(X)$很好的区分了数据，由于惩罚项的存在，目标函数可能并不会是最小值   论文做了一个简单的实验，说明了如果使用hinge loss，那么在先验$\\pi$增大的情况下，阈值增大的非常快，也就是说，会将所有的样本都标记为正样本（阈值为1），因此false positive概率为1。这样会导致总的分类错误率为$1-\\pi$：      因此，作者用实验和公式说明了，光光最小化分类错误率不够（因为当$\\pi$很大时，会将所有类标记为正类以获得最小的损失$1- \\pi$，因此需要使用ramp loss对其进行惩罚  Effect of inaccurate class-prior estimation 现实中有很多方法来估计先验$\\pi$，但如果估计值离实际值很大（有偏），那么会对我们的PU问题造成什么样的影响？\n Ordinary classification  考虑普通分类的情形。我们的目标函数时最小化$R(f,\\pi) = \\pi R_1(f) + (1-\\pi) R_{-1}(f)$ 注意，这是一个凹函数。 如果我们的先验为$\\hat{\\pi}$，最小化后得到的分类器为$\\hat{f}$，这时候固定$\\hat{f}$，真实的先验为$\\pi$，可以发现当靠近$\\hat{\\pi}$时两个risk 的差距最小，随着$\\pi$的变化而逐渐增大：        PU classification  通过变量替换，我们同时定义当前的先验为$\\hat{\\pi}$，真实的先验为$\\pi$，可以得到risk为：  $R(f) = (2\\hat{\\pi} - \\pi) R_1(f) + (1-\\pi)R_{-1}(f) + \\pi - \\hat{\\pi}$   可以发现，如果 $\\hat{\\pi } \\le \\frac{1}{2}\\pi$ 时，该分类就完全失效了（risk 的符号改变） 将其进行归一化，定义effective class prior为：       观察图片可以发现，当真实的$\\pi$很大，那么估计的先验就算相差大一点，影响也不大（顶部较为平缓），这与现实相符。例如，如果我们在异常检测中，正类远远大于负类，那么估计的阈值稍微小优点，也不会对risk造成太大的改变。 同样，如果真实的正类并不多，那么对正类的估计如果不准的话，会对结果造成较大改变。    Generalization error bounds for PU classiﬁcation 待完成。。。\nReference  Analysis of Learning from Positive and Unlabeled Data Introduction to Statistical Machine Learning By Masashi Sugiyama Why are the popular loss functions convex?  ",
  "wordCount" : "298",
  "inLanguage": "en",
  "datePublished": "2019-07-29T11:23:30+08:00",
  "dateModified": "2019-07-29T11:23:30+08:00",
  "author":{
    "@type": "Person",
    "name": "Scott Du"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tech.zealscott.com/deeplearning/pulearning/pu-learning-non-convex/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Scott'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tech.zealscott.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tech.zealscott.com" accesskey="h" title="Scott&#39;Log (Alt + H)">Scott&#39;Log</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tech.zealscott.com/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/deeplearning/" title="Deep Learning">
                    <span>Deep Learning</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/curriculum/" title="Curriculum">
                    <span>Curriculum</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/misc/" title="Misc">
                    <span>Misc</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Analysis of Learning from Positive and Unlabeled Data
    </h1>
    <div class="post-meta"><span title='2019-07-29 11:23:30 +0800 CST'>July 29, 2019</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Scott Du

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5%e5%92%8c%e5%ae%9a%e4%b9%89" aria-label="基本概念和定义">基本概念和定义</a></li>
                <li>
                    <a href="#necessity-of-non-convex-loss-functions" aria-label="Necessity of non-convex loss functions">Necessity of non-convex loss functions</a></li>
                <li>
                    <a href="#effect-of-inaccurate-class-prior-estimation" aria-label="Effect of inaccurate class-prior estimation">Effect of inaccurate class-prior estimation</a></li>
                <li>
                    <a href="#generalization-error-bounds-for-pu-classi%ef%ac%81cation" aria-label="Generalization error bounds for PU classiﬁcation">Generalization error bounds for PU classiﬁcation</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>本文从基本的分类损失出发，推导了PU的分类问题其实就是<code>Cost-sensitive classiﬁcation</code>的形式，同时，通过实验证明了如果使用凸函数作为loss function，例如<code>hinge loss</code>会导致错误的分类边界（有bias），因此需要使用例如<code>ramp loss</code>之类的凹函数。同时，论文还对先验$\pi$存在偏差的情况进行了讨论，说明了如果样本中大部分都是正样本，那么就算先验差距比较大，但对总体的分类效果没有太大影响。最后对分类边界进行讨论，证明了使用PU进行分类的误差小于监督学习误差的$2\sqrt{2}$倍。</p>
<h2 id="基本概念和定义">基本概念和定义<a hidden class="anchor" aria-hidden="true" href="#基本概念和定义">#</a></h2>
<ol>
<li><code>Ordinary classification</code>
<ul>
<li><code>Bayes optimal classiﬁer</code>的目标是最小化<code>misclassiﬁcation rate</code>，这在<em>Introduction to Statistical Machine Learning By Masashi Sugiyama</em> 书里有定义，直观理解就是最小化期望错分率：</li>
<li>$R(f) = \pi R_1 (f) + (1 - \pi) R_{-1}(f)$</li>
<li>这里的$R_1$表示<code>false negative rate</code>，也就是分错正类的概率，乘以先验正类的概率$\pi$</li>
<li>$R_{-1}$表示<code>false positive rate</code>，也就是分错负类的概率，乘以先验负类的概率$1-\pi$</li>
<li>这样，对分错样本的概率分别乘以其先验概率，就是其错分概率的期望。</li>
</ul>
</li>
<li><code>Cost-sensitive classiﬁcation</code>
<ul>
<li>如果对于某种错误我们的敏感程度不一样，那么就乘以不同的权重，重新定义为：</li>
<li>$R(f) = \pi c_1 R_1(f) + (1-\pi) c_{-1}R_{-1}(f)$</li>
<li>这里用$c_1$和$c_{-1}$分别表示对两种错分的代价</li>
</ul>
</li>
<li><code>PU classification</code>
<ul>
<li>
<p>定义在<strong>未标记数据集</strong>$X$ 中的分布：</p>
<ul>
<li>
<p>$P_X = \pi P_1 + (1-\pi) P_{-1}$</p>
</li>
<li>
<p>注意，这里的$P_X$可以理解为样本的分布：</p>
<p>$$P(x) = P(y=1)P(x|y=1) + P(y=-1)P(x|y=-1)$$</p>
</li>
<li>
<p>也就是说，$P_1 = P(x|y = 1), P_{-1} = P(x|y=-1)$</p>
</li>
<li>
<p>论文认为两个数据集的分布不同：</p>
<ul>
<li>对于positive sample：$x \sim P(x|y=1)$</li>
<li>对于unlabel sample：$x\sim P_X$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>对于PU问题，我们没有办法直接得到负类的信息，因此我们想要把目标函数中的$R_{-1}(f)$去掉。定义$R_X(f)$表示在$P_X$分布下预测为正类的风险risk：</p>
<p>$$\begin{equation}\begin{split}R_X(f) &amp;= P_X(f(X = 1)) \&amp;= \pi P_1(f(X) = 1) + (1-\pi) P_{-1}(f(X) = 1)  \&amp;= \pi(1-R_1(f)) + (1-\pi) R_{-1}(f)  \end{split}\end{equation}$$</p>
</li>
<li>
<p>这样，我们就可以将$R_{-1}$替换为$R_X(f)$：</p>
<p>$$\begin{equation}\begin{split}R(f) &amp;= \pi R_1(f) + (1-\pi)R_{-1}(f) \&amp;= \pi R_1(f) - \pi(1-R_1(f)) + R_X(f)  \&amp;= 2\pi R_1(f) + R_X(f) - \pi  \end{split}\end{equation}$$</p>
</li>
<li>
<p>我们可以定义$\eta \sim \frac{n}{n + n&rsquo;}$是$P_1$与$P_X$的占比，也就是在我们正类数据集样本数占所有样本数的比例，因此可进一步写成：</p>
<ul>
<li>$R(f) = c_1\eta R_1(f) + c_X(1-\eta)R_X(f)- \pi$</li>
<li>其中$c_1 = \frac{2\pi}{\eta},c_X = \frac{1}{1-\eta}$</li>
</ul>
</li>
<li>
<p>这样，我们就把PU分类问题转换为了Cost-sensitive classiﬁcation问题。通过设置不同的阈值并最小化分类错误率，就可以使用SVM等分类器进行训练。</p>
</li>
</ul>
</li>
</ol>
<h2 id="necessity-of-non-convex-loss-functions">Necessity of non-convex loss functions<a hidden class="anchor" aria-hidden="true" href="#necessity-of-non-convex-loss-functions">#</a></h2>
<p>论文认为，如果在PU分类问题中使用常见的凸函数作为loss function，可能导致结果有biased，因此需要选择非凸函数。</p>
<ol>
<li>Loss functions in ordinary classiﬁcation
<ul>
<li>在分类器上定义一个符号函数：$sign (g(x)) = f(x)$</li>
<li>使用01损失，仿照之前的期望错分率定义损失函数：
<ul>
<li>$J_{0-1}(g) = \pi E_1[l_{0-1}(g(X))] + (1-\pi )E_{-1}[l_{0-1}(-g(X))] $</li>
<li>$l_{0-1}$在大于0的时候取0，小于0时取1</li>
</ul>
</li>
<li>由于01损失在实际中很难优化，因此用<em>ramp loss</em>代替：
<ul>
<li>$l_R(z) = \frac{1}{2}\max(0,\min(2,1-z))$</li>
</ul>
</li>
<li>而为了保证凸性，因此一般使用<em>hinge loss</em>：
<ul>
<li>$l_H(z) = \frac{1}{2}\max(1-z,0)$</li>
</ul>
</li>
<li>
<img src="/images/deeplearning/pulearning/pu2.png" width=50%>
</li>
<li>可以发现，<code>ramp loss</code> 在大于1时没有损失，在-1到1之间为线性损失，而大于1以后损失恒定为1</li>
<li>而<code>hinge loss</code>在小于1时也依然为线性损失（在SVM中使用）</li>
</ul>
</li>
<li>Ramp loss function in PU classiﬁcation
<ul>
<li>将<code>ramp loss</code>带入之前定义的PU目标函数中，同时根据<code>ramp loss</code>的特殊性质：$l_R(-z) +l_R(z) = 1$，我们可以得到</li>
<li>$J_{PU-R} (g) = \pi E_1[l_R(g(X))] + (1-\pi)E_{-1}[l_R(-g(X))]$</li>
<li>这个形式和最初的分类损失相同，也就是说，它们会有相同的分类边界</li>
</ul>
</li>
<li>Hinge loss function in PU classiﬁcation
<ul>
<li>如果用<code>hinge loss</code>，同样的道理我们可以得到：
<ul>
<li><img loading="lazy" src="/images/deeplearning/pulearning/pu3.png" alt="pu3"  />
</li>
</ul>
</li>
<li>除了最初分类损失的项，还有另外的一项惩罚</li>
<li>作者认为，这个惩罚会导致分类边界的改变，及时$g(X)$很好的区分了数据，由于惩罚项的存在，目标函数可能并不会是最小值</li>
</ul>
</li>
<li>论文做了一个简单的实验，说明了如果使用<code>hinge loss</code>，那么在先验$\pi$增大的情况下，阈值增大的非常快，也就是说，会将所有的样本都标记为正样本（阈值为1），因此false positive概率为1。这样会导致总的分类错误率为$1-\pi$：
<ul>
<li>
<img src="/images/deeplearning/pulearning/pu4.png" width=90%>
</li>
</ul>
</li>
<li>因此，作者用实验和公式说明了，光光最小化分类错误率不够（因为当$\pi$很大时，会将所有类标记为正类以获得最小的损失$1- \pi$，因此需要使用<code>ramp loss</code>对其进行惩罚</li>
</ol>
<h2 id="effect-of-inaccurate-class-prior-estimation">Effect of inaccurate class-prior estimation<a hidden class="anchor" aria-hidden="true" href="#effect-of-inaccurate-class-prior-estimation">#</a></h2>
<p>现实中有很多方法来估计先验$\pi$，但如果估计值离实际值很大（有偏），那么会对我们的PU问题造成什么样的影响？</p>
<ol>
<li>Ordinary classification
<ul>
<li>考虑普通分类的情形。我们的目标函数时最小化$R(f,\pi) = \pi R_1(f) + (1-\pi) R_{-1}(f)$</li>
<li>注意，这是一个凹函数。</li>
<li>如果我们的先验为$\hat{\pi}$，最小化后得到的分类器为$\hat{f}$，这时候固定$\hat{f}$，真实的先验为$\pi$，可以发现当靠近$\hat{\pi}$时两个risk 的差距最小，随着$\pi$的变化而逐渐增大：
<ul>
<li>
<img src="/images/deeplearning/pulearning/pu5.png" width=60%>
</li>
</ul>
</li>
</ul>
</li>
<li>PU classification
<ul>
<li>通过变量替换，我们同时定义当前的先验为$\hat{\pi}$，真实的先验为$\pi$，可以得到risk为：
<ul>
<li>$R(f) = (2\hat{\pi} - \pi) R_1(f) + (1-\pi)R_{-1}(f) + \pi - \hat{\pi}$</li>
</ul>
</li>
<li>可以发现，如果 $\hat{\pi } \le \frac{1}{2}\pi$ 时，该分类就完全失效了（risk 的符号改变）</li>
<li>将其进行归一化，定义<code>effective class prior</code>为：
<ul>
<li>
<img src="/images/deeplearning/pulearning/pu6.png" width=50%>
</li>
</ul>
</li>
<li><img loading="lazy" src="/images/deeplearning/pulearning/pu7.png" alt="pu7"  />
</li>
<li>观察图片可以发现，当真实的$\pi$很大，那么估计的先验就算相差大一点，影响也不大（顶部较为平缓），这与现实相符。例如，如果我们在异常检测中，正类远远大于负类，那么估计的阈值稍微小优点，也不会对risk造成太大的改变。</li>
<li>同样，如果真实的正类并不多，那么对正类的估计如果不准的话，会对结果造成较大改变。</li>
</ul>
</li>
</ol>
<h2 id="generalization-error-bounds-for-pu-classiﬁcation">Generalization error bounds for PU classiﬁcation<a hidden class="anchor" aria-hidden="true" href="#generalization-error-bounds-for-pu-classiﬁcation">#</a></h2>
<p>待完成。。。</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ol>
<li>Analysis of Learning from Positive and Unlabeled Data</li>
<li>Introduction to Statistical Machine Learning By Masashi Sugiyama</li>
<li><a href="https://www.quora.com/Why-are-the-popular-loss-functions-convex-especially-in-the-context-of-deep-learning">Why are the popular loss functions convex?</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tech.zealscott.com/tags/deep-learning/">deep learning</a></li>
      <li><a href="https://tech.zealscott.com/tags/paper/">paper</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://tech.zealscott.com/deeplearning/pulearning/npu/">
    <span class="title">«</span>
    <br>
    <span>Convex Formulation for Learning from Positive and Unlabeled Data</span>
  </a>
  <a class="next" href="https://tech.zealscott.com/deeplearning/pulearning/pu-learning/">
    <span class="title">»</span>
    <br>
    <span>Learning Classiﬁers from Only Positive and Unlabeled Data</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Analysis of Learning from Positive and Unlabeled Data on twitter"
        href="https://twitter.com/intent/tweet/?text=Analysis%20of%20Learning%20from%20Positive%20and%20Unlabeled%20Data&amp;url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fpulearning%2fpu-learning-non-convex%2f&amp;hashtags=deeplearning%2cpaper">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Analysis of Learning from Positive and Unlabeled Data on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fpulearning%2fpu-learning-non-convex%2f&amp;title=Analysis%20of%20Learning%20from%20Positive%20and%20Unlabeled%20Data&amp;summary=Analysis%20of%20Learning%20from%20Positive%20and%20Unlabeled%20Data&amp;source=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fpulearning%2fpu-learning-non-convex%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Analysis of Learning from Positive and Unlabeled Data on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fpulearning%2fpu-learning-non-convex%2f&title=Analysis%20of%20Learning%20from%20Positive%20and%20Unlabeled%20Data">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Analysis of Learning from Positive and Unlabeled Data on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fpulearning%2fpu-learning-non-convex%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Analysis of Learning from Positive and Unlabeled Data on whatsapp"
        href="https://api.whatsapp.com/send?text=Analysis%20of%20Learning%20from%20Positive%20and%20Unlabeled%20Data%20-%20https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fpulearning%2fpu-learning-non-convex%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Analysis of Learning from Positive and Unlabeled Data on telegram"
        href="https://telegram.me/share/url?text=Analysis%20of%20Learning%20from%20Positive%20and%20Unlabeled%20Data&amp;url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fpulearning%2fpu-learning-non-convex%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://tech.zealscott.com">Scott&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
