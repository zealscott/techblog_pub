{"./":{"url":"./","title":"Introduction","keywords":"","body":"Statistics And Data Analysis In this course, we study some basic probility and sampling theories which are commonly used in machine learning. This GitBook notes are maintained by zealscott. Syllabus Lecture Key Notes Reading Material 1. Statistics review - Random Variable- pdf/cdf/pmf- Joint distribution- Expectation/covariance- Conditional expectation Probability theory Review - Chernoff bound 2. common distributions - Discrete- Continuous Useful distributionConjugate PriorNormal - Gamma Conjugate - expoential family 3. sample and limit theory - sampling mean and variance- order statistics distribution- weak and strong law of large numbers- central limit theorem sample and limit theory 4. T and F distribution - χ2\\chi^2χ2 distribution- ttt distribution- FFF distribution T and F distribution 5. Sampling - Sample Random sampling- Confident interval- MCMC- Gibbs Sampling Survey Sampling - 拒绝采样- 别名采样- MCMC 6. Estimation of Parameters&Fitting of Probability Distribution - MLE（parameter）- MLE for exponential family- Conjugate family （Bayes）- EM algorithm LDA思考与总结 - 文本主题模型之LDA(一) LDA基础 7. Testing Hypotheses and assessing Goodness of fit - Neyman-Perason lemma- Likelihood Ratio Tests 8. Testing and Summarizing Testing and Summarizing) 9. The analysis of Variance The Analysis Of Variance 10. notes for exam Notes For Exam @Last updated at 1/23/2021 "},"Statistics Review.html":{"url":"Statistics Review.html","title":"Probability theory review","keywords":"","body":"Basics of Probability Theorylec1_{lec1}lec1​ The Calculus of Probabilities Probability operators Probability properties If PPP is a probability function, AAA and BBB are any two sets in BBB, then P(∅)=0P(\\emptyset ) = 0P(∅)=0, where ∅\\emptyset ∅ is the empty set P(A)≤1P(A) \\le 1P(A)≤1 P(B∩Ac)=P(B)−P(A∩B)P(B \\cap A^c ) = P(B) - P(A \\cap B)P(B∩Ac)=P(B)−P(A∩B) P(A∪B)=P(A)+P(B)−P(A∩B)P(A \\cup B) = P(A) + P(B) - P(A \\cap B)P(A∪B)=P(A)+P(B)−P(A∩B) If A⊂BA \\subset BA⊂B, then P(A)≤P(B)P(A) \\le P(B)P(A)≤P(B) Events A1A_1A1​ and A2A_2A2​ are pair-wise independent (statistically independent) if and only if P(A1∩A2)=P(A1)P(A2)P(A_1 \\cap A_2) = P(A_1)P(A_2)P(A1​∩A2​)=P(A1​)P(A2​) mutually independent: P(A1∩A2∩...∩An)=P(A1)P(A2)...P(An)P(A_1 \\cap A_2 \\cap ... \\cap A_n) = P(A_1)P(A_2)...P(A_n)P(A1​∩A2​∩...∩An​)=P(A1​)P(A2​)...P(An​) Note the difference between independent and mutually exclusive mutually exclusive: cov(X,Y)=0cov(X,Y) = 0cov(X,Y)=0 independent: P(X,Y)=P(X)P(Y)P(X,Y) = P(X)P(Y)P(X,Y)=P(X)P(Y) Let AA AandB BB be events with P(B)>0P(B) > 0P(B)>0. The conditional probability of A givenB BB, denoted by P(A∣B)P(A|B)P(A∣B), is defined as P(A∣B)=P(A∩B)P(B)P(A|B) = \\frac{P(A\\cap B)}{P(B)}P(A∣B)=P(B)P(A∩B)​ Total probability theorem: P(A)=P(B1)P(A∣B1)+P(B2)P(A∣B2)+P(B3)P(A∣B3)P(A) = P(B_1) P(A|B_1) + P(B_2) P(A|B_2) + P(B_3) P(A|B_3)P(A)=P(B1​)P(A∣B1​)+P(B2​)P(A∣B2​)+P(B3​)P(A∣B3​) P(A)=∑i=1nP(Bi)(A∣Bi)P(A) = \\sum\\limits_{i=1}^{n}P(B_i) {(A|B_i)}P(A)=i=1∑n​P(Bi​)(A∣Bi​) Bayes' Theorem P(Bi∣A)=P(A∣Bi)P(Bi)∑k=1nP(A∣Bk)P(Bk)P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum\\limits_{k=1}^{n}P(A|B_k)P(B_k)}P(Bi​∣A)=k=1∑n​P(A∣Bk​)P(Bk​)P(A∣Bi​)P(Bi​)​ Counting inclusion-exclusion ∣A∪B∣=∣A∣+∣B∣−∣A∩B∣|A\\cup B| = |A| + |B| - |A\\cap B|∣A∪B∣=∣A∣+∣B∣−∣A∩B∣ Permutations and combinations P(n,m)=n!(n−m)!P(n,m) = \\frac{n!}{(n-m)!}P(n,m)=(n−m)!n!​ C(n,m)=n!m!(n−m)!C(n,m) = \\frac{n!}{m!(n-m)!}C(n,m)=m!(n−m)!n!​ Random Variablelec2_{lec2}lec2​ A random variable (r.v.) X is a function from sample space of an experiment to the set of real numbers in R: ∀w∈Ω,X(w)=x∈R\\forall w\\in \\Omega, X(w) = x \\in R ∀w∈Ω,X(w)=x∈R Note that a random variable is a function, and not a variable, and not random. Cumulative distribution function The cdf of a r.v denoted by Fx(X)F_x(X)Fx​(X) is defined by : FX(x)=PX(X≤x)F_X(x) = P_X(X\\le x)FX​(x)=PX​(X≤x) lim⁡x→−∞=0\\lim _{x \\rightarrow -\\infty} = 0 limx→−∞​=0 lim⁡x→∞=1\\lim _{x \\rightarrow \\infty} = 1 limx→∞​=1 F(x)F(x) F(x) is nondecreasing function of xxx F(x)F(x)F(x) is right-continuous two r.v.s that are identically distributed are not necessarily equal. Probability mass function The pmf of a discrete r.v. XXX is given by fX(x)=P(X=x)f_X(x) = P(X = x)fX​(x)=P(X=x) Probability density function The probability density function or pdf, fX(x)f_X(x)fX​(x), of a continuous r.v. XXX is the function that satisfies: FX(x)=∫−∞xfX(t)dtF_X(x) = \\int_{- \\infty}^x f_X(t) dtFX​(x)=∫−∞x​fX​(t)dt XXX has a distribution given by FX(x)F_X (x)FX​(x) is abbreviated symbolically by X∼FX(x)X \\sim F_X (x)X∼FX​(x) or X∼fX(x)X \\sim f_X (x)X∼fX​(x). Joint distributionlec3_{lec3}lec3​ P((X,Y)∈A)=∑(x,y)∈Af(x,y)P((X,Y)\\in A) = \\sum_{(x,y)\\in A} f(x,y)P((X,Y)∈A)=∑(x,y)∈A​f(x,y) P((X,Y)∈A)=∫∫Af(x,y)dxdyP((X,Y)\\in A) = \\int \\int_A f(x,y)dxdyP((X,Y)∈A)=∫∫A​f(x,y)dxdy fX(x)=∫−∞+∞fX,Y(x,y)dyf_X(x) = \\int_{-\\infty}^{+\\infty}f_{X,Y}(x,y)dyfX​(x)=∫−∞+∞​fX,Y​(x,y)dy ∂2F(x,y)∂x∂y=f(x,y)\\frac{\\partial ^2F(x,y)}{\\partial x\\partial y} = f(x,y)∂x∂y∂2F(x,y)​=f(x,y) f(x∣y)=f(x,y)fY(y)f(x|y) = \\frac{f(x,y)}{f_Y(y)}f(x∣y)=fY​(y)f(x,y)​ if f(x,y)=fX(x)fY(y)f(x,y) = f_X(x)f_Y(y)f(x,y)=fX​(x)fY​(y), then X,YX,YX,Y are independent. 若变量可分离，则不需要计算边际分布，直接可判断相互独立 Bivariate function (X,Y)(X,Y)(X,Y) be a bivariate r.v, consider a new bivariate r.v (U,V)(U,V)(U,V), define by U=g1(X,Y)U = g_1(X,Y)U=g1​(X,Y) and V=g2(X,Y)V = g_2(X,Y)V=g2​(X,Y) Transformation of discrete B={(u,v)∣u=g1(x,y),v=g2(x,y),(x,y)∈A}B = \\{(u,v) | u = g_1(x,y), v=g_2(x,y) ,(x,y) \\in A \\}B={(u,v)∣u=g1​(x,y),v=g2​(x,y),(x,y)∈A} Auv={(x,y)∈A∣u=g1(x,y),v=g2(x,y)}A_{uv} = \\{ (x,y)\\in A | u = g_1(x,y), v=g_2(x,y) \\}Auv​={(x,y)∈A∣u=g1​(x,y),v=g2​(x,y)} fu,v=P(I=u,V=v)=P((X,Y)∈Auv)=∑(x,y)∈AuvfX,Y(x,y)f_{u,v} = P(I = u,V= v) = P((X,Y) \\in A_{uv}) = \\sum_{(x,y)\\in A_{uv} } f_{X,Y}(x,y)fu,v​=P(I=u,V=v)=P((X,Y)∈Auv​)=∑(x,y)∈Auv​​fX,Y​(x,y) Transformation of continuous J=∣∂x∂u∂x∂v∂y∂u∂y∂v∣ J=\\left|\\begin{array}{ll} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\end{array}\\right| J=∣∣∣∣​∂u∂x​∂u∂y​​∂v∂x​∂v∂y​​∣∣∣∣​ fu,v=fX,Y(h1(u,v),h2(u,v))∣J∣f_{u,v} = f_{X,Y}(h_1(u,v),h_2(u,v))|J|fu,v​=fX,Y​(h1​(u,v),h2​(u,v))∣J∣ 这是用反函数求解的方法，若有些题无法用反函数求解，则使用累计密度函数带入计算 Expectation & covariancelec4_{lec4}lec4​ Expectation value denoted as R(g(X))R(g(X))R(g(X)): E(g(X))=∫−∞+∞g(x)fX(x) if X is continuous =∑x∈Xg(x)P(X=x) if X is discrete  \\begin{aligned} &E(g(X))=\\int_{-\\infty}^{+\\infty} g(x) f_{X}(x) \\text { if } X \\text { is continuous }\\\\ &=\\sum_{x \\in X} g(x) P(X=x) \\text { if } \\mathrm{X} \\text { is discrete } \\end{aligned} ​E(g(X))=∫−∞+∞​g(x)fX​(x) if X is continuous =x∈X∑​g(x)P(X=x) if X is discrete ​ note: expectation is not always exist Cauchy r.v, the pdf: fX(x)=1π(1+x2)f_X(x) = \\frac{1}{\\pi (1+x^2)}fX​(x)=π(1+x2)1​ E(X)=∞E(X) = \\inftyE(X)=∞ Linearity of expectations E(ag1(X)+bg2(X)+c)=aE(g1(X))+bE(g2(X))+cE(ag_1(X) + bg_2(X) + c ) = aE(g_1(X)) + bE(g_2(X)) + cE(ag1​(X)+bg2​(X)+c)=aE(g1​(X))+bE(g2​(X))+c if a≤g1(x)≤ba \\le g_1(x) \\le ba≤g1​(x)≤b for all xxx, then a≤E(g1(X))≤ba\\le E(g_1(X)) \\le ba≤E(g1​(X))≤b Uniform exponential relationship can use uniform distribution to form other distribution: exponential, normalization, which is actually do in computer suppose X∼U(0,1)X \\sim U(0,1)X∼U(0,1),let Y=g(X)=−log⁡XY = g(X) = -\\log XY=g(X)=−logX FY(y)=P(Y≤y)=P(−log⁡X≤y)=PX(x≥e−y)=1−e−yF_Y(y) = P(Y\\le y) = P(-\\log X \\le y) = P_X(x\\ge e^{-y}) = 1- e^{-y}FY​(y)=P(Y≤y)=P(−logX≤y)=PX​(x≥e−y)=1−e−y fY(y)=e−yf_Y(y) = e^{-y}fY​(y)=e−y so Y∼exp⁡(1)Y \\sim \\exp(1)Y∼exp(1) Moment For each integer nnn, the n−thn-thn−th moment of XXX, is μn=E(Xn)\\mu_n = E(X^n)μn​=E(Xn) The n−thn-thn−th central moment of XXX, μn=E(X−μ)n\\mu_n = E(X - \\mu)^nμn​=E(X−μ)n Variance The variance of a r.v. XXX is its second central moment: var(X)=E(X−μ)2var (X) = E(X-\\mu)^2var(X)=E(X−μ)2 var(X)=E(X2)−(E(X))2var(X) = E(X^2) - (E(X))^2var(X)=E(X2)−(E(X))2 Nonlinearity of variance var(aX+b)=a2var(X)var(aX+b) = a^2var(X)var(aX+b)=a2var(X) if XXX and YYY are tow independent r.v.s on a sample space Ω\\OmegaΩ, then: var(X+Y)=var(X)+var(Y)var(X+Y) = var(X) + var(Y)var(X+Y)=var(X)+var(Y) Independence if XXX and YYY are independent r.v.s on a sample space Ω\\OmegaΩ, then: E(XY)=E(X)E(Y)E(XY) = E(X)E(Y)E(XY)=E(X)E(Y) var(X+Y)=var(X)+var(Y)var(X+Y)= var(X)+var(Y)var(X+Y)=var(X)+var(Y) var(X−Y)=var(X)+var(Y)var(X-Y) = var(X) +var(Y)var(X−Y)=var(X)+var(Y) Moment Generating Function can be used to calculate moment the moment generating function of XXX, denoted by MX(t)M_X(t)MX​(t), is: MX(t)=E(etX)M_X(t) = E(e^{tX})MX​(t)=E(etX) MaX+b(t)=ebtEX(at)M_{aX+b}(t) =e^{bt}E_X(at)MaX+b​(t)=ebtEX​(at) is applied to Chernoff bound if the expectation dose not exist, the moment generating function dose not exist. XXX is continuous, MX(t)=∫−∞+∞etxfX(x)dxM_X(t) = \\int _{-\\infty}^{+\\infty} e^{tx}f_X(x) dxMX​(t)=∫−∞+∞​etxfX​(x)dx XXX is discrete, MX(t)=∑xetxP(X=x)M_X(t) = \\sum_xe^{tx}P(X= x)MX​(t)=∑x​etxP(X=x) Theorem if XXX has moment generating function MX(t)M_X(t)MX​(t), then : E(Xn)=Mn(n)(0)E(X^n) = M_n^{(n)}(0)E(Xn)=Mn(n)​(0) where we define: MX(n)(0)=dndtnMX(t)∣t=0M_X^{(n)}(0) = \\frac{d^n}{dt^n} M_X(t) | _{t=0}MX(n)​(0)=dtndn​MX​(t)∣t=0​ can be used to calculate Gamma E(X)E(X)E(X) Property MaX+b(t)=ebtMX(at)M_{aX+b}(t) = e^{bt}M_X(at)MaX+b​(t)=ebtMX​(at) Covariance The covariance and correlation of XXX and YYY are the numbers defined by: Cov(X,Y)=E((X−μX)(Y−μY))Cov(X,Y) = E((X-\\mu _X)(Y-\\mu_Y))Cov(X,Y)=E((X−μX​)(Y−μY​)) ρXY=Cov(X,Y)σXσY\\rho_{XY} = \\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}ρXY​=σX​σY​Cov(X,Y)​ Cov(X,Y)=E(XY)−μXμYCov(X,Y) = E(XY) - \\mu_X\\mu_YCov(X,Y)=E(XY)−μX​μY​ if X,YX,YX,Y are independent r.v.s, then Cov(X,Y)=0Cov(X,Y) = 0Cov(X,Y)=0 and ρXY=0\\rho_{XY} = 0ρXY​=0 Var(aX+bY)=a2Var(X)+b2Var(Y)+2abCov(X,Y)Var(aX+bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)Var(aX+bY)=a2Var(X)+b2Var(Y)+2abCov(X,Y) 相关系数只能说明是否存在线性关系，若等于0，不能说没有关系。 但若使用ρ(X2,Y)\\rho(X^2,Y)ρ(X2,Y)，也可以衡量。 由于任何函数都可以用多项式拟合，因此都可以用相关系数衡量 Bivariate normal pdf f(x,y)=(2πρXρY1−ρ2)−1⋅exp⁡(−12(1−ρ2)((x−μxσX)2−2ρ(x−μxρX)(y−μYρY)+(y−μYσY)2))f(x,y) = (2\\pi \\rho_X\\rho_Y\\sqrt{1-\\rho^2})^{-1}\\cdot \\exp(-\\frac{1}{2(1-\\rho^2)}((\\frac{x-\\mu_x}{\\sigma_X})^2 - 2\\rho(\\frac{x-\\mu_x}{\\rho_X})(\\frac{y-\\mu_Y}{\\rho_Y}) + (\\frac{y-\\mu_Y}{\\sigma_Y})^2))f(x,y)=(2πρX​ρY​1−ρ2​)−1⋅exp(−2(1−ρ2)1​((σX​x−μx​​)2−2ρ(ρX​x−μx​​)(ρY​y−μY​​)+(σY​y−μY​​)2)) marginal distribution X∼N(μX,σX2)X\\sim N(\\mu_X,\\sigma_X^2)X∼N(μX​,σX2​) Y∼N(μY,σY2)Y\\sim N(\\mu_Y,\\sigma_Y^2)Y∼N(μY​,σY2​) ρ=ρXY\\rho = \\rho_{XY}ρ=ρXY​ aX+bY∼N(aμX+bμY,a2σX2+b2σY2+2abρσXσY)aX+bY \\sim N(a\\mu_X+b\\mu_Y,a^2\\sigma_X^2 + b^2\\sigma_Y^2 + 2ab\\rho \\sigma_X\\sigma_Y)aX+bY∼N(aμX​+bμY​,a2σX2​+b2σY2​+2abρσX​σY​) conditional expectationlec4_{lec4}lec4​ Theorem E(X)=E(E(X∣Y))E(X) = E(E(X|Y))E(X)=E(E(X∣Y)) 可以理解为先分组求期望，与直接求期望一样 Var(X)=E(Var(X∣Y))+Var(E(X∣Y))Var(X) = E(Var(X|Y)) + Var(E(X|Y))Var(X)=E(Var(X∣Y))+Var(E(X∣Y)) 可以理解为组内方差的期望 + 组间方差 Mixture distribution Binomial-Poisson hierarchy if X∣Y∼Binomial(Y,P),Y∼Possion(λ)X| Y \\sim Binomial(Y,P),Y\\sim Possion(\\lambda)X∣Y∼Binomial(Y,P),Y∼Possion(λ): P(X=x)=∑P(X=x,Y=y)=∑P(X=x∣Y=y)P(Y=y)=(λP)xx!eλPP(X=x)= \\sum P(X=x,Y=y) = \\sum P(X=x|Y=y)P(Y=y) = \\frac{(\\lambda P)^x}{x!} e^{\\lambda P}P(X=x)=∑P(X=x,Y=y)=∑P(X=x∣Y=y)P(Y=y)=x!(λP)x​eλP ∴X∼Possion(λP)\\therefore X\\sim Possion(\\lambda P)∴X∼Possion(λP) using E(X)=E(E(X∣Y))E(X) = E(E(X|Y))E(X)=E(E(X∣Y)), can easily get E(X)=E(pY)=pλE(X) = E(pY) = p\\lambdaE(X)=E(pY)=pλ Beta-binomial hierarchy if X∣P∼Binomial(n,p),P∼β(α,β)X|P \\sim Binomial(n,p),P\\sim \\beta (\\alpha,\\beta)X∣P∼Binomial(n,p),P∼β(α,β) so E(X)=E(E(X∣P))=E(np)=nαα+βE(X) = E(E(X|P)) = E(np) = \\frac{n\\alpha}{\\alpha + \\beta}E(X)=E(E(X∣P))=E(np)=α+βnα​ "},"Common distribution.html":{"url":"Common distribution.html","title":"Common distribution","keywords":"","body":"Discrete Bernoulli distribution pmf fX(x)=P(X=x)={(1−p)1−xpx for x=0 or 10 otherwise  f_{X}(x)=P(X=x)=\\left\\{\\begin{array}{cl} (1-p)^{1-x} p^{x} & \\text { for } \\mathrm{x}=0 \\text { or } 1 \\\\ 0 & \\text { otherwise } \\end{array}\\right. fX​(x)=P(X=x)={(1−p)1−xpx0​ for x=0 or 1 otherwise ​ expectation E(X)=pE(X) = pE(X)=p variance var(X)=(1−p)pvar(X) = (1-p)pvar(X)=(1−p)p Binomial distribution pmf fX(k)=P(X=k)={Cnkpk(1−p)n−k for k=0,1,…,n0 otherwise  f_{X}(k)=P(X=k)=\\left\\{\\begin{aligned} C_{n}^{k} p^{k}(1-p)^{n-k} & \\text { for } \\mathrm{k}=0,1, \\ldots, \\mathrm{n} \\\\ 0 & \\text { otherwise } \\end{aligned}\\right. fX​(k)=P(X=k)={Cnk​pk(1−p)n−k0​ for k=0,1,…,n otherwise ​ expectation E(X)=npE(X) = npE(X)=np variance var(X)=np(1−p)var(X) = np(1-p)var(X)=np(1−p) Geometric distribution pmf fX(k)=P(X=k)={p(1−p)k−1 for k=1,2,3…0 otherwise  f_{X}(k)=P(X=k)=\\left\\{\\begin{aligned} p(1-p)^{k-1} & \\text { for } \\mathrm{k}=1,2,3 \\ldots \\\\ 0 & \\text { otherwise } \\end{aligned}\\right. fX​(k)=P(X=k)={p(1−p)k−10​ for k=1,2,3… otherwise ​ expectation E(X)=1PE(X) = \\frac{1}{P}E(X)=P1​ variance var(X)=1−PP2var(X) = \\frac{1-P}{P^2}var(X)=P21−P​ memoryless P(X>m+n∣X>m)=P(X>n)P(X>m+n|X>m) = P(X>n)P(X>m+n∣X>m)=P(X>n) Negative binomial distribution(Pascal) The negative binomial distribution arises as a generalization of the geometric distribution. Suppose that a sequence of independent trials each with probability of success ppp is performed until there are rrr successes in all. so can be denote as p⋅Ck−1r−1pr−1(1−p)(k−1)−(r−1)p \\cdot C_{k-1}^{r-1} p^{r-1}(1-p)^{(k-1)-(r-1)}p⋅Ck−1r−1​pr−1(1−p)(k−1)−(r−1) X∼NB(r,p)X\\sim NB(r,p)X∼NB(r,p) pmf fX(k)=P(X=k)={Ck−1r−1pr(1−p)k−r for k=r,r+1,r+2…0 otherwise  f_{X}(k)=P(X=k)=\\left\\{\\begin{aligned} C_{k-1}^{r-1} p^{r}(1-p)^{k-r} & \\text { for } \\mathrm{k}=\\mathrm{r}, \\mathrm{r}+1, \\mathrm{r}+2 \\ldots \\\\ 0 & \\text { otherwise } \\end{aligned}\\right. fX​(k)=P(X=k)={Ck−1r−1​pr(1−p)k−r0​ for k=r,r+1,r+2… otherwise ​ expectation E(X)=rpE(X) = \\frac{r}{p}E(X)=pr​ variance var(X)=r(1−p)p2var(X) = \\frac{r(1-p)}{p^2}var(X)=p2r(1−p)​ the conduct method can be seen there. Hypergeometric distribution Suppose that an urn contains nnn balls, of which rrr are black and n−rn-rn−r are white. Let XXX denote the number of black balls drawn when taking mmm balls without replacement. denoted as X∼h(m,n,r)X\\sim h(m,n,r)X∼h(m,n,r) pmf fX(k)=P(X=k)={Ck−1r−1pr(1−p)k−r for k=r,r+1,r+2…0 otherwise  f_{X}(k)=P(X=k)=\\left\\{\\begin{array}{cl} C_{k-1}^{r-1} p^{r}(1-p)^{k-r} & \\text { for } \\mathrm{k}=\\mathrm{r}, \\mathrm{r}+1, \\mathrm{r}+2 \\ldots \\\\ 0 & \\text { otherwise } \\end{array}\\right. fX​(k)=P(X=k)={Ck−1r−1​pr(1−p)k−r0​ for k=r,r+1,r+2… otherwise ​ expectation E(X)=mrnE(X) = m\\frac{r}{n}E(X)=mnr​ variance var(X)=mr(n−m)(n−r)n2(n−1)var(X) = \\frac{mr(n-m)(n-r)}{n^2(n-1)}var(X)=n2(n−1)mr(n−m)(n−r)​ Poisson distribution can be derived as the limit of a binomial distribution as the number of trials approaches infinity and the probability of success on each trial approaches zero in such a way that np=λnp = \\lambdanp=λ,λ\\lambdaλ can be seen as the successful trials pmf P(X=k)=λkk!e−λk=0,1,2...P(X = k) = \\frac{\\lambda^k }{k!} e^{-\\lambda} \\quad k = 0,1,2...P(X=k)=k!λk​e−λk=0,1,2... expectation E(X)=λE(X) = \\lambdaE(X)=λ variance var(X)=λvar(X) = \\lambdavar(X)=λ Property Let XXX and YYY are independent Poisson r.v.s with parameters θ1\\theta_1θ1​ and θ2\\theta_2θ2​, and X+Y∼Possion(θ1+θ2)X+Y \\sim Possion(\\theta_1 + \\theta_2)X+Y∼Possion(θ1​+θ2​) Continuous Uniform distribution A uniform r.v on the interval [a,b] is a model for what we mean when we say \"choose a number at random between a and b\" pdf fX(x)={1b−aa≤x≤b0 otherwise  f_{X}(x)=\\left\\{\\begin{aligned} \\frac{1}{b-a} & a \\leq x \\leq b \\\\ 0 & \\text { otherwise } \\end{aligned}\\right. fX​(x)=⎩⎨⎧​b−a1​0​a≤x≤b otherwise ​ cdf(easy to get) FX(x)={0x≤ax−ab−aa≤x≤b1b≤x F_{X}(x)=\\left\\{\\begin{array}{rl} 0 & x \\leq a \\\\ \\frac{x-a}{b-a} & a \\leq x \\leq b \\\\ 1 & b \\leq x \\end{array}\\right. FX​(x)=⎩⎨⎧​0b−ax−a​1​x≤aa≤x≤bb≤x​ expectation E(X)=a+b2E(X) = \\frac{a+b}{2}E(X)=2a+b​ variance var(X)=(b−a)212var(X) = \\frac{(b-a)^2}{12}var(X)=12(b−a)2​ Exponential distribution Exponential distribution is often used to model lifetimes or waiting times, in which context it is conventional to replace xxx by ttt. pdf fX(x)={λe−λxx≥00 otherwise  f_{X}(x)=\\left\\{\\begin{array}{rl} \\lambda e^{-\\lambda x} & x \\geq 0 \\\\ 0 & \\text { otherwise } \\end{array}\\right. fX​(x)={λe−λx0​x≥0 otherwise ​ cdf(easy to get) FX(x)={1−e−λxx≥00 otherwise  F_{X}(x)=\\left\\{\\begin{array}{rl} 1-e^{-\\lambda x} & x \\geq 0 \\\\ 0 & \\text { otherwise } \\end{array}\\right. FX​(x)={1−e−λx0​x≥0 otherwise ​ expectation E(X)=1λE(X) = \\frac{1}{\\lambda}E(X)=λ1​ variance var(X)=1λ2var(X) = \\frac{1}{\\lambda^2}var(X)=λ21​ property let X,YX,YX,Y are independent Poisson r.v.s with θ1,θ2\\theta_1,\\theta_2θ1​,θ2​,then X+Y∼Poisson(θ1+θ2)X+Y\\sim Poisson (\\theta_1+\\theta_2)X+Y∼Poisson(θ1​+θ2​) Memoryless P(X>s+t∣X>s)=P(X>t)P(X > s+t | X> s) = P(X>t)P(X>s+t∣X>s)=P(X>t) Gamma distribution pdf g(t)={λατ(α)tα−1e−λtt≥00 otherwise  g(t)=\\left\\{\\begin{array}{rl} \\frac{\\lambda^{\\alpha}}{\\tau(\\alpha)} t^{\\alpha-1} e^{-\\lambda t} & t \\geq 0 \\\\ 0 & \\text { otherwise } \\end{array}\\right. g(t)={τ(α)λα​tα−1e−λt0​t≥0 otherwise ​ τ(x)=∫0∞ux−1e−udu,x>0\\tau(x) = \\int _0^\\infty u^{x-1}e^{-u}du,x>0τ(x)=∫0∞​ux−1e−udu,x>0 expectation E(X)=αλE(X) = \\frac{\\alpha}{\\lambda}E(X)=λα​ variance Var(X)=αλ2Var(X)= \\frac{\\alpha}{\\lambda ^2}Var(X)=λ2α​ Property Ga(1,λ)=exp⁡(λ)Ga(1,\\lambda) = \\exp (\\lambda)Ga(1,λ)=exp(λ) Ga(n2,12)=χ2(n)Ga(\\frac{n}{2},\\frac{1}{2}) = \\chi ^2 (n)Ga(2n​,21​)=χ2(n) E(X)=nE(X) = nE(X)=n Var(X)=2nVar(X) = 2nVar(X)=2n X∼Ga(α,λ)→kX∼Ga(α,λk),k>0X\\sim Ga(\\alpha,\\lambda) \\to kX\\sim Ga(\\alpha,\\frac{\\lambda}{k}),k>0X∼Ga(α,λ)→kX∼Ga(α,kλ​),k>0 if X∼Ga(α,λ),Y∼Ga(β,λ),i.i.dX\\sim Ga(\\alpha,\\lambda),Y\\sim Ga(\\beta,\\lambda),i.i.dX∼Ga(α,λ),Y∼Ga(β,λ),i.i.d,then X+Y∼Ga(α+β,λ)X+Y \\sim Ga(\\alpha+\\beta ,\\lambda)X+Y∼Ga(α+β,λ) conduct ∵τ(α)=∫0∞xα−1e−xdx\\because \\tau(\\alpha ) =\\int_{0}^{\\infty} x^{\\alpha-1}e^{-x}dx ∵τ(α)=∫0∞​xα−1e−xdx ∴x=λt,→τ(α)=λα∫0∞tα−1e−λtdt\\therefore x = \\lambda t,\\to \\tau (\\alpha) = \\lambda^\\alpha \\int _{0}^{\\infty} t^{\\alpha-1}e^{-\\lambda t}dt ∴x=λt,→τ(α)=λα∫0∞​tα−1e−λtdt ∴1τ(α)λα∫0∞tα−1e−λtdt=1\\therefore \\frac{1}{\\tau (\\alpha)}\\lambda^\\alpha \\int _{0}^{\\infty} t^{\\alpha-1}e^{-\\lambda t}dt = 1∴τ(α)1​λα∫0∞​tα−1e−λtdt=1 ∴g(t)=λατ(α)tα−1e−λt\\therefore g(t) =\\frac{\\lambda^\\alpha}{\\tau(\\alpha)}t^{\\alpha-1}e^{-\\lambda t} ∴g(t)=τ(α)λα​tα−1e−λt α\\alphaα is called a shape parameter for the gamma density, Varying α\\alphaα changes the shape of the density λ\\lambdaλ is called a scale parameter Varying λ\\lambdaλ corresponds to changing the units of measurement and does not affect the shape of the density how to understand gamma? Normal distribution pdf g(t)={1σ2πe−(x−μ)2/(2σ2)t≥00 otherwise  g(t)=\\left\\{\\begin{aligned} \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-(x-\\mu)^{2} /\\left(2 \\sigma^{2}\\right)} & t \\geq 0 \\\\ 0 & \\text { otherwise } \\end{aligned}\\right. g(t)=⎩⎪⎨⎪⎧​σ2π​1​e−(x−μ)2/(2σ2)0​t≥0 otherwise ​ μ\\muμ is the mean σ\\sigmaσ is the standard deviation If X∼N(μ;σ2)X \\sim N(\\mu; \\sigma^2)X∼N(μ;σ2) ,and Y=aX+bY = aX + bY=aX+b, then Y∼N(aμ+b,a2σ2)Y \\sim N(a\\mu+b,a^2\\sigma^2)Y∼N(aμ+b,a2σ2) especially, if X∼N(μ,σ2)X \\sim N(\\mu,\\sigma^2)X∼N(μ,σ2), then Z=x−μσ∼N(0,1)Z = \\frac{x-\\mu}{\\sigma}\\sim N(0,1)Z=σx−μ​∼N(0,1) aX+bY∼N(aμX+bμY,a2σX2+b2σY2+2abρσXσY)aX+bY \\sim N(a\\mu_X+b\\mu_Y,a^2\\sigma_X^2 + b^2\\sigma_Y^2 + 2ab\\rho \\sigma_X\\sigma_Y)aX+bY∼N(aμX​+bμY​,a2σX2​+b2σY2​+2abρσX​σY​) property if X,Y∼N(0,1)X,Y \\sim N(0,1)X,Y∼N(0,1),then U=XYU = \\frac{X}{Y} U=YX​ is Cauchy r.v (lec3) fU(u)=1π(u2+1)f_U(u) = \\frac{1}{\\pi (u^2+1)}fU​(u)=π(u2+1)1​ if X1,..,Xn∼N(0,1)X_1,..,X_n\\sim N(0,1)X1​,..,Xn​∼N(0,1) ,i.i.d,, then X12+...Xn2∼χ2(n)X_1^2 + ... X_n^2 \\sim \\chi^2(n)X12​+...Xn2​∼χ2(n) Logistic distribution consider the special logistic distribution(0,1): FX(x)=11+e−xF_X(x) = \\frac{1}{1+e^{-x}}FX​(x)=1+e−x1​ Exponential family A family of pdfs or pmfs is called an exponential family if it can be expressed as: p(x,θ)=H(x)exp⁡(θTϕ(x)−A(θ))p(x,\\theta) = H(x)\\exp(\\theta^T \\phi(x) - A(\\theta))p(x,θ)=H(x)exp(θTϕ(x)−A(θ)) H(x)H(x)H(x) is the normalization factor It is very helpful to model heterogeneous data in the era of big data. Bernoulli, Gaussian, Binomial, Poisson, Exponential, Weibull, Laplace, Gamma, Beta, Multinomial, Wishart distributions are all exponential families for Bernoulli: X∼px(1−p)1−x,forx∈{0,1}X\\sim p^x(1-p)^{1-x}, for x\\in \\{0,1\\}X∼px(1−p)1−x,forx∈{0,1} Px(1−P)1−x=exp⁡{xln⁡p+(1−x)ln⁡(1−p)}=exp⁡{ln⁡p1−px+ln⁡(1−p)}P^x(1-P)^{1-x} = \\exp\\{x\\ln p + (1-x)\\ln (1-p)\\} = \\exp\\{\\ln \\frac{p}{1-p} x + \\ln (1-p)\\}Px(1−P)1−x=exp{xlnp+(1−x)ln(1−p)}=exp{ln1−pp​x+ln(1−p)} θ=ln⁡p1−p,ϕ(x)=x,A(θ)=ln⁡11−p,H(x)=1\\theta =\\ln \\frac{p}{1-p}, \\phi(x) = x,A(\\theta ) = \\ln\\frac{1}{1-p},H(x) = 1θ=ln1−pp​,ϕ(x)=x,A(θ)=ln1−p1​,H(x)=1 the explain can be seen here Sample Var(Xˉ)=σ2nVar(\\bar{X} ) = \\frac{\\sigma^2}{n}Var(Xˉ)=nσ2​ (n−1)S2=∑X2−nXˉ2(n-1)S^2 = \\sum X^2 - n\\bar{X}^2(n−1)S2=∑X2−nXˉ2 Xˉ\\bar{X}Xˉ 和S2S^2S2相互独立 Xˉ∼N(μ,σ2n)\\bar{X} \\sim N(\\mu,\\frac{\\sigma^2}{n})Xˉ∼N(μ,nσ2​) (n−1)S2σ2∼χ2(n−1)\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(n-1)σ2(n−1)S2​∼χ2(n−1) Property E(X)=E(E(X∣Y))E(X) = E(E(X|Y))E(X)=E(E(X∣Y)) 可以理解为先分组求期望，与直接求期望一样 Var(X)=E(Var(X∣Y))+Var(E(X∣Y))Var(X) = E(Var(X|Y)) + Var(E(X|Y))Var(X)=E(Var(X∣Y))+Var(E(X∣Y)) 可以理解为组内方差的期望 + 组间方差 if r.v.s X and Y are independent, E(X∣Y)=E(X)E(X|Y) = E(X)E(X∣Y)=E(X) Inequality Markov's inequality P(X≥a)≤E(X)aP(X\\ge a) \\le \\frac{E(X)}{a}P(X≥a)≤aE(X)​ Chebyshev's inequality P(∣X−E(X)∣≥a)≤Var(X)a2P(|X-E(X)| \\ge a) \\le \\frac{Var(X)}{a^2}P(∣X−E(X)∣≥a)≤a2Var(X)​ Chernoff bounds The generic Chernoff bound requires only the moment generating function of XXX, defined as MX(t)=E(etX)M_X(t) = E(e^{tX})MX​(t)=E(etX), provided it exists. P(X≥a)≤E(etx)et⋅aP(X\\ge a) \\le \\frac{E(e^{tx})}{e^{t\\cdot a}}P(X≥a)≤et⋅aE(etx)​ other inequalities can be seen here. "},"sample and limit-theory.html":{"url":"sample and limit-theory.html","title":"Sample And Limit Theory","keywords":"","body":"sampling distribution Mean Xˉ=X1+...+Xnn=1n∑Xi\\bar{X} =\\frac{X_1+...+X_n}{n} = \\frac{1}{n} \\sum X_iXˉ=nX1​+...+Xn​​=n1​∑Xi​ min⁡a∑(x1−a)2=∑(xi−xˉ)2\\min _a \\sum (x_1 -a)^2 = \\sum (x_i -\\bar x)^2mina​∑(x1​−a)2=∑(xi​−xˉ)2 xˉ\\bar {x}xˉ使得xix_ixi​的距离和最短 E(Xˉ)=μE(\\bar{X}) = \\muE(Xˉ)=μ 表示样本均值是一个无偏估计 Var(Xˉ)=σ2nVar(\\bar{X} ) = \\frac{\\sigma^2}{n}Var(Xˉ)=nσ2​ 可以看出，其方差随着nnn的增大减小，也就是说，增大样本量可以使得估计更为准确。 Variance S=1n−1∑(Xi−Xˉ)2S= \\frac{1}{n-1}\\sum (X_i-\\bar{X})^2S=n−11​∑(Xi​−Xˉ)2 (n−1)S2=∑i=1nxi2−nxˉ2(n-1)S^2 = \\sum_{i=1}^n x_i^2 - n\\bar{x}^2(n−1)S2=∑i=1n​xi2​−nxˉ2 只需要扫描一遍数据 E(S2)=σ2E(S^2) = \\sigma^2E(S2)=σ2 表示严格方差为无偏估计 Lemma 若X1,...,XnX_1,...,X_nX1​,...,Xn​ 是来自同一分布的样本，令g(x)g(x)g(x)为它的一个函数，同时，若E(g(X1))E(g(X_1))E(g(X1​))和Var(g(X1))Var(g(X_1))Var(g(X1​))存在，则： E(∑g(Xi))=n⋅E(g(X1))E(\\sum g(X_i)) = n \\cdot E(g(X_1))E(∑g(Xi​))=n⋅E(g(X1​)) Var(∑(Xi))=n⋅Var(g(X1))Var(\\sum (X_i)) = n\\cdot Var(g(X_1))Var(∑(Xi​))=n⋅Var(g(X1​)) 考虑均值矩母函数与随机样本的关系： MXˉ(t)=[MX(tn)]nM_{\\bar{X}}(t) = [M_{X}(\\frac{t}{n})]^nMXˉ​(t)=[MX​(nt​)]n Theorem Xˉ\\bar{X}Xˉ 和S2S^2S2相互独立 Xˉ∼N(μ,σ2n)\\bar{X} \\sim N(\\mu,\\frac{\\sigma^2}{n})Xˉ∼N(μ,nσ2​) (n−1)S2σ2∼χ2(n−1)\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(n-1)σ2(n−1)S2​∼χ2(n−1) 为什么自由度为n−1n-1n−1？ 在用样本方差估计总体方差时会需要用到样本均值，而样本均值就决定了变量值的总数。 Convolution theorem 若XXX和YYY是两个相互独立的连续随机变量，那么Z=X+YZ = X+YZ=X+Y的pdf为： fZz=∫−∞+∞fX(w)fY(z−w)dwf_Z{z} = \\int_{-\\infty}^{+\\infty} f_X(w)f_Y(z-w)dwfZ​z=∫−∞+∞​fX​(w)fY​(z−w)dw Order statistics The order statistics of a random sample X1,...,XnX_1,...,X_nX1​,...,Xn​ are the sample values placed in ascending order. denoted by X(1),...X(n)X_{(1)},...X_{(n)}X(1)​,...X(n)​ Distribution discrete case Define Pi=p1+p2+...+piP_i = p_1 + p_2 + ... + p_iPi​=p1​+p2​+...+pi​,then: P(X(j)≤xi)=∑k=jnCnkPik(1−Pi)n−kP(X_{(j)}\\le x_i )= \\sum_{k=j}^nC_n^k P_i^k(1-P_i)^{n-k}P(X(j)​≤xi​)=∑k=jn​Cnk​Pik​(1−Pi​)n−k continuous case fX(j)(x)=n!(j−1)!(n−j)!fX(x)[FX(x)]j−1[1−FX(x)]n−jf_{X_{(j)}}(x) = \\frac{n!}{(j-1)!(n-j)!}f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}fX(j)​​(x)=(j−1)!(n−j)!n!​fX​(x)[FX​(x)]j−1[1−FX​(x)]n−j Joint distribution fX(i),X(j)(u,v)=n!(i−1)!(j−1−i)!(n−j)!fX(u)fX(v)[FX(u)]i−1[FX(v)−FX(u)]j−i−1[1−FX(v)]n−jf_{X_{(i)},X_{(j)}}(u,v) = \\frac{n!}{(i-1)!(j-1-i)!(n-j)!} f_X(u)f_X(v)[F_X(u)]^{i-1}[F_X(v) - F_X(u)]^{j-i-1}[1-F_X(v)]^{n-j}fX(i)​,X(j)​​(u,v)=(i−1)!(j−1−i)!(n−j)!n!​fX​(u)fX​(v)[FX​(u)]i−1[FX​(v)−FX​(u)]j−i−1[1−FX​(v)]n−j Limit theory Convergence in probability A sequence of X1,X2,...,X_1,X_2,...,X1​,X2​,..., converges in probability to a r.v XXX, if for every ϵ>0\\epsilon >0ϵ>0: lim⁡n→∞P(∣Xn−X∣≥ϵ)=0\\lim\\limits_{n\\to \\infty} P(|X_n - X| \\ge \\epsilon ) = 0n→∞lim​P(∣Xn​−X∣≥ϵ)=0 可以使用切比雪夫不等式证明，样本均值依概率收敛到0 Weak law of large numbers Let X1,X2,..X_1,X_2,..X1​,X2​,.. be i.i.d. with E(Xi)=μE(X_i) = \\muE(Xi​)=μ and Var(Xi)=θ2∞Var(X_i) = \\theta^2 Var(Xi​)=θ2∞: lim⁡n→∞P(∣Xˉ−μ∣ϵ)=1\\lim\\limits_{n\\to \\infty} P(|\\bar{X} -\\mu | n→∞lim​P(∣Xˉ−μ∣ϵ)=1 可以使用切比雪夫不等式证明，样本方差依概率收敛到0，符合弱大数定理 Almost sure convergence A sequence of X1,X2,...,X_1,X_2,...,X1​,X2​,..., converges almost to a r.v XXX, if for every ϵ>0\\epsilon >0ϵ>0: P(lim⁡n→∞∣Xnˉ−X∣ϵ)=1P(\\lim\\limits_{n\\to \\infty}|\\bar{X_n}-X|P(n→∞lim​∣Xn​ˉ​−X∣ϵ)=1 几乎处处收敛一定是依概率收敛 Strong law of large numbers Let X1,X2...X_1,X_2...X1​,X2​... be i.i.d. r.v.s with E(Xi)=μE(X_i) = \\muE(Xi​)=μ and Var(Xi)=θ2∞Var(X_i) = \\theta^2 Var(Xi​)=θ2∞: P(lim⁡n→∞∣Xnˉ−μ∣ϵ)=1P(\\lim\\limits_{n\\to \\infty}|\\bar{X_n}-\\mu|P(n→∞lim​∣Xn​ˉ​−μ∣ϵ)=1 Convergence in distribution A sequence of X1,X2,...,X_1,X_2,...,X1​,X2​,..., converges in distribution to a r.v XXX, if for every ϵ>0\\epsilon >0ϵ>0: lim⁡n→∞FXn(x)=FX(x)\\lim\\limits_{n\\to \\infty} F_{X_{n}} (x) = F_{X}(x)n→∞lim​FXn​​(x)=FX​(x) 依分布收敛最弱 若满足依概率收敛，一定满足依分布收敛 Central limit theorem Let X1,X2...X_1,X_2...X1​,X2​... be a sequence of i.i.d r.v.s whose mgfs exist in a neighborhood of 0. Let E(Xi)=μE(X_i)= \\muE(Xi​)=μ and Var(Xi)=σ2>0Var(X_i) = \\sigma^2>0Var(Xi​)=σ2>0. n(Xnˉ−μ)σ∼N(0,1)\\frac{\\sqrt{n}(\\bar{X_n}-\\mu)}{\\sigma} \\sim N(0,1)σn​(Xn​ˉ​−μ)​∼N(0,1) Slutsky's Theorem Let Xn→XX_n \\to XXn​→X in distribution and Yn→aY_n \\to aYn​→a,a constant, in probability, then: YnXn→aXY_nX_n \\to aXYn​Xn​→aX in distribution Xn+Yn→X+aX_n+Y_n \\to X + aXn​+Yn​→X+a in distribution 这告诉我们，乘积和极限可以交换位置。因此不难得到： n(Xnˉ−μ)Sn=σSnn(Xnˉ−μ)σ→N(0,1)\\frac{\\sqrt{n}(\\bar{X_n}-\\mu)}{S_n} = \\frac{\\sigma}{S_n}\\frac{\\sqrt{n}(\\bar{X_n}-\\mu)}{\\sigma} \\to N(0,1)Sn​n​(Xn​ˉ​−μ)​=Sn​σ​σn​(Xn​ˉ​−μ)​→N(0,1) "},"T and F distribution.html":{"url":"T and F distribution.html","title":"T And F Distribution","keywords":"","body":"χ2\\chi^2χ2 Distribution If ZZZ is a standard normal r.v., the distribution of U=Z2U = Z^2U=Z2 is called the chi-square distribution with 1 degree of freedom. If U1,U2...U_1,U_2...U1​,U2​... are independent chi-square r.v. with 1 degree of freedom, the distribution of V=U1+U2+...V = U_1+U_2+...V=U1​+U2​+... is called the chi-square distribution with n degrees of freedom and is denoted by χn2\\chi_n^2χn2​ (n−1)S2σ2\\frac{(n-1)S^2}{\\sigma^2}σ2(n−1)S2​ has a χ2(n−1)\\chi^2(n-1)χ2(n−1) distribution ttt distribution If Z∼N(0,1)Z \\sim N(0, 1)Z∼N(0,1) and U∼χn2U \\sim \\chi^2_nU∼χn2​ and UUU are independent, then the distribution of ZU/n\\frac{Z}{\\sqrt{U/n}}U/n​Z​ is called the ttt distribution with n degrees of freedom. if X1,...,XnX_1,...,X_nX1​,...,Xn​ are a random sample from a N(μ,σ2)N(\\mu,\\sigma^2)N(μ,σ2), we know that Xˉ−μσ/n∼N(0,1)\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)σ/n​Xˉ−μ​∼N(0,1) but if σ\\sigmaσ is unknown, we can use SSS to replace σ\\sigmaσ, and Xˉ−μSn/n∼t(n−1)\\frac{\\bar{X} - \\mu}{S_n/\\sqrt{n}} \\sim t(n-1)Sn​/n​Xˉ−μ​∼t(n−1) t-distribution converges to a normal distribution if the freedom degree approaches to infinity. Property t(1)∼Cauchyt(1) \\sim Cauchyt(1)∼Cauchy E(t(p))=0E(t(p)) = 0E(t(p))=0 Var(T(p))=pp−2Var(T(p)) = \\frac{p}{p-2}Var(T(p))=p−2p​ FFF distribution Let UU U and VVV be independent χ2\\chi^2χ2 r.v. with mmm and nnn degrees of freedom, respectively. The distribution of W=U/mV/nW = \\frac{U/m}{V/n}W=V/nU/m​ is call the FFF distribution with mmm and nnn degrees of freedom and is denoted by Fm,nF_{m,n}Fm,n​ U=(n−1)Sx2σx2∼χ2(n−1)U = \\frac{(n-1)S_x^2}{\\sigma ^2_x} \\sim \\chi^2(n-1)U=σx2​(n−1)Sx2​​∼χ2(n−1) V=(n−1)Sy2σy2∼χ2(m−1)V = \\frac{(n-1)S_y^2}{\\sigma ^2_y} \\sim \\chi^2(m-1)V=σy2​(n−1)Sy2​​∼χ2(m−1) U/n−1V/m−1=Sx2/σx2Sy2/σy2∼F(m,n)\\frac{U/n-1}{V/m-1} = \\frac{S_x^2 / \\sigma ^2_x}{S_y^2 / \\sigma ^2_y} \\sim F(m,n)V/m−1U/n−1​=Sy2​/σy2​Sx2​/σx2​​∼F(m,n) Property A variance ratio may have an F distribution even if the parent populations are not normal. It is enough that their pdf are symmetry functions. if X∼Fp,q,X \\sim F_{p,q} , X∼Fp,q​,then 1X∼Fq,p\\frac{1}{X} \\sim F_{q,p}X1​∼Fq,p​ if X∼t(q)X \\sim t(q)X∼t(q), then X2∼F1,qX^2 \\sim F_{1,q}X2∼F1,q​ "},"Survey Sampling.html":{"url":"Survey Sampling.html","title":"Survey Sampling","keywords":"","body":"Population Let the population be of size NNN. These numerical values will be denoted by x1;x2;...;xNx_1; x_2; ...; x_Nx1​;x2​;...;xN​. μ=1N∑xi\\mu = \\frac{1}{N}\\sum x_iμ=N1​∑xi​ τ=Nμ\\tau = N\\muτ=Nμ σ2=1N∑(xi−μ)2=1N∑xi2−μ2\\sigma^2 = \\frac{1}{N} \\sum(x_i - \\mu)^2 = \\frac{1}{N}\\sum x_i^2 -\\mu^2σ2=N1​∑(xi​−μ)2=N1​∑xi2​−μ2 Simple Random Sampling We will denote the sample size by nnn (nnn is less than NNN) and the values of the sample members by X1;X2;...;XnX_1; X_2; ...; XnX1​;X2​;...;Xn. μ^=Xˉ=1n∑Xi\\hat{\\mu} = \\bar{X} = \\frac{1}{n} \\sum X_iμ^​=Xˉ=n1​∑Xi​ τ^=NXˉ\\hat{\\tau} = N\\bar{X}τ^=NXˉ Sampling Mean E(Xˉ)=μE(\\bar{X} ) = \\muE(Xˉ)=μ Var(Xˉ)=σ2nVar(\\bar{X}) = \\frac{\\sigma^2} {n}Var(Xˉ)=nσ2​ if sampling without replacement（default）: Var(Xˉ)=σ2n(1−n−1N−1)Var(\\bar{X}) = \\frac{\\sigma^2}{n} (1- \\frac{n-1}{N-1})Var(Xˉ)=nσ2​(1−N−1n−1​) σXˉ≈σn\\sigma_{\\bar{X}} \\approx \\frac{\\sigma}{\\sqrt{n}}σXˉ​≈n​σ​ it is easy to see, the larger nnn is, the small σX^\\sigma_{\\hat{X}}σX^​ is, hence more accuracy Estimation of the population σ\\sigmaσ σ2\\sigma^2σ2 σ^2=1n∑Xi2−Xˉ2\\hat{\\sigma}^2 = \\frac{1}{n} \\sum X_i^2 - \\bar{X}^2σ^2=n1​∑Xi2​−Xˉ2 E(σ^2)=σ2(n−1n)NN−1E(\\hat{\\sigma}^2) = \\sigma^2 (\\frac{n-1}{n}) \\frac{N}{N-1}E(σ^2)=σ2(nn−1​)N−1N​, this is a biased estimate of σ2\\sigma^2σ2 so the unbiased estimate of σ2\\sigma^2σ2 is 1n−1N−1N∑(Xi−Xˉ)2\\frac{1}{n-1}\\frac{N-1}{N} \\sum (X_i - \\bar{X})^2n−11​NN−1​∑(Xi​−Xˉ)2 Var(Xˉ)Var(\\bar{X})Var(Xˉ) Var(Xˉ)=σ2n(1−n−1N−1)Var(\\bar{X}) = \\frac{\\sigma^2}{n} (1- \\frac{n-1}{N-1})Var(Xˉ)=nσ2​(1−N−1n−1​) An unbiased estimate of Var(Xˉ)Var(\\bar{X})Var(Xˉ) is : sXˉ2=σ^n(nn−1)(N−1N)(N−nN−1)s_{\\bar{X}}^2 = \\frac{\\hat{\\sigma}}{n} (\\frac{n}{n-1}) (\\frac{N-1}{N}) (\\frac{N-n}{N-1})sXˉ2​=nσ^​(n−1n​)(NN−1​)(N−1N−n​) E(sXˉ2)=Var(Xˉ)E(s_{\\bar{X}}^2) = Var(\\bar{X})E(sXˉ2​)=Var(Xˉ) 注意，这里是总体均值的估计量Xˉ\\bar{X}Xˉ，其方差的无偏估计，目的是看该估计量的波动大小。 Random Sampling Direct model Pseudo-random number generation how to get random distribution in computer? using uniform distribution to get Box-muller algorithm Aliasing sample 在计算机处理中，找范围需要log⁡n\\log nlogn的时间，而别名采样只需要常数时间。 Indirect model Reject sampling 尽量选择最小的能包含原分布的分布，能使得拒绝的次数减少。 证明 Stratified sampling population μ\\muμ and σ2\\sigma^2σ2 The population mean and variance of the lth stratum are denoted by μl\\mu_lμl​ and σl2\\sigma^2_lσl2​ μ=1N∑l=1L∑i=1Nixil=∑l=1LWlμl\\mu = \\frac{1}{N}\\sum_{l=1}^{L}\\sum_{i=1}^{N_i}x_{il} = \\sum_{l=1}^{L} W_l\\mu_lμ=N1​∑l=1L​∑i=1Ni​​xil​=∑l=1L​Wl​μl​ sample μ\\muμ and σ2\\sigma^2σ2 ll l strata mean: Xlˉ=1nl∑i=1nlXil\\bar{X_l} = \\frac{1}{n_l}\\sum\\limits_{i=1}^{n_l}X_{il}Xl​ˉ​=nl​1​i=1∑nl​​Xil​ ll l strata variance: sl2=1nl−1∑i=1nl(Xil−Xlˉ)2s_l^2 = \\frac{1}{n_l-1}\\sum\\limits_{i=1}^{n_l}(X_{il} - \\bar{X_l})^2sl2​=nl​−11​i=1∑nl​​(Xil​−Xl​ˉ​)2 the obvious estimate of μ\\muμ Xsˉ=∑l=1LNlXlˉN=∑l=1LWlXˉl\\bar{X_s} = \\sum\\limits_{l=1}^{L}\\frac{N_l\\bar{X_l}}{N} = \\sum\\limits_{l=1}^{L}W_l \\bar X_lXs​ˉ​=l=1∑L​NNl​Xl​ˉ​​=l=1∑L​Wl​Xˉl​ it is an unbiased estimator of μ\\muμ The variance of the stratied sample mean is given by: Var(Xsˉ)=∑l=1LWl2σl2nl(1−nl−1Nl−1)Var(\\bar{X_s}) = \\sum\\limits_{l=1}^{L}W_l^2\\frac{\\sigma_l^2}{n_l}(1 - \\frac{n_l-1}{N_l-1})Var(Xs​ˉ​)=l=1∑L​Wl2​nl​σl2​​(1−Nl​−1nl​−1​) If the sampling fractions within all strata are small: Var(Xsˉ)≈∑l=1LWl2σl2nlVar(\\bar{X_s}) \\approx \\sum\\limits_{l=1}^{L}W_l^2\\frac{\\sigma_l^2}{n_l}Var(Xs​ˉ​)≈l=1∑L​Wl2​nl​σl2​​ using sl2s_l^2sl2​ to replace σl2\\sigma_l^2σl2​, the estimator of Var(Xsˉ)Var(\\bar{X_s})Var(Xs​ˉ​): sXsˉ2=∑l=1LWl2sl2nl(1−nl−1Nl−1)s_{\\bar{X_s}}^2 = \\sum\\limits_{l=1}^L W_l^2 \\frac{s_l^2}{n_l}(1-\\frac{n_l-1}{N_l-1})sXs​ˉ​2​=l=1∑L​Wl2​nl​sl2​​(1−Nl​−1nl​−1​) estimator of population E(Ts)=τE(T_s) = \\tauE(Ts​)=τ Var(Ts)=N2Var(Xsˉ)=N2∑l=1LWl2σl2nl(1−nl−1Nl−1)=∑l=1LNl2σl2nl(1−nl−1Nl−1)Var(T_s) = N^2Var(\\bar{X_s}) = N^2\\sum\\limits_{l=1}^{L}W_l^2\\frac{\\sigma_l^2}{n_l}(1 - \\frac{n_l-1}{N_l-1}) = \\sum\\limits_{l=1}^{L}N_l^2\\frac{\\sigma_l^2}{n_l}(1 - \\frac{n_l-1}{N_l-1}) Var(Ts​)=N2Var(Xs​ˉ​)=N2l=1∑L​Wl2​nl​σl2​​(1−Nl​−1nl​−1​)=l=1∑L​Nl2​nl​σl2​​(1−Nl​−1nl​−1​) methods of allocation ideal The sample sizes n1;...;nLn_1;... ;n_Ln1​;...;nL​ that minimize Var(Xsˉ)Var (\\bar{X_s})Var(Xs​ˉ​) subject to the constraint n1+....+nL=nn_1 + .... + n_L = nn1​+....+nL​=n are given by: nl=nWlσl∑l=1LWkσkn_l = n \\frac{W_l\\sigma_l}{\\sum_{l=1}^{L}W_k\\sigma_k}nl​=n∑l=1L​Wk​σk​Wl​σl​​ 可以发现，每一层的采样个数取决于某一层的方差和个数。 the stratified estimate using the optimal allocations as given in nln_lnl​ and neglecting the finite population correction: Var(Xsoˉ)=(∑Wlσl)2nVar(\\bar{X_{so}}) = \\frac{(\\sum W_l \\sigma _l)^2}{n}Var(Xso​ˉ​)=n(∑Wl​σl​)2​ σl\\sigma_lσl​ unknown if nl=nWln_l = nW_lnl​=nWl​: Var(Xspˉ)=∑l=1LWl2σl2nl=1n∑l=1LWlσl2Var(\\bar{X_{sp}}) = \\sum\\limits_{l=1}^L W_l^2\\frac{\\sigma^2_l}{n_l} = \\frac{1}{n} \\sum\\limits_{l=1}^LW_l\\sigma^2_l Var(Xsp​ˉ​)=l=1∑L​Wl2​nl​σl2​​=n1​l=1∑L​Wl​σl2​ it is easy to see that the different of Var(Xspˉ)Var(\\bar{X_{sp}})Var(Xsp​ˉ​) and Var(Xsoˉ)Var(\\bar{X_{so}}) Var(Xso​ˉ​): Var(Xspˉ)−Var(Xsoˉ)=1n∑l=1LWl(σl−σˉ)2Var(\\bar{X_{sp}}) - Var(\\bar{X_{so}}) = \\frac{1}{n}\\sum\\limits_{l=1}^L W_l(\\sigma_l - \\bar{\\sigma})^2 Var(Xsp​ˉ​)−Var(Xso​ˉ​)=n1​l=1∑L​Wl​(σl​−σˉ)2 where σˉ=∑l=1LWlσl\\bar{\\sigma} = \\sum\\limits_{l=1}^L W_l\\sigma_lσˉ=l=1∑L​Wl​σl​ MCMC We cannot sample directly from the target distribution p(x)p(x)p(x) in the integral f(x)p(x)dxf (x)p(x)dxf(x)p(x)dx. Create a Markov chain whose transition matrix does not depend on the normalization term. Make sure the chain has a stationary distribution π(i)\\pi(i)π(i) and it is equal to the target distribution. After sufficient number of iterations, the chain will converge the stationary distribution. Markov Chain and Random Walk A random process is called a Markov Process if conditional on the current state, its future is independent of its past. P(Xn+1=xn+1∣X0=x0,...,Xn=xn)=P(Xn+1=xn+1∣Xn=xn)P(X_{n+1} = x_{n+1}| X_0 = x_0,...,X_n = x_n) = P(X_{n+1} = x_{n+1}|X_{n} = x_n)P(Xn+1​=xn+1​∣X0​=x0​,...,Xn​=xn​)=P(Xn+1​=xn+1​∣Xn​=xn​) The term Markov property refers to the memoryless property of a stochastic process. A Markov chain XtX_tXt​ is said to be time homogeneous if P(Xs+t=j∣Xs=i)P(X_{s+t} = j | X_s = i)P(Xs+t​=j∣Xs​=i) is independent of s. When this holds, putting s = 0 gives: P(Xs+t=j∣Xs=i)=P(Xt=j∣X0=i)P(X_{s+t} = j| X_s = i) = P(X_t = j| X_0 = i)P(Xs+t​=j∣Xs​=i)=P(Xt​=j∣X0​=i) If moreover P(Xn+1=j∣Xn=i)=PijP(X_{n+1} = j |X_n = i) = P_{i}jP(Xn+1​=j∣Xn​=i)=Pi​j is independent of nnn, then XXX is said to be time homogeneous Markov chain. Transition matrix The transition matrix is an N×NN\\times NN×N matrix of nonnegative entries such that the sum over each row of P(t)P^{(t)}P(t) is 1, since ∀n\\forall n ∀n: ∑yPx,y(t+1)=∑yP[Xt+1=y∣Xt=x]=1\\sum_{y}P_{x,y}^{(t+1)} = \\sum_yP[X_{t+1} = y| X_t = x] =1y∑​Px,y(t+1)​=y∑​P[Xt+1​=y∣Xt​=x]=1 this is not a symmetric matrix using normalized laplacian matrix to calculate the eigenvalues State distribution Let π(t)\\pi(t)π(t) be the state distribution of the chain at time t, that πx(t)=P[Xt=x]\\pi^{(t)}_x =P[X_t = x]πx(t)​=P[Xt​=x]. For a finite chain, π(t)\\pi^{(t)}π(t) is a vector of NNN nonnegative entries such that ∑xπx(t)=1\\sum_x\\pi_x^{(t)} =1∑x​πx(t)​=1. Then, it holds that π(t+1)=π(t)P(t+1)\\pi^{(t+1)} = \\pi^{(t)}P^{(t+1)}π(t+1)=π(t)P(t+1). We apply the law of total probability: πyt+1=∑xP[Xt+1=y∣Xt=x]P[Xt=x]=∑xπx(t)Px,y(t+1)\\pi_y^{t+1} = \\sum_xP[X_{t+1} = y| X_t = x]P[X_t = x] = \\sum_x\\pi_x^{(t)}P_{x,y}^{(t+1)}πyt+1​=x∑​P[Xt+1​=y∣Xt​=x]P[Xt​=x]=x∑​πx(t)​Px,y(t+1)​ A stationary distribution of a finite Markov chain with transition matrix PPP is a probability distribution π\\piπ such that πP=π\\pi P = \\piπP=π. Irreducibility State yyy is accessible from state xxx if it is possible for the chain to visit state yyy if the chain starts in state xxx, in other words, Pn(x,y)>0,∀nP^n{(x,y)} > 0, \\forall nPn(x,y)>0,∀n. State xxx communicates with state yyy if yyy is accessible from xxx and xxx is accessible from yyy. We say that the Markov chain is irreducible if all pairs of states communicates. Aperiodicity The period of a state xxx is the greatest common divisor (gcd), such that dx=gcd{n∣(Pn)x,x>0}d_x = gcd\\{n|(P^n)x,x > 0\\}dx​=gcd{n∣(Pn)x,x>0}. A state is aperiodic if its period is 1. A Markov chain is aperiodic if all its states are aperiodic. Theorem If the states xxx and yyy communicate, then dx=dyd_x = d_ydx​=dy​ . We have (Pn)x,x=0(P^n)x,x = 0(Pn)x,x=0 if nmod(dx)≠0n \\mod(d_x ) \\ne 0nmod(dx​)≠0. Detailed Balance Condition Let X0,X1,...,X_0,X_1, ...,X0​,X1​,..., be an aperiodic Markov chain with transition matrix PPP and distribution π\\piπ. If the following condition holds, π(i)Pij=π(j)P(ji)\\pi(i)P_{ij} = \\pi(j)P_{(ji)}π(i)Pij​=π(j)P(ji)​ then π(x)\\pi(x)π(x) is the stationary distribution of the Markov chain. The above equation is called the detailed balance condition. MCMC algorithm Revising the Markov Chain How to choose α(i,j)\\alpha(i , j)α(i,j) such that π(i)Pijα(i,j)=π(j)Pjiα(j,i)\\pi(i)P_{ij} \\alpha (i , j) = \\pi (j)P_{ji} \\alpha(j,i)π(i)Pij​α(i,j)=π(j)Pji​α(j,i): In terms of symmetry, we simply choose α(i,j)=π(j)Pji,α(j,i)=π(i)Pij\\alpha (i , j) = \\pi(j)P_{ji} ,\\alpha (j,i) = \\pi(i)P_{ij}α(i,j)=π(j)Pji​,α(j,i)=π(i)Pij​ . its easy to see the equation got. 这就是MCMC方法的基本思想：既然马尔科夫链可以收敛于一个平稳分布，如果这个分布恰好是我们需要采样的分布，那么当马尔科夫链在第n步收敛之后，其后续不断生成的序列X(n), X(n+1), ... 就可以当做是采样的样本。 我们构造一个QijQ_{ij}Qij​使得满足细致平衡条件，计算转移概率时，对QQQ的采样可以处理成先对PPP采样（可以用刚才说的均匀分布，或者正态分布也有成熟的方法，即box muller变换），然后把α\\alphaα当成一个接受率的概念，随机采样之后看是否到达α(i,j)=π(j)P(j,i)\\alpha(i,j) = \\pi(j)P(j,i)α(i,j)=π(j)P(j,i)的条件，满足条件说明可以转移。 MCMC Sampling Algorithm 现在我们的目的是要按照转移矩阵QQQ进行跳转， 3步代表先用原来的PPP进行跳转，跳转之后的值假设是xxx。 4步的接受率就是将前后两个状态带入α(i,j)=π(j)P(j,i)\\alpha(i,j) = \\pi(j)P(j,i)α(i,j)=π(j)P(j,i)，如果要跳转，其概率应该为P(x∣x(i))α(i,j)P(x|x^{(i)})\\alpha(i,j)P(x∣x(i))α(i,j)，比以前减小了，且这个概率值恰好为新的QQQ转移矩阵的概率值，满足细致平衡条件。 Metropolis-Hastings algorithm 对于MCMC方法，唯一不好的地方是我们需要考虑α\\alphaα，当α\\alphaα很小的时候，我们需要进行很多次抽样才能前进一步，效率很低，因此，我们是否可以改进一下，使得α\\alphaα增大？ 对于目前第x(i)x^{(i)}x(i)，我们使用平稳分布的条件： π(x(i))P(x(j)∣x(ij))α(i,j)=π(x(j))P(x(i)∣x(j))α(j,i)\\pi(x^{(i)})P(x^{(j)}|x^{(ij)}) \\alpha(i,j) = \\pi(x^{(j)})P(x^{(i)}| x^{(j)})\\alpha(j,i)π(x(i))P(x(j)∣x(ij))α(i,j)=π(x(j))P(x(i)∣x(j))α(j,i) 我们只需要将右边的α\\alphaα除到左边，或者将左边的α\\alphaα除到右边，这样就增大了接受率。因此，现在的接受率为： α(i,j)=min⁡{1,π(x)P(x(i)∣x)P(x(i))P(x∣x(i))}\\alpha(i,j) = \\min \\{1,\\frac{\\pi(x)P(x^{(i)}| x)}{P(x^{(i)})P(x|x^{(i)})}\\}α(i,j)=min{1,P(x(i))P(x∣x(i))π(x)P(x(i)∣x)​} 因此，算法会稍微改变一点： Gibbs Sampling 提高接受率是一种很好的办法，那么有没有一种方法能保证接受率为1呢？ Intuition 考虑一个二维分布： P(x1,y1)P(y2∣x1)=P(x1)P(y1∣x1)P(y2∣x1)P(x_1,y_1)P(y_2|x_1) = P(x_1) P(y_1|x_1)P(y_2|x_1)P(x1​,y1​)P(y2​∣x1​)=P(x1​)P(y1​∣x1​)P(y2​∣x1​) P(x1,y2)P(y1∣x1)=P(x1)P(y2∣x1)P(y1∣x1)P(x_1,y_2)P(y_1|x_1) = P(x_1)P(y_2|x_1)P(y_1|x_1)P(x1​,y2​)P(y1​∣x1​)=P(x1​)P(y2​∣x1​)P(y1​∣x1​) 不难发现： P(x1,y1)P(y2∣x1)=P(x1,y2)P(y1∣x1)P(x_1,y_1)P(y_2|x_1) =P(x_1,y_2)P(y_1|x_1)P(x1​,y1​)P(y2​∣x1​)=P(x1​,y2​)P(y1​∣x1​) 我们可以发现，若沿着坐标轴移动，其分布满足平稳分布。 因此： 很容易拓展到多维情形 优点 不需要事先知道联合概率，用于只知道条件概率的情形 每次接受率都为1 达到平稳分布后，采样点逼近联合概率 "},"Testing and Summarizing.html":{"url":"Testing and Summarizing.html","title":"Testing And Summarizing","keywords":"","body":"假设检验 概念 第一类错误（拒真） 概率为显著性水平，记为α\\alphaα 第二类错误（存伪） 当H0H_0H0​为假时接受的概率为β\\betaβ 当H0H_0H0​为假时拒绝的概率为检验的势，等于1−β1-\\beta1−β 当原假设为真时， 检验统计量的分布为零分布 简单假设 假设中的参数都是确定的 在所有构造的统计量中，基于似然比的统计量有最大的势 要求H0H_0H0​和HAH_AHA​都是简单假设 在同样的检验水平下 uniformly most powerful 若HAH_AHA​是复杂的，H1H_1H1​相对于HAH_AHA​中的每一个假设都是最优的，则H0H_0H0​是一致最优的 公式 设ω0\\omega_0ω0​ 为原假设的参数集合，ω1\\omega_1ω1​是备择假设的参数集合，Ω=ω0∪ω1\\Omega = \\omega_0 \\cup \\omega_1Ω=ω0​∪ω1​，则−2log⁡A-2\\log A−2logA的零分步服从自由度为dim⁡Ω−dim⁡ω0\\dim \\Omega - \\dim \\omega_0dimΩ−dimω0​的卡方分布 例子 多项分布的似然比检验 注意自由度有和为1的约束 posisson散度检验 由于EiE_iEi​都一样，因此可以写成 T=nσ2xˉT = \\frac{n\\sigma^2}{\\bar{x}}T=xˉnσ2​ 其统计量都一样（自由度不一样）： −2log⁡A=2∑Oilog⁡(OiEi)=∑(Oi−Ei)2Ei-2\\log A = 2\\sum O_i\\log (\\frac{O_i}{E_i}) = \\sum \\frac{(O_i-E_i)^2}{E_i}−2logA=2∑Oi​log(Ei​Oi​​)=∑Ei​(Oi​−Ei​)2​ plot 悬挂根图 用估计的概率得到拟合值：nj^=npj^\\hat{n_j} = n\\hat{p_j}nj​^​=npj​^​ 可以发现，pj^\\hat{p_j}pj​^​较大的值含有较大的方差：var(nj−nj^)=npjvar (n_j - \\hat{n_j}) = np_jvar(nj​−nj​^​)=npj​ 需要进行方差稳定性变换： varY=σ2(μ)[f′(μ)]2var Y = \\sigma^2(\\mu)[f'(\\mu)]^2varY=σ2(μ)[f′(μ)]2 因此，一般取nj−nj^\\sqrt{n_j} - \\sqrt{\\hat{n_j}}nj​​−nj​^​​ 悬挂卡方图 nj−nj^nj^\\frac{n_j - \\hat{n_j}}{\\sqrt{\\hat{n_j}}}nj​^​​nj​−nj​^​​ 数据汇总 概念 经验累计分布函数Fn(x)F_n(x)Fn​(x) 期望：E(Fn(x))=F(x)E(F_n(x)) = F(x)E(Fn​(x))=F(x) 方差：1nF(x)(1−F(x))\\frac{1}{n}F(x)(1-F(x))n1​F(x)(1−F(x)) 生存函数 Sn(t)=1−Fn(t)S_n(t) = 1 - F_n(t)Sn​(t)=1−Fn​(t) 危险函数 h(t)=f(t)1−F(t)=−ddtlog⁡S(t)h(t) = \\frac{f(t)}{1- F(t)} = -\\frac{d}{d t}\\log S(t)h(t)=1−F(t)f(t)​=−dtd​logS(t) 核密度估计 假设正太分布，则可以构造wh=1hw(xh)w_h = \\frac{1}{h}w(\\frac{x}{h})wh​=h1​w(hx​) 则概率密度函数fff 的估计为：fh(x)=1n∑wh(x−Xi)f_h(x) = \\frac{1}{n} \\sum w_h(x-X_i)fh​(x)=n1​∑wh​(x−Xi​) 中位数 样本中位数可以作为总体分位数的估计。构造一个中位数的置信区间： (X(k),Xn−k−1)(X_{(k)},X_{n-k-1})(X(k)​,Xn−k−1​) 可以计算这个区间的覆盖概率为：P(X(k)≤η≤X(n−k+1))=1−2P(η≤X(k))P(X_{(k)} \\le \\eta \\le X_{(n-k+1)}) = 1 - 2P(\\eta\\le X_(k))P(X(k)​≤η≤X(n−k+1)​)=1−2P(η≤X(​k)) 由于是否大于中位数都是1/2的概率，因此可使用二项分布：12n∑j=0k−1Cnj=P(Y≤k−1)\\frac{1}{2^n}\\sum\\limits _{j=0}^{k-1} C_n^j = P(Y\\le k-1)2n1​j=0∑k−1​Cnj​=P(Y≤k−1) 截尾均值 按照顺序排列数据，丢掉最小的100α\\alphaα%和最大的100α\\alphaα%，计算剩余数据的算术平均 M估计 min⁡∑Φ(Xi−uσ)\\min \\sum \\Phi (\\frac{X_i - u}{\\sigma})min∑Φ(σXi​−u​) 散度度量 最普通的度量为样本标准差S2S^2S2 若观测时来自正太分布的样本，则：(n−1)s2σ2∼χn−12\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi ^2_{n-1}σ2(n−1)s2​∼χn−12​ 但样本标准差对离群观测比较敏感 四分位差（IQR）（25%与75%分位数的差值） 中位数绝对偏差（∣xi−x^∣|x_i - \\hat{x}|∣xi​−x^∣的中位数）（MAD） 可以将其转为正太分布的σ\\sigmaσ 估计： IQR1.35=MAD0.675=σ\\frac{IQR}{1.35} = \\frac{MAD}{0.675} = \\sigma1.35IQR​=0.675MAD​=σ 皮尔逊相关系数 r=cov(X,Y)σ(X)σ(Y)=∑(xi−xˉ)(yi−y)ˉ)∑(xi−xˉ)2∑(yi−yˉ)2r = \\frac{cov (X,Y)}{\\sigma(X)\\sigma(Y)} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y)})}{\\sqrt{\\sum(x_i - \\bar{x})^2\\sum(y_i - \\bar{y})^2}}r=σ(X)σ(Y)cov(X,Y)​=∑(xi​−xˉ)2∑(yi​−yˉ​)2​∑(xi​−xˉ)(yi​−y)ˉ​)​ 注意，这个系数只能衡量线性关系 两样本比较 两独立样本比较 基于正态的方法 需要假设X/YX/YX/Y都服从正态分布且方差相等，这里只是检验其均值是否有差异 Xˉ−Yˉ∼N(μx−μy,σ2(1n+1m))\\bar{X} - \\bar{Y} \\sim N(\\mu_x-\\mu_y,\\sigma^2(\\frac{1}{n }+ \\frac{1}{m}))Xˉ−Yˉ∼N(μx​−μy​,σ2(n1​+m1​)) 若σ\\sigmaσ已知，则可以构造μx−μy\\mu_x - \\mu_yμx​−μy​ 的置信区间： (Xˉ−Yˉ)±z(α/2)σ1n+1m(\\bar{X} - \\bar{Y}) \\pm z(\\alpha/2) \\sigma\\sqrt{\\frac{1}{n}+ \\frac{1}{m}} (Xˉ−Yˉ)±z(α/2)σn1​+m1​​ 更一般的，σ\\sigmaσ未知时， 计算合并样本方差：sp2=(n−1)sx2+(m−1)xy2m+n−2s_p^2 = \\frac{(n-1)s_x^2 + (m-1)x_y^2} {m+n-2}sp2​=m+n−2(n−1)sx2​+(m−1)xy2​​ 构造ttt 统计量(自由度为m+n−2m+n-2m+n−2 ）：t=(Xˉ−Yˉ)−(μx−μy)sp1n+1mt = \\frac{(\\bar{X} -\\bar{Y})- (\\mu_x -\\mu_y)}{s_p\\sqrt{\\frac{1}{n} + \\frac{1}{m}}}t=sp​n1​+m1​​(Xˉ−Yˉ)−(μx​−μy​)​ 记Xˉ−Yˉ\\bar{X} - \\bar{Y}Xˉ−Yˉ的估计标准差为：sXˉ−Yˉ=sp1n+1ms_{\\bar{X} - \\bar{Y}} = s_p\\sqrt{\\frac{1}{n} + \\frac{1}{m}}sXˉ−Yˉ​=sp​n1​+m1​​ 此时，构造μx−μy\\mu_x - \\mu_yμx​−μy​ 的置信区间： (Xˉ−Yˉ)±t(α/2)sXˉ−Yˉ(\\bar{X} - \\bar{Y}) \\pm t(\\alpha/2)s_{\\bar{X} - \\bar{Y}} (Xˉ−Yˉ)±t(α/2)sXˉ−Yˉ​ 可以证明，这种ttt检验与基于似然比的检验等价 势 势是拒绝假的概率，可以用来确定样本容量的大小 若设备则假设为μX−μY=Δ\\mu_X -\\mu_Y = \\DeltaμX​−μY​=Δ，则对于原假设μX=μy\\mu_X = \\mu_yμX​=μy​的拒绝域为： ∣Xˉ−Yˉ∣>z(α/2)σ2n|\\bar{X} - \\bar{Y}| > z(\\alpha/2)\\sigma\\sqrt{\\frac{2}{n}}∣Xˉ−Yˉ∣>z(α/2)σn2​​ 对备则假设进行标准化后，可以得到落入拒绝域的概率为： 1−Θ[z(α/2)−Δσn2]+Θ[−z(α/2)−Δσn2]1-\\Theta[z(\\alpha/2) - \\frac{\\Delta}{\\sigma}\\sqrt{\\frac{n}{2}}] + \\Theta[-z(\\alpha/2) - \\frac{\\Delta}{\\sigma}\\sqrt{\\frac{n}{2}}]1−Θ[z(α/2)−σΔ​2n​​]+Θ[−z(α/2)−σΔ​2n​​] 非参数检验（Mann-Whitney Test） 问题 用x1…xnx_1…x_nx1​…xn​和y1...ymy_1...y_my1​...ym​ 表示处理1和处理2的观测值，要比较X和Y的分布是否相同 原假设是完全随机（没有效应），当控制组观测的秩和太大或太小时，拒绝原假设。 操作流程： 若有m+nm+nm+n 个实验单元，随机分成实验组和控制组 计算控制组的秩，并根据秩和进行判断 在原假设成立下，我们可以计算控制组秩和的统计量为： E(TY)=mμ=m(m+n+1)2E(T_Y) = m\\mu = \\frac{m(m+n+1)}{2}E(TY​)=mμ=2m(m+n+1)​ Var(TY)=mσ2(N−mN−1)=mn(m+n+1)12Var(T_Y) = m\\sigma^2(\\frac{N-m}{N-1}) = \\frac{mn(m+n+1)}{12} Var(TY​)=mσ2(N−1N−m​)=12mn(m+n+1)​（不放回的抽样） 同时，可以计算XYXXY的个数的统计量UY=TY−m(m+1)2U_Y= T_Y - \\frac{m(m+1)}{2} UY​=TY​−2m(m+1)​： E(UY)=mn2E(U_Y) = \\frac{mn}{2}E(UY​)=2mn​ Var(UY)=mn(m+n+1)12Var(U_Y) = \\frac{mn(m+n+1)}{12}Var(UY​)=12mn(m+n+1)​ 在m和n大于10的时候可以用正态近似，否则需要查表 配对样本比较 基于正态的方法 在方差已知的情况下： E(Dˉ)=μX−μYE(\\bar{D}) = \\mu_X - \\mu_YE(Dˉ)=μX​−μY​ Var(Dˉ)=1nσX2+σY2−2ρσXσYVar(\\bar{D}) = \\frac{1}{n}{\\sigma^2_X + \\sigma^2_Y - 2\\rho\\sigma_X\\sigma_Y}Var(Dˉ)=n1​σX2​+σY2​−2ρσX​σY​ 更一般的，在方差未知的情况下，用样本方差代替总体方差，可得到统计量为： t=Dˉ−μDsDˉ∼t(n−1)t = \\frac{\\bar{D} - \\mu_D}{s_{\\bar{D}}}\\sim t(n-1)t=sDˉ​Dˉ−μD​​∼t(n−1) 其中，sDˉs_{\\bar{D}}sDˉ​为样本差的样本方差，即sX−Y/ns_{X-Y}/\\sqrt{n}sX−Y​/n​ 符号秩检验 若两个配对样本没有差别，那么可以预计一般的DDD是正的，一般的DDD是负的 计算步骤： 计算差DDD，差的绝对值 将差的符号用在秩上，得到符号秩 计算具有正的符号的秩W+W_+W+​ 若W+W_+W+​ 太大或太小，则拒绝原假设 若样本容量超过20，可以用正态近似： E(W+)=n(n+1)4E(W_+) = \\frac{n(n+1)}{4}E(W+​)=4n(n+1)​ Var(W+)=n(n+1)(2n+1)24Var(W_+) = \\frac{n(n+1)(2n+1)}{24}Var(W+​)=24n(n+1)(2n+1)​ "},"The analysis of Variance.html":{"url":"The analysis of Variance.html","title":"The Analysis Of Variance","keywords":"","body":"单因子实验设计 在每个实验组下都进行独立的观测，并比较不同测量值均值之间的差异是否显著。 F检验 模型假设 令YijY_{ij}Yij​表示第i个实验组的第j个观测，首先确定统计模型为： Yij=μ+αi+ϵijY_{ij} = \\mu + \\alpha_i + \\epsilon_{ij} Yij​=μ+αi​+ϵij​ 这里的μ\\muμ是总的均值水平，αi\\alpha_iαi​是第i个实验组的不同效应，ϵij\\epsilon_{ij} ϵij​是第i个实验组中的第j个观测值的随机误差。 假设误差是独立的，服从正态分布，且ϵij∼N(0,σ2)\\epsilon_{ij} \\sim N(0,\\sigma^2)ϵij​∼N(0,σ2)。 对α\\alphaα进行规范化：∑Iαi=0\\sum^I \\alpha_i = 0∑Iαi​=0 我们的原假设是，各组之间的观测没有区别，αi=0\\alpha_i = 0αi​=0 基本性质 Yijˉ\\bar{Y_{ij}}Yij​ˉ​ E(Yijˉ)=μ+αiE(\\bar{Y_{ij}}) = \\mu + \\alpha_iE(Yij​ˉ​)=μ+αi​ Var(Yijˉ)=σ2Var(\\bar{Y_{ij}}) = \\sigma^2Var(Yij​ˉ​)=σ2 Yiˉ=1J∑JYij\\bar{Y_{i}} = \\frac{1}{J}\\sum^JY_{ij}Yi​ˉ​=J1​∑JYij​ E(Yiˉ)=μ+αiE(\\bar{Y_{i}}) = \\mu + \\alpha_iE(Yi​ˉ​)=μ+αi​ Var(Yiˉ)=σ2JVar(\\bar{Y_{i}}) = \\frac{\\sigma^2}{J}Var(Yi​ˉ​)=Jσ2​ Yˉ=1IJ∑I∑JYij\\bar{Y} = \\frac{1}{IJ}\\sum^I\\sum^JY_{ij}Yˉ=IJ1​∑I∑JYij​ E(Yˉ)=μE(\\bar{Y}) = \\muE(Yˉ)=μ Var(Yˉ)=1IJσ2Var(\\bar{Y}) = \\frac{1}{IJ}\\sigma^2Var(Yˉ)=IJ1​σ2 方差分析基于以下的等式： ∑I∑J(Yij−Yˉ)=∑I∑J(Yij−Yiˉ)2+J∑I(Yiˉ−Yˉ)2\\sum^I\\sum^J(Y_{ij} - \\bar{Y}) = \\sum^I\\sum^J(Y_{ij} - \\bar{Y_i})^2 + J\\sum^I(\\bar{Y_i} - \\bar{Y})^2∑I∑J(Yij​−Yˉ)=∑I∑J(Yij​−Yi​ˉ​)2+J∑I(Yi​ˉ​−Yˉ)2 即SSTOT=SSW+SSBSS_{TOT }= SS_W+SS_BSSTOT​=SSW​+SSB​，分别表示组内误差平方和、组间误差平方和。 E(SSW)=I(J−1)σ2E(SS_W ) = I(J-1)\\sigma^2E(SSW​)=I(J−1)σ2 SSW=∑I(J−1)si2SS_W = \\sum^I(J-1)s_i^2SSW​=∑I(J−1)si2​ sp2=SSWI(J−1)s^2_p = \\frac{SSW}{I(J-1)} sp2​=I(J−1)SSW​ E(SSB)=J∑Iαi2+(I−1)σ2E(SS_B) = J\\sum^I \\alpha^2_i + (I-1)\\sigma^2E(SSB​)=J∑Iαi2​+(I−1)σ2 可以发现，若αi=0\\alpha_i = 0αi​=0，则SSW/I(J−1)SS_W/I(J-1)SSW​/I(J−1)和SSB/(I−1)SS_B/(I-1)SSB​/(I−1)的值应该是相近的。因此，我们需要找到比较两个平方和的方法，特别是在原假设成立下的统计量。 统计量 在误差的方差相同的情况下 对于SSWSS_WSSW​，每一个组的样本方差都服从(J−1)Si2σ2∼χ2(J−1)\\frac{(J-1)S^2_i}{\\sigma^2} \\sim \\chi^2(J-1) σ2(J−1)Si2​​∼χ2(J−1)，那么SSWσ2∼χ2(I(J−1))\\frac{SS_W}{\\sigma^2} \\sim \\chi ^2(I(J-1))σ2SSW​​∼χ2(I(J−1)) 对于SSBSS_BSSB​，∑I(Yiˉ−Y)2σ2/J=SSBσ2∼χ2(I−1)\\frac{\\sum^I(\\bar{Y_i}- Y)^2}{\\sigma^2/J} = \\frac{SS_B}{\\sigma^2}\\sim \\chi^2(I-1)σ2/J∑I(Yi​ˉ​−Y)2​=σ2SSB​​∼χ2(I−1) 因此，可以定义F统计量： F=SSB/(I−1)SSW/[I(J−1)]F = \\frac{SS_B/(I-1)}{SS_W/[I(J-1)]}F=SSW​/[I(J−1)]SSB​/(I−1)​ 因此，若原假设成立，F统计量应该接近于1。 多重比较问题 我们使用F统计量可以检验不同的测量均值是否不全相同，但没有比较任意两个测量均值之间是否有差异。 图基方法 可以用来构建所有均值对差异的置信区间 若样本容量是全部相等的，误差服从常数方差的正态分布，则Yiˉ−μi∼N(0,σ2/J)\\bar{Y_i} - \\mu_i \\sim N(0,\\sigma^2/J)Yi​ˉ​−μi​∼N(0,σ2/J)，可以利用sp2/Js^2_p/Jsp2​/J估计这个方差。 构造一个Studentized range distribution： max⁡i1,i2∣(Yi1ˉ−μi1)−(Yi2ˉ−μi2)∣sp/J\\max\\limits_{i_1,i_2}\\frac{|(\\bar{Y_{i1}} - \\mu_{i1}) - (\\bar{Y_{i2}} - \\mu_{i2}) |}{s_p/\\sqrt{J}}i1​,i2​max​sp​/J​∣(Yi1​ˉ​−μi1​)−(Yi2​ˉ​−μi2​)∣​ 可以用来求得置信区间 "},"LDA.html":{"url":"LDA.html","title":"LDA思考与总结","keywords":"","body":"资料 首先推荐一些资料，这些都是我在学习LDA模型时深受启发的资料： 数学模型 首先需要了解Bayes推断的相关概念，并掌握常见的共轭分布。 同时，需要对Beta分布和二项分布有很好的了解，二项分布大家都很熟悉了，这里推荐一篇通俗理解Beta分布的文章：带你理解beta分布 LDA 接下来就可以进军LDA了 文本主题模型之LDA(一) LDA基础 这是我看的第一篇博文，深入浅出，很有启发 LDA数学八卦 LDA 文本建模 这是很出名的系列博文了，在理解数学模型的基础上从物理过程进行了很好的阐述 思考 Bayes推断 Bayes学派认为，如果我们假设一个随机变量服从一定的分布，那么这个分布中的参数也是随机变量。因此，我们可以通过后验分布，也就是得到数据后，来更新我们的参数值，使得模型更加准确。LDA也是一样，我们在没有数据之前有一个假定的分布（Multinomial和Dirichlet），在得到数据后，我们可以分别更新这两个分布中的参数，也就是训练（training）的过程。 为什么需要共轭分布？ 总所周知，理解LDA中最重要的就是理解其中Multinomial与Dirichlet分布为共轭结构，之所以需要这种结构，是因为我们如果能从后验分布（得到数据后）更新原有参数，且在给定数据下，参数服从的分布与原分布相同，那么这种学习就可以不断迭代进行下去。 Multinomial与Dirichlet分布 这两个分布不光为共轭，同时，我们可以很轻松的写出后验分布的参数值： 若假设似然函数为multi(m1,...,mn∣p1,...,pn)multi(m_1,...,m_n|p_1,...,p_n)multi(m1​,...,mn​∣p1​,...,pn​)，先验分布为Dirichlet(p1,...,pn∣α1,...,αn)Dirichlet(p_1,...,p_n|\\alpha_1,...,\\alpha_n)Dirichlet(p1​,...,pn​∣α1​,...,αn​)，其中，α\\alphaα为超参数（认为已知，那么，其后验分布为Dirichlet(p1,...,pn∣m1+α1,...,mn+αn)Dirichlet(p_1,...,p_n|m_1+\\alpha_1,...,m_n+\\alpha_n)Dirichlet(p1​,...,pn​∣m1​+α1​,...,mn​+αn​)，可以简记为Dirichlet(p⃗∣m⃗+α⃗)Dirichlet(\\vec{p}| \\vec{m} + \\vec{\\alpha})Dirichlet(p​∣m+α) 这样优美的性质使得我们很容易求得其后验分布 两个共轭分布过程 LDA过程中涉及到两个相互独立的共轭分布过程 对于每个文档，其topic分布为θ∼Dirichlet\\theta \\sim Dirichletθ∼Dirichlet，而对于该文档中某一位置的词，其属于哪一个topic的概率服从z∼multinomial(θ)z \\sim multinomial(\\theta)z∼multinomial(θ)，因此，在得到数据后，统计每个文档中词的topic，可以更新每个文档的topic分布。 抛开文档，对于每个topic，其词服从βk∼Dirichlet\\beta_k \\sim Dirichletβk​∼Dirichlet，而对于这个topic，我们看到的词服从w∼multinomial(βk)w \\sim multinomial(\\beta_k)w∼multinomial(βk​)，因此，统计每个topic中词出现个数量，可以更新每个topic中词的分布。 采样过程 对于任何一个文档，在先验分布DirichletDirichletDirichlet中采样得到θ\\thetaθ(这里是一个向量，代表每个topic的概率)，再由这些概率得到topic分布（多项式分布，表明每个词为这个topic的概率），然后由多项式分布采样得到每个词的topic。 "},"notes for exam.html":{"url":"notes for exam.html","title":"Notes For Exam","keywords":"","body":"计算概率函数 求范围AAA的概率 例如，已知cdf，求A={(x,y)∣x+y≥1}A = \\{(x,y)| x+ y \\ge1\\}A={(x,y)∣x+y≥1}: P(X+Y≥1)=1−P(X+Y1)P(X+Y\\ge 1)= 1- P(X+Y P(X+Y≥1)=1−P(X+Y1) 巧用求反 画图解决，根据约束条件分别对x,yx,yx,y进行积分 求新随机变量的分布 离散 对于离散变量来说，例如新随机变量U,VU,VU,V，只需要找到与原始变量X,YX,YX,Y的对应关系，然后带入FX,YF_{X,Y}FX,Y​中即可 连续 最常见的方法是使用Jacobian行列式代替，但这要求反函数存在，公式为： J=∣∂x∂u∂x∂v∂y∂u∂y∂v∣J = \\begin{vmatrix}\\frac{\\partial x}{\\partial u} &\\frac{\\partial x}{\\partial v} \\\\\\\\ \\frac{\\partial y}{\\partial u}& \\frac{\\partial y}{\\partial v} \\end{vmatrix}J=∣∣∣∣∣∣​∂u∂x​∂u∂y​​∂v∂x​∂v∂y​​∣∣∣∣∣∣​ fu,v=fX,Y(h1(u,v),h2(u,v))∣J∣f_{u,v} = f_{X,Y}(h_1(u,v),h_2(u,v))|J|fu,v​=fX,Y​(h1​(u,v),h2​(u,v))∣J∣ 注意这里是行列式的绝对值 对于带绝对值的转换，可以分别使用Jacobian行列式，例子见Lec3 34。 X,Y∼N(0,1),U=XY,V=∣Y∣X,Y\\sim N(0,1), U = \\frac{X}{Y}, V= |Y|X,Y∼N(0,1),U=YX​,V=∣Y∣ 对于反函数不存在或者不好求导的函数，则使用累计概率函数的定义求解，例如： find the pdf of Y=X2Y = X^2Y=X2 FY(y)=P(Y≤y)=P(X2≤y)=P(−y≤x≤y)F_Y(y) = P(Y \\le y) = P(X^2 \\le y) = P(-\\sqrt y \\le x \\le \\sqrt y)FY​(y)=P(Y≤y)=P(X2≤y)=P(−y​≤x≤y​) 考点 注意试卷1上的共轭分布，自己再找几个例子多做一下 "}}