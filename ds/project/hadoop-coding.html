
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <title>hadoop编程练习 · Distribution System</title>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.0.0">
        <meta name="author" content="zealscott">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-disqus/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex-plus/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search-pro/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="hadoop-recap.html" />
    
    
    <link rel="prev" href="hadoop2.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://tech.zealscott.com" target="_blank" class="custom-link">Homepage</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    Notes
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../notes/Distribution-File-System-DFS.html">
            
                <a href="../notes/Distribution-File-System-DFS.html">
            
                    
                    Distribution File System DFS
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../notes/mapreduce.html">
            
                <a href="../notes/mapreduce.html">
            
                    
                    MapReduce处理系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../notes/mrcoding.html">
            
                <a href="../notes/mrcoding.html">
            
                    
                    MapReduce编程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="../notes/RPC-and-serialization.html">
            
                <a href="../notes/RPC-and-serialization.html">
            
                    
                    RPC And Serialization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="../notes/spark.html">
            
                <a href="../notes/spark.html">
            
                    
                    Spark 处理框架
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="../notes/sparkcoding.html">
            
                <a href="../notes/sparkcoding.html">
            
                    
                    Spark 编程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="../notes/yarn.html">
            
                <a href="../notes/yarn.html">
            
                    
                    Yarn 资源管理框架
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="../notes/zookeeper.html">
            
                <a href="../notes/zookeeper.html">
            
                    
                    ZooKeeper 元数据管理系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.9" data-path="../notes/pregel.html">
            
                <a href="../notes/pregel.html">
            
                    
                    分布式图处理系统--Pregel
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.10" data-path="../notes/flink.html">
            
                <a href="../notes/flink.html">
            
                    
                    批流融合系统--Flink
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.11" data-path="../notes/beam.html">
            
                <a href="../notes/beam.html">
            
                    
                    批流融合系统--展望
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.12" data-path="../notes/ps.html">
            
                <a href="../notes/ps.html">
            
                    
                    机器学习系统--Parameter Server
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.13" data-path="../notes/graphlab.html">
            
                <a href="../notes/graphlab.html">
            
                    
                    机器学习系统--GraphLab
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.14" data-path="../notes/mahout.html">
            
                <a href="../notes/mahout.html">
            
                    
                    机器学习系统--mahout
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.15" data-path="../notes/stream.html">
            
                <a href="../notes/stream.html">
            
                    
                    流计算系统概述
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    Lab
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="rpc.html">
            
                <a href="rpc.html">
            
                    
                    RPC model in Java
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="hadoop.html">
            
                <a href="hadoop.html">
            
                    
                    hadoop安装与配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="hadoop1.html">
            
                <a href="hadoop1.html">
            
                    
                    hadoop编程实践（一）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="hadoop2.html">
            
                <a href="hadoop2.html">
            
                    
                    hadoop编程实践（二）
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.5" data-path="hadoop-coding.html">
            
                <a href="hadoop-coding.html">
            
                    
                    hadoop编程练习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.6" data-path="hadoop-recap.html">
            
                <a href="hadoop-recap.html">
            
                    
                    Hadoop 编程总结
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.7" data-path="spark-installation.html">
            
                <a href="spark-installation.html">
            
                    
                    spark安装与配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.8" data-path="spark1.html">
            
                <a href="spark1.html">
            
                    
                    spark编程实践
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.9" data-path="spark2.html">
            
                <a href="spark2.html">
            
                    
                    spark编程练习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.10" data-path="spark-recap.html">
            
                <a href="spark-recap.html">
            
                    
                    Spark 编程总结
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.11" data-path="docker1.html">
            
                <a href="docker1.html">
            
                    
                    使用 Docker 配置 hadoop/spark
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.12" data-path="docker2.html">
            
                <a href="docker2.html">
            
                    
                    使用 docker 搭建 spark(2.3.1) 集群
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.13" data-path="zookeeper.html">
            
                <a href="zookeeper.html">
            
                    
                    ZooKeeper配置及简单使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.14" data-path="yarn.html">
            
                <a href="yarn.html">
            
                    
                    Yarn框架下的系统部署
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.15" data-path="storm-installation.html">
            
                <a href="storm-installation.html">
            
                    
                    Storm部署与运行
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.16" data-path="storm-coding.html">
            
                <a href="storm-coding.html">
            
                    
                    Storm编程练习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.17" data-path="storm-summary.html">
            
                <a href="storm-summary.html">
            
                    
                    Storm 编程总结
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.18" data-path="sparksteaming.html">
            
                <a href="sparksteaming.html">
            
                    
                    SparkSteaming使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.19" data-path="flink-installation.html">
            
                <a href="flink-installation.html">
            
                    
                    Flink安装及使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.20" data-path="flink1.html">
            
                <a href="flink1.html">
            
                    
                    Flink编程练习（一）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.21" data-path="flink2.html">
            
                <a href="flink2.html">
            
                    
                    Flink编程练习（二）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.22" data-path="graph-in-hadoop.html">
            
                <a href="graph-in-hadoop.html">
            
                    
                    常用图算法实现--Hadoop
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.23" data-path="graph-in-spark.html">
            
                <a href="graph-in-spark.html">
            
                    
                    常用图算法实现--Spark
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.24" data-path="graph-in-flink.html">
            
                <a href="graph-in-flink.html">
            
                    
                    常用图算法实现--Flink
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.25" data-path="giraph.html">
            
                <a href="giraph.html">
            
                    
                    Giraph配置及使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.26" data-path="flink-iteration.html">
            
                <a href="flink-iteration.html">
            
                    
                    Flink迭代小记
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    Recap
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../recap/fundamental-of-distributed-system.html">
            
                <a href="../recap/fundamental-of-distributed-system.html">
            
                    
                    分布式系统基础
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../recap/batch-system.html">
            
                <a href="../recap/batch-system.html">
            
                    
                    批处理系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../recap/management-system.html">
            
                <a href="../recap/management-system.html">
            
                    
                    支持数据管理的底层系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../recap/streaming-system.html">
            
                <a href="../recap/streaming-system.html">
            
                    
                    流处理系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="../recap/flink.html">
            
                <a href="../recap/flink.html">
            
                    
                    批流融合系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="../recap/graph.html">
            
                <a href="../recap/graph.html">
            
                    
                    图处理系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="../recap/ml.html">
            
                <a href="../recap/ml.html">
            
                    
                    机器学习系统
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.8" data-path="../recap/summary.html">
            
                <a href="../recap/summary.html">
            
                    
                    总结与对比
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >hadoop编程练习</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="&#x57FA;&#x7840;mapreduce">&#x57FA;&#x7840;MapReduce</h1>
<h2 id="&#x95EE;&#x9898;">&#x95EE;&#x9898;</h2>
<p>&#x73ED;&#x7EA7;&#x5B66;&#x751F;&#x6210;&#x7EE9;&#x7684;&#x968F;&#x673A;&#x751F;&#x6210;</p>
<ul>
<li>&#x8F93;&#x5165;&#xFF1A;&#x672C;&#x73ED;&#x540C;&#x5B66;&#x7684;&#x5B66;&#x53F7;</li>
<li>&#x8F93;&#x51FA;&#xFF1A;&lt;&#x5B66;&#x53F7;&#xFF0C;&#x6210;&#x7EE9;&gt;</li>
</ul>
<h2 id="&#x6570;&#x636E;&#x51C6;&#x5907;">&#x6570;&#x636E;&#x51C6;&#x5907;</h2>
<ol>
<li><p>&#x9996;&#x5148;&#x9700;&#x8981;&#x4E00;&#x4E2A;<code>stuID.csv&#x6587;&#x4EF6;</code>&#xFF0C;&#x6BCF;&#x4E00;&#x5217;&#x4E3A;&#x4E00;&#x4E2A;&#x5B66;&#x53F7;&#xFF1A;</p>
<ul>
<li><img src="../images/hadoop30.png" alt="hadoop30"></li>
</ul>
</li>
<li><p>&#x7136;&#x540E;&#x5C06;&#x6587;&#x4EF6;&#x653E;&#x5165;<code>HDFS</code>&#x4E2D;&#xFF1A;</p>
<pre><code class="lang-shell">hdfs dfs put stuID.csv input
</code></pre>
</li>
</ol>
<h2 id="&#x7F16;&#x5199;&#x7A0B;&#x5E8F;">&#x7F16;&#x5199;&#x7A0B;&#x5E8F;</h2>
<pre><code class="lang-java">   <span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
   <span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
   <span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;
   <span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;
   <span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
   <span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;
   <span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;
   <span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;
   <span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   <span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   <span class="hljs-keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;

   <span class="hljs-keyword">import</span> java.io.IOException;
   <span class="hljs-keyword">import</span> java.util.Random;

   <span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Score</span> </span>{

       <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Score</span><span class="hljs-params">()</span> </span>{

       }

       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StuIDMapper</span>
               <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">Object</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">LongWritable</span>, <span class="hljs-title">IntWritable</span>&gt; </span>{

           <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">static</span> IntWritable one = <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>);
           LongWritable ID = <span class="hljs-keyword">new</span> LongWritable();

           <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(Object key, Text value, Context context)</span>
                   <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{

               ID.set(Long.valueOf(value.toString()));

               context.write(<span class="hljs-keyword">this</span>.ID, one);

           }
       }

       <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ScoreReducer</span>
               <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">LongWritable</span>, <span class="hljs-title">IntWritable</span>&gt; </span>{
           <span class="hljs-keyword">private</span> IntWritable score = <span class="hljs-keyword">new</span> IntWritable();
           <span class="hljs-keyword">private</span> Random rand = <span class="hljs-keyword">new</span> Random();

           <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(LongWritable ID, Iterable&lt;IntWritable&gt; values, Context context)</span>
                   <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{
               score.set(rand.nextInt(<span class="hljs-number">100</span>) + <span class="hljs-number">1</span>);
               context.write(ID, <span class="hljs-keyword">this</span>.score);
           }

       }

       <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>{

           org.apache.hadoop.conf.Configuration conf = <span class="hljs-keyword">new</span> Configuration();
           String[] otherArgs = (<span class="hljs-keyword">new</span> GenericOptionsParser(conf, args)).getRemainingArgs();
           <span class="hljs-keyword">if</span> (otherArgs.length &lt; <span class="hljs-number">2</span>) {
               System.err.println(<span class="hljs-string">&quot;Usage: wordcount &lt;in&gt;[&lt;in&gt;...] &lt;out&gt;&quot;</span>);
               System.exit(<span class="hljs-number">2</span>);
           }

           Job job = Job.getInstance(conf, <span class="hljs-string">&quot;student score&quot;</span>);
           job.setJarByClass(Score.class);
           job.setMapperClass(Score.StuIDMapper.class);
           job.setReducerClass(Score.ScoreReducer.class);
           job.setOutputKeyClass(LongWritable.class);
           job.setOutputValueClass(IntWritable.class);

           <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; otherArgs.length - <span class="hljs-number">1</span>; i++) {
               FileInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(otherArgs[i]));
           }

           FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(otherArgs[otherArgs.length - <span class="hljs-number">1</span>]));
           System.exit(job.waitForCompletion(<span class="hljs-keyword">true</span>) ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);
       }

   }
</code></pre>
<ul>
<li>&#x8FD9;&#x91CC;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x968F;&#x673A;&#x6570;&#x7684;&#x751F;&#x6210;<code>new Random()</code></li>
<li>&#x540C;&#x65F6;&#xFF0C;&#x7531;&#x4E8E;&#x5B66;&#x53F7;&#x6BD4;&#x8F83;&#x957F;&#xFF0C;&#x5FC5;&#x987B;&#x7528;<code>LongWritable</code>&#x7C7B;&#x578B;&#x8F93;&#x5165;</li>
</ul>
<h2 id="&#x8FD0;&#x884C;">&#x8FD0;&#x884C;</h2>
<pre><code class="lang-shell">hadoop jar Score.jar input/stuID.csv output
hdfs dfs -ls output/*
</code></pre>
<p><img src="../images/hadoop31.png" alt="hadoop31"></p>
<h1 id="&#x5206;&#x7EC4;mapreduce">&#x5206;&#x7EC4;MapReduce</h1>
<h2 id="&#x95EE;&#x9898;">&#x95EE;&#x9898;</h2>
<p>&#x6C42;&#x5E73;&#x5747;&#x6210;&#x7EE9;&#xFF1A;&#x5C06;&#x5168;&#x73ED;&#x540C;&#x5B66;&#x6BCF;&#x9694;5&#x53F7;&#x5206;&#x4E3A;&#x4E00;&#x7EC4;&#xFF0C;&#x6C42;&#x6BCF;&#x7EC4;&#x7684;&#x5E73;&#x5747;&#x6210;&#x7EE9;</p>
<p>&#x8F93;&#x5165;&#xFF1A; &lt;&#x5B66;&#x53F7;&#xFF0C;&#x6210;&#x7EE9;&gt;</p>
<p>&#x8F93;&#x51FA;&#xFF1A;&lt;&#x7EC4;&#x53F7;&#xFF0C;&#x5E73;&#x5747;&#x5206;&gt;</p>
<h2 id="&#x6570;&#x636E;&#x51C6;&#x5907;">&#x6570;&#x636E;&#x51C6;&#x5907;</h2>
<ol>
<li><p>&#x9996;&#x5148;&#x9700;&#x8981;&#x4E00;&#x4E2A;<code>score.csv&#x6587;&#x4EF6;</code>&#xFF0C;&#x6BCF;&#x4E00;&#x5217;&#x4E3A;&#x5B66;&#x53F7;&#x548C;&#x5B66;&#x751F;&#x6210;&#x7EE9;&#xFF1A;</p>
<ul>
<li><img src="../images/hadoop32.png" alt="hadoop32"></li>
</ul>
</li>
<li><p>&#x7136;&#x540E;&#x5C06;&#x6587;&#x4EF6;&#x653E;&#x5165;<code>HDFS</code>&#x4E2D;&#xFF1A;</p>
<pre><code class="lang-shell">hdfs dfs put score.csv input
</code></pre>
</li>
</ol>
<h2 id="&#x7F16;&#x5199;&#x7A0B;&#x5E8F;">&#x7F16;&#x5199;&#x7A0B;&#x5E8F;</h2>
<pre><code class="lang-java"><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.FloatWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
<span class="hljs-keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;

<span class="hljs-keyword">import</span> java.io.IOException;
<span class="hljs-keyword">import</span> java.util.Random;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GetAvgScore</span> </span>{

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">GetAvgScore</span><span class="hljs-params">()</span> </span>{

    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StuScoreMapper</span>
            <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">IntWritable</span>&gt; </span>{

        IntWritable Group = <span class="hljs-keyword">new</span> IntWritable();
        IntWritable Score = <span class="hljs-keyword">new</span> IntWritable();

        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(Text key, Text value, Context context)</span>
                <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{

            <span class="hljs-comment">// every 5 students as a group,[1,2,3,4,5] =&gt; group1</span>
            Group.set((Integer.parseInt(key.toString()) - <span class="hljs-number">1</span>) / <span class="hljs-number">5</span> + <span class="hljs-number">1</span>);
            Score.set(Integer.parseInt(value.toString()));

            context.write(<span class="hljs-keyword">this</span>.Group, <span class="hljs-keyword">this</span>.Score);

        }
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AvgScoreReducer</span>
            <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">IntWritable</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">FloatWritable</span>&gt; </span>{
        <span class="hljs-keyword">private</span> FloatWritable avgscore = <span class="hljs-keyword">new</span> FloatWritable();

        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(IntWritable Group, Iterable&lt;IntWritable&gt; Score, Context context)</span>
                <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{

            <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;
            <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;
            <span class="hljs-keyword">for</span> (IntWritable val : Score) {
                sum += val.get();
                count++;
            }
            avgscore.set((<span class="hljs-keyword">float</span>) sum / count);
            context.write(Group, avgscore);
        }

    }


    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>{

        org.apache.hadoop.conf.Configuration conf = <span class="hljs-keyword">new</span> Configuration();
<span class="hljs-comment">//        set seperator</span>
        conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="hljs-string">&quot;,&quot;</span>);
        String[] otherArgs = (<span class="hljs-keyword">new</span> GenericOptionsParser(conf, args)).getRemainingArgs();
        <span class="hljs-keyword">if</span> (otherArgs.length &lt; <span class="hljs-number">2</span>) {
            System.err.println(<span class="hljs-string">&quot;Usage: wordcount &lt;in&gt;[&lt;in&gt;...] &lt;out&gt;&quot;</span>);
            System.exit(<span class="hljs-number">2</span>);
        }

        Job job = Job.getInstance(conf, <span class="hljs-string">&quot;student avg score&quot;</span>);
        job.setInputFormatClass(KeyValueTextInputFormat.class);
        job.setJarByClass(GetAvgScore.class);

        job.setMapperClass(GetAvgScore.StuScoreMapper.class);
        job.setReducerClass(GetAvgScore.AvgScoreReducer.class);

        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(IntWritable.class);
        job.setOutputValueClass(FloatWritable.class);

        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; otherArgs.length - <span class="hljs-number">1</span>; i++) {
            FileInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(otherArgs[i]));
        }

        FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(otherArgs[otherArgs.length - <span class="hljs-number">1</span>]));
        System.exit(job.waitForCompletion(<span class="hljs-keyword">true</span>) ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);
    }

}
</code></pre>
<p>&#x8FD9;&#x91CC;&#x9700;&#x8981;&#x6CE8;&#x610F;&#xFF0C;&#x4F7F;&#x7528;<code>KeyValueTextInputFormat</code>&#x4EE3;&#x66FF;&#x4E86;&#x9ED8;&#x8BA4;&#x7684;<code>TextInputFormat</code>&#xFF0C;&#x5728;&#x8FDB;&#x5165;mapper&#x524D;&#x5C31;&#x5BF9;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x4E86;&#x5206;&#x5272;&#x5904;&#x7406;&#x3002;</p>
<p>&#x540C;&#x65F6;&#xFF0C;&#x5728;map&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x5C06;&#x6BCF;&#x4E2A;key&#xFF08;&#x5B66;&#x751F;&#x7684;&#x5B66;&#x53F7;&#xFF09;&#x5F97;&#x5230;&#x5BF9;&#x5E94;&#x7684;&#x7EC4;&#xFF0C;&#x4F5C;&#x4E3A;key&#x4F20;&#x5165;reduce&#x4E2D;&#x8BA1;&#x7B97;&#x3002;</p>
<p>&#x8FD9;&#x91CC;&#x9700;&#x8981;&#x6CE8;&#x610F;&#xFF0C;&#x5FC5;&#x987B;&#x6307;&#x660E;map&#x7684;&#x8F93;&#x51FA;&#x7C7B;&#x578B;&#xFF1A;<code>job.setOutputKeyClass</code></p>
<h2 id="&#x8FD0;&#x884C;">&#x8FD0;&#x884C;</h2>
<pre><code class="lang-java">hadoop jar avgcore.jar input/score.csv output/avgscore/
hdfs dfs -cat output/avgscore<span class="hljs-comment">/*
</span></code></pre>
<p>&#x7ED3;&#x679C;&#x4E3A;&#xFF1A;</p>
<p><img src="../images/hadoop37.png" alt="hadoop37"></p>
<pre><code class="lang-shell">hadoop jar Natural\ join.jar input/person.txt input/address.txt output/natural_join
</code></pre>
<h1 id="&#x81EA;&#x7136;&#x8FDE;&#x63A5;&#xFF08;natural-join&#xFF09;">&#x81EA;&#x7136;&#x8FDE;&#x63A5;&#xFF08;natural join&#xFF09;</h1>
<h2 id="&#x6570;&#x636E;&#x51C6;&#x5907;">&#x6570;&#x636E;&#x51C6;&#x5907;</h2>
<p>&#x6709;&#x4E24;&#x4E2A;&#x6587;&#x4EF6;</p>
<ol>
<li><p>person.txt</p>
<blockquote>
<p>1 Aaron 210000</p>
<p>2 Abbott 214000
3 Abel 221000
4 Abner 215000
5 Abraham 226000
6 Adair 225300
7 Adam 223800
8 Addison 224000
9 Adolph 223001</p>
</blockquote>
</li>
<li><p>address.txt</p>
<blockquote>
<p>210000 Nanjing
214000 Wuxi
221000 Xuzhou
213000 Changzhou</p>
</blockquote>
</li>
</ol>
<p>&#x8981;&#x6C42;&#x4EE5;code&#x4E3A;&#x8FDE;&#x63A5;&#x5C5E;&#x6027;&#xFF0C;&#x5339;&#x914D;&#x51FA;person&#x4E2D;&#x6BCF;&#x4E2A;&#x4EBA;&#x6240;&#x5728;&#x7684;&#x4F4D;&#x7F6E;&#x4FE1;&#x606F;&#xFF1B;&#x6BCF;&#x6761;&#x8BB0;&#x5F55;&#x5404;&#x4E2A;&#x5B57;&#x6BB5;&#x4E4B;&#x95F4;&#x4EE5;&#x7A7A;&#x683C;&#x4E3A;&#x5206;&#x9694;&#x7B26;&#x3002;</p>
<h2 id="&#x7F16;&#x5199;&#x7A0B;&#x5E8F;">&#x7F16;&#x5199;&#x7A0B;&#x5E8F;</h2>
<pre><code class="lang-java"><span class="hljs-keyword">import</span> org.apache.commons.lang.StringUtils;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
<span class="hljs-keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;


<span class="hljs-keyword">import</span> java.io.IOException;
<span class="hljs-keyword">import</span> java.util.ArrayList;
<span class="hljs-keyword">import</span> java.util.List;


<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">natural_join</span> </span>{
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">natural_join</span><span class="hljs-params">()</span> </span>{

    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">joinMapper</span>
            <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">Object</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>&gt; </span>{

        <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String PERSON_FLAG = <span class="hljs-string">&quot;person&quot;</span>;
        <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String ADDRESS_FLAG = <span class="hljs-string">&quot;address&quot;</span>;

        <span class="hljs-keyword">private</span> FileSplit fileSplit;
        <span class="hljs-keyword">private</span> Text outKey = <span class="hljs-keyword">new</span> Text();
        <span class="hljs-keyword">private</span> Text outValue = <span class="hljs-keyword">new</span> Text();


        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(Object key, Text value, Context context)</span>
                <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{

            fileSplit = (FileSplit) context.getInputSplit();
            String filePath = fileSplit.getPath().toString();

            String line = value.toString();
            String[] fields = StringUtils.split(line, <span class="hljs-string">&quot; &quot;</span>);

            <span class="hljs-comment">// &#x5224;&#x65AD;&#x8BB0;&#x5F55;&#x6765;&#x81EA;&#x54EA;&#x4E2A;&#x6587;&#x4EF6;</span>
            <span class="hljs-keyword">if</span> (filePath.contains(PERSON_FLAG)) {
                <span class="hljs-keyword">if</span> (fields.length &lt; <span class="hljs-number">3</span>)
                    <span class="hljs-keyword">return</span>;
                <span class="hljs-comment">//   fields[2] is code</span>
                outKey.set(fields[<span class="hljs-number">2</span>]);
                outValue.set(PERSON_FLAG + <span class="hljs-string">&quot;,&quot;</span> + line);
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (filePath.contains(ADDRESS_FLAG)) {
                <span class="hljs-comment">//   fields[0] is city code</span>
                outKey.set(fields[<span class="hljs-number">0</span>]);
                outValue.set(ADDRESS_FLAG + <span class="hljs-string">&quot;,&quot;</span> + fields[<span class="hljs-number">1</span>]);
            }

            context.write(outKey, outValue);


        }
    }

    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">joinReducer</span>
            <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">Text</span>&gt; </span>{
        <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String PERSON_FLAG = <span class="hljs-string">&quot;person&quot;</span>;
        <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String ADDRESS_FLAG = <span class="hljs-string">&quot;address&quot;</span>;

        <span class="hljs-keyword">private</span> String fileFlag = <span class="hljs-keyword">null</span>;
        <span class="hljs-keyword">private</span> String cityName = <span class="hljs-keyword">null</span>;

        <span class="hljs-keyword">private</span> Text outCity = <span class="hljs-keyword">new</span> Text();
        <span class="hljs-keyword">private</span> Text outPerson = <span class="hljs-keyword">new</span> Text();

        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(Text key, Iterable&lt;Text&gt; values, Context context)</span>
                <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{

            List&lt;String&gt; perosonInfo = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();
            <span class="hljs-keyword">for</span> (Text val : values) {
                String[] fields = StringUtils.split(val.toString(), <span class="hljs-string">&quot;,&quot;</span>);
                fileFlag = fields[<span class="hljs-number">0</span>];
                <span class="hljs-comment">// choose what file it is</span>
                <span class="hljs-keyword">if</span> (fileFlag.equals(ADDRESS_FLAG)) {
                    cityName = fields[<span class="hljs-number">1</span>];
                    outCity.set(cityName);
                } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (fileFlag.equals(PERSON_FLAG)) {
                    perosonInfo.add(fields[<span class="hljs-number">1</span>]);
                }
            }

            <span class="hljs-comment">// Cartesian product</span>
            <span class="hljs-keyword">for</span> (String person : perosonInfo) {
                outPerson.set(person);
                context.write(outPerson, outCity);
            }
        }

    }

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>{

        org.apache.hadoop.conf.Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        conf.set(<span class="hljs-string">&quot;mapreduce.output.textoutputformat.separator&quot;</span>, <span class="hljs-string">&quot; &quot;</span>);
        String[] otherArgs = (<span class="hljs-keyword">new</span> GenericOptionsParser(conf, args)).getRemainingArgs();
        <span class="hljs-keyword">if</span> (otherArgs.length &lt;= <span class="hljs-number">2</span>) {
            System.err.println(<span class="hljs-string">&quot;Usage: natural &lt;in&gt; &lt;in&gt; &lt;out&gt;&quot;</span>);
            System.exit(<span class="hljs-number">2</span>);
        }

        Job job = Job.getInstance(conf, <span class="hljs-string">&quot;natural join&quot;</span>);
        job.setJarByClass(natural_join.class);

        job.setMapperClass(natural_join.joinMapper.class);
        job.setReducerClass(natural_join.joinReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; otherArgs.length - <span class="hljs-number">1</span>; i++) {
            FileInputFormat.addInputPath(job, <span class="hljs-keyword">new</span> Path(otherArgs[i]));
        }

        FileOutputFormat.setOutputPath(job, <span class="hljs-keyword">new</span> Path(otherArgs[otherArgs.length - <span class="hljs-number">1</span>]));
        System.exit(job.waitForCompletion(<span class="hljs-keyword">true</span>) ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);
    }
}
</code></pre>
<p>&#x7A0B;&#x5E8F;&#x903B;&#x8F91;&#x4E3B;&#x8981;&#x662F;&#x4EE5;code&#x4F5C;&#x4E3A;key&#x8FDB;&#x884C;reduce&#xFF0C;&#x5C06;&#x6587;&#x4EF6;&#x6765;&#x6E90;&#x4FE1;&#x606F;&#x653E;&#x5165;value&#x4E2D;&#xFF0C;&#x4F7F;&#x7528;&#x7B1B;&#x5361;&#x5C14;&#x79EF;&#x8FDB;&#x884C;&#x5339;&#x914D;&#x3002;</p>
<p>&#x8FD9;&#x91CC;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x4E00;&#x70B9;&#x7684;&#x662F;&#xFF0C;<code>person</code>&#x4E2D;&#x7684;code&#x53EF;&#x80FD;&#x662F;&#x7F3A;&#x7701;&#x7684;&#xFF0C;&#x56E0;&#x6B64;&#x4E0D;&#x80FD;&#x8FDB;&#x884C;&#x5339;&#x914D;&#xFF01;</p>
<p>&#x540C;&#x65F6;&#xFF0C;&#x9700;&#x8981;&#x6CE8;&#x610F;Reducer&#x5B9A;&#x4E49;&#x7684;<code>perosonInfo</code>&#xFF0C;&#x53EA;&#x80FD;&#x5728;&#x7C7B;&#x91CC;&#x9762;&#x5B9A;&#x4E49;&#xFF0C;&#x4E0D;&#x80FD;&#x5B9A;&#x4E49;&#x6210;&#x7C7B;&#x6210;&#x5458;&#x53D8;&#x91CF;&#x3002;</p>
<h2 id="&#x8FD0;&#x884C;">&#x8FD0;&#x884C;</h2>
<pre><code class="lang-shell">hadoop jar Natural\ join.jar input/person.txt input/address.txt output/natural_join
hdfs dfs -cat output/natural_join/*
</code></pre>
<p><img src="../images/hadoop33.png" alt="hadoop33"></p>
<h1 id="kmeans&#x5B9E;&#x73B0;">Kmeans&#x5B9E;&#x73B0;</h1>
<h2 id="&#x6570;&#x636E;&#x51C6;&#x5907;">&#x6570;&#x636E;&#x51C6;&#x5907;</h2>
<p>&#x8F93;&#x5165;&#x6570;&#x636E;&#xFF08;<code>k-means.dat</code>&#xFF09;&#xFF1A;</p>
<blockquote>
<p>4,400
96,826
606,776
474,866
400,768
2,920
356,766
36,687
-26,824</p>
</blockquote>
<ul>
<li>&#x7B2C;&#x4E00;&#x884C;&#x6807;&#x660E;K&#x7684;&#x503C;&#x548C;&#x6570;&#x636E;&#x4E2A;&#x6570;N, &#x5747;&#x4E3A;&#x6574;&#x5F62;, &#x7531;&quot;,&quot;&#x9694;&#x5F00; (&#x5982; 3,10 &#x8868;&#x793A;K=3, N=10)&#x3002;</li>
<li>&#x4E4B;&#x540E;N&#x884C;&#x4E2D;&#x6BCF;&#x884C;&#x4EE3;&#x8868;&#x4E00;&#x4E2A;&#x4E8C;&#x7EF4;&#x5411;&#x91CF;, &#x5411;&#x91CF;&#x5143;&#x7D20;&#x5747;&#x4E3A;&#x6574;&#x5F62;, &#x7531;&quot;,&quot;&#x9694;&#x5F00; (&#x5982; 1,2 &#x8868;&#x793A;&#x5411;&#x91CF;(1, 2))&#x3002;</li>
</ul>
<p>&#x8F93;&#x51FA;: K&#x884C;, &#x6BCF;&#x884C;&#x662F;&#x4E00;&#x4E2A;&#x805A;&#x7C7B;&#x56FE;&#x5FC3;&#x7684;&#x4E8C;&#x7EF4;&#x5411;&#x91CF;, &#x5411;&#x91CF;&#x5143;&#x7D20;&#x5747;&#x4E3A;&#x6D6E;&#x70B9;&#x578B; (&#x5982; 1.1,2.3)&#x3002;</p>
<h2 id="&#x7F16;&#x7A0B;&#x601D;&#x8DEF;">&#x7F16;&#x7A0B;&#x601D;&#x8DEF;</h2>
<ul>
<li>&#x9996;&#x5148;&#x9700;&#x8981;&#x521D;&#x59CB;&#x5316;&#x4E2D;&#x5FC3;&#x70B9;&#xFF0C;&#x8FD9;&#x91CC;&#x4F7F;&#x7528;&#x524D;&#x56DB;&#x884C;&#x4F5C;&#x4E3A;&#x521D;&#x59CB;&#x4E2D;&#x5FC3;&#x70B9;&#xFF0C;&#x73B0;&#x5C06;&#x5176;&#x5199;&#x5165;<code>cache</code>&#x4E2D;&#x3002;</li>
<li>&#x6BCF;&#x6B21;<code>map</code>&#x65F6;&#x4F7F;&#x7528;<code>setup</code>&#x51FD;&#x6570;&#x8BFB;&#x5165;&#x53D8;&#x91CF;&#x4E2D;&#xFF0C;&#x4F9D;&#x636E;&#x6240;&#x6709;&#x70B9;&#x4E0E;&#x4E2D;&#x5FC3;&#x5E97;&#x7684;&#x8DDD;&#x79BB;&#x9009;&#x62E9;&#x5176;&#x5C5E;&#x4E8E;&#x7684;&#x7C7B;&#x3002;</li>
<li>&#x5728;<code>reduce</code>&#x4E2D;&#xFF0C;&#x6839;&#x636E;&#x7C7B;&#x522B;&#x8FDB;&#x884C;&#x5206;&#x7EC4;&#xFF0C;&#x5BF9;&#x6BCF;&#x7EC4;&#x805A;&#x7C7B;&#x91CD;&#x65B0;&#x9009;&#x62E9;&#x4E2D;&#x5FC3;&#x70B9;&#xFF0C;&#x5C06;&#x4E2D;&#x5FC3;&#x70B9;&#x8F93;&#x51FA;&#x5230;&#x76EE;&#x6807;&#x6587;&#x4EF6;&#x4E2D;&#x3002;</li>
<li>&#x5224;&#x65AD;&#x4E24;&#x6B21;&#x7684;&#x4E2D;&#x5FC3;&#x70B9;&#x662F;&#x5426;&#x6EE1;&#x8DB3;&#x9608;&#x503C;&#x6761;&#x4EF6;&#xFF0C;&#x82E5;&#x4E0D;&#x6EE1;&#x8DB3;&#xFF0C;&#x5219;&#x5C06;&#x65B0;&#x751F;&#x6210;&#x7684;&#x4E2D;&#x5FC3;&#x70B9;&#x79FB;&#x52A8;&#x5230;<code>cache</code>&#x4E2D;&#xFF0C;&#x4F5C;&#x4E3A;&#x4E0B;&#x4E00;&#x6B21;&#x8FED;&#x4EE3;&#x7684;&#x4E2D;&#x5FC3;&#x70B9;&#x3002;</li>
<li>&#x8FED;&#x4EE3;&#x7ED3;&#x675F;&#x7684;&#x6807;&#x5FD7;&#x4E3A;&#xFF1A;&#x6EE1;&#x8DB3;&#x6700;&#x5927;&#x8FED;&#x4EE3;&#x6B21;&#x6570;&#x6216;&#x6EE1;&#x8DB3;&#x9608;&#x503C;&#x6761;&#x4EF6;&#x3002;</li>
</ul>
<h3 id="point&#x7C7B;">Point&#x7C7B;</h3>
<p>&#x7531;&#x4E8E;&#x6211;&#x4EEC;&#x8981;&#x5BF9;&#x70B9;&#x8FDB;&#x884C;&#x64CD;&#x4F5C;&#xFF0C;&#x66F4;&#x65B9;&#x4FBF;&#x7684;&#x65B9;&#x6CD5;&#x662F;&#x5148;&#x65B0;&#x5EFA;&#x4E00;&#x4E2A;Point&#x7C7B;&#xFF1A;</p>
<pre><code class="lang-java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Point</span></span>{
    <span class="hljs-keyword">double</span> x;
    <span class="hljs-keyword">double</span> y;
    Point(){

    }

    Point(<span class="hljs-keyword">double</span> x,<span class="hljs-keyword">double</span> y){
        <span class="hljs-keyword">this</span>.x=x;
        <span class="hljs-keyword">this</span>.y=y;
    }

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">double</span> <span class="hljs-title">EuclideanDis</span><span class="hljs-params">(Point other)</span> </span>{
        <span class="hljs-keyword">double</span> distance = <span class="hljs-number">0</span>;

        distance = Math.pow((<span class="hljs-keyword">this</span>.x - other.getX()),<span class="hljs-number">2</span>) + Math.pow((<span class="hljs-keyword">this</span>.y - other.getY()),<span class="hljs-number">2</span>);

        <span class="hljs-keyword">return</span> Math.sqrt(distance);
    }

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">double</span> <span class="hljs-title">getX</span><span class="hljs-params">()</span>
    </span>{
        <span class="hljs-keyword">return</span> x;
    }
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">double</span> <span class="hljs-title">getY</span><span class="hljs-params">()</span></span>{
        <span class="hljs-keyword">return</span> y;
    }
}
</code></pre>
<h3 id="kmeansmapper">kmeansMapper</h3>
<p>&#x8FD9;&#x91CC;&#x4F7F;&#x7528;<code>Distribution cache</code>&#x5C06;&#x5C0F;&#x6587;&#x4EF6;&#xFF08;&#x4E2D;&#x5FC3;&#x70B9;&#xFF09;&#x5E7F;&#x64AD;&#x51FA;&#x53BB;&#xFF0C;&#x907F;&#x514D;&#x4E86;&#x5927;&#x91CF;&#x7684;&#x6570;&#x636E;&#x79FB;&#x52A8;&#x3002;</p>
<p>&#x6CE8;&#x610F;&#xFF0C;&#x5728;&#x4F7F;&#x7528;Hadoop2.7.3&#x4E2D;&#xFF0C;&#x53EA;&#x80FD;&#x4F7F;&#x7528;&#x65E7;&#x7684;<code>Distribution cache</code>API&#xFF0C;&#x65B0;&#x7684;API&#x4F1A;&#x62A5;&#x9519;&#xFF1A;</p>
<pre><code class="lang-java">String localCacheFiles = context.getLocalCacheFiles()[<span class="hljs-number">0</span>].getName();
BufferedReader br = <span class="hljs-keyword">new</span> BufferedReader(<span class="hljs-keyword">new</span> FileReader(localCacheFiles));
</code></pre>
<p>Mapper&#x7A0B;&#x5E8F;&#x4E3A;&#xFF1A;</p>
<pre><code class="lang-java"><span class="hljs-keyword">import</span> org.apache.commons.lang.StringUtils;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;

<span class="hljs-keyword">import</span> java.io.BufferedReader;
<span class="hljs-keyword">import</span> java.io.FileReader;
<span class="hljs-keyword">import</span> java.io.IOException;
<span class="hljs-keyword">import</span> java.io.InputStreamReader;
<span class="hljs-keyword">import</span> java.net.URI;
<span class="hljs-keyword">import</span> java.util.ArrayList;
<span class="hljs-keyword">import</span> java.util.List;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">kmeansMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Mapper</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>&gt; </span>{

    <span class="hljs-keyword">private</span> List&lt;Point&gt; means;

    <span class="hljs-comment">/**
     * reading the data from the distributed cache
     */</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setup</span><span class="hljs-params">(Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{
        means = <span class="hljs-keyword">new</span> ArrayList&lt;Point&gt;();

<span class="hljs-comment">//</span>
<span class="hljs-comment">//        URI[] cacheFiles  = context.getCacheFiles();</span>
<span class="hljs-comment">//        BufferedReader br = new BufferedReader(new FileReader(cacheFiles[0].toString()));</span>

        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        FileSystem fs = FileSystem.get(conf);
        FSDataInputStream hdfsInStream = fs.open(<span class="hljs-keyword">new</span> Path(<span class="hljs-string">&quot;output/cache/part-r-00000&quot;</span>));
        InputStreamReader isr = <span class="hljs-keyword">new</span> InputStreamReader(hdfsInStream, <span class="hljs-string">&quot;utf-8&quot;</span>);
        BufferedReader br = <span class="hljs-keyword">new</span> BufferedReader(isr);


        String lineString = <span class="hljs-keyword">null</span>;
        <span class="hljs-keyword">while</span>((lineString = br.readLine()) != <span class="hljs-keyword">null</span>){
            String[] keyValue = StringUtils.split(lineString,<span class="hljs-string">&quot;,&quot;</span>);

            Point tmpCluster = <span class="hljs-keyword">new</span> Point(Double.parseDouble(keyValue[<span class="hljs-number">0</span>]),Double.parseDouble(keyValue[<span class="hljs-number">1</span>]));
            means.add(tmpCluster);

        }
        br.close();
    }

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(LongWritable key, Text keyvalue, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException</span>{
        <span class="hljs-comment">// ignore first line</span>
        <span class="hljs-keyword">if</span> (key.get() == <span class="hljs-number">0</span>)
            <span class="hljs-keyword">return</span>;

        String[] keyValue = StringUtils.split(keyvalue.toString(),<span class="hljs-string">&quot;,&quot;</span>);

        String X = keyValue[<span class="hljs-number">0</span>];
        String Y = keyValue[<span class="hljs-number">1</span>];

        Point tmpPoint = <span class="hljs-keyword">new</span> Point(Double.parseDouble(X),Double.parseDouble(Y));

        context.write(<span class="hljs-keyword">new</span> IntWritable(findClosest(tmpPoint)), <span class="hljs-keyword">new</span> Text(X + <span class="hljs-string">&quot;,&quot;</span> + Y));
    }

    <span class="hljs-comment">/**
     * method that returns the closest mean from the point
     * <span class="hljs-doctag">@param</span> value
     * <span class="hljs-doctag">@return</span>
     */</span>
    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> <span class="hljs-title">findClosest</span><span class="hljs-params">(Point value)</span></span>{
        <span class="hljs-keyword">int</span> argmin = <span class="hljs-number">0</span>;
        <span class="hljs-keyword">double</span> minimalDistance = Double.MAX_VALUE ;
        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i&lt;means.size(); i++){
            Point tmpCluster = means.get(i);
            <span class="hljs-keyword">double</span> distance = value.EuclideanDis(tmpCluster);
            <span class="hljs-keyword">if</span>(distance &lt; minimalDistance){
                minimalDistance = distance;
                argmin = i;
            }
        }
        <span class="hljs-keyword">return</span> argmin;
    }
}
</code></pre>
<h3 id="kmeansreducer">kmeansReducer</h3>
<pre><code class="lang-java"><span class="hljs-keyword">import</span> org.apache.commons.lang.StringUtils;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.DoubleWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;

<span class="hljs-keyword">import</span> java.io.IOException;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">kmeansReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Reducer</span>&lt;<span class="hljs-title">IntWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">DoubleWritable</span>, <span class="hljs-title">DoubleWritable</span>&gt; </span>{

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(IntWritable key, Iterable&lt;Text&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException,
            InterruptedException</span>{

        <span class="hljs-keyword">double</span> sumX = <span class="hljs-number">0.0</span>;
        <span class="hljs-keyword">double</span> sumY = <span class="hljs-number">0.0</span>;
        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;
        <span class="hljs-keyword">for</span>(Text value : values){
            String[] keyValue = StringUtils.split(value.toString(),<span class="hljs-string">&quot;,&quot;</span>);
            sumX +=  Double.parseDouble(keyValue[<span class="hljs-number">0</span>]);
            sumY +=  Double.parseDouble(keyValue[<span class="hljs-number">1</span>]);
            count ++;
        }


        context.write(<span class="hljs-keyword">new</span> DoubleWritable(sumX/count), <span class="hljs-keyword">new</span> DoubleWritable(sumY/count));

    }
}
</code></pre>
<h3 id="kmeansmain">kmeansMain</h3>
<p>&#x8FD9;&#x91CC;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x6211;&#x4EEC;&#x77E5;&#x9053;&#x6BCF;&#x6B21;MapReduce&#x751F;&#x6210;&#x7684;&#x6587;&#x4EF6;&#x540E;&#x7F00;&#x4E3A;<code>part-r-00000</code>&#xFF0C;&#x56E0;&#x6B64;&#x9700;&#x8981;&#x6307;&#x5B9A;&#x6587;&#x4EF6;&#x540D;&#x8FDB;&#x884C;&#x79FB;&#x52A8;&#x3002;</p>
<p>&#x540C;&#x65F6;&#xFF0C;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x5728;&#x8BFB;&#x5199;&#x6587;&#x4EF6;&#x65F6;&#xFF0C;&#x9700;&#x8981;&#x6307;&#x5B9A;&#x7F16;&#x7801;&#x683C;&#x5F0F;&#x4E3A;<code>UTF-8</code></p>
<pre><code class="lang-java">
<span class="hljs-keyword">import</span> org.apache.commons.lang.StringUtils;
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.FileSystem;
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.DoubleWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;
<span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
<span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;
<span class="hljs-keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;

<span class="hljs-keyword">import</span> java.io.BufferedReader;
<span class="hljs-keyword">import</span> java.io.IOException;
<span class="hljs-keyword">import</span> java.io.InputStreamReader;
<span class="hljs-keyword">import</span> java.net.URI;
<span class="hljs-keyword">import</span> java.net.URISyntaxException;

<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">kmeansMain</span> </span>{

    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String CACHED_PATH = <span class="hljs-string">&quot;output/cache&quot;</span>;
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String ACTUAL_PATH = <span class="hljs-string">&quot;output/means&quot;</span>;

    <span class="hljs-comment">// directory store acutal result</span>
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String CACHED_MEANS = <span class="hljs-string">&quot;output/cache/part-r-00000&quot;</span>;
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String ACTUAL_MEANS = <span class="hljs-string">&quot;output/means/part-r-00000&quot;</span>;


    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">writeFileByline</span><span class="hljs-params">(String dst, String contents)</span>  <span class="hljs-keyword">throws</span> IOException</span>{
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        Path dstPath = <span class="hljs-keyword">new</span> Path(dst);
        FileSystem fs = dstPath.getFileSystem(conf);
        FSDataOutputStream outputStream = <span class="hljs-keyword">null</span>;

        <span class="hljs-keyword">if</span> (!fs.exists(dstPath)) {
            outputStream = fs.create(dstPath);
        }<span class="hljs-keyword">else</span>{
            outputStream = fs.append(dstPath);
        }
        contents = contents + <span class="hljs-string">&quot;\n&quot;</span>;
        outputStream.write(contents.getBytes(<span class="hljs-string">&quot;utf-8&quot;</span>));
        outputStream.close();
    }

    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> <span class="hljs-title">readFileByLines</span><span class="hljs-params">(String fileName,String meansPath)</span> <span class="hljs-keyword">throws</span> IOException </span>{
        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        FileSystem fs = FileSystem.get(URI.create(fileName), conf);
        FSDataInputStream hdfsInStream = fs.open(<span class="hljs-keyword">new</span> Path(fileName));
        InputStreamReader isr = <span class="hljs-keyword">new</span> InputStreamReader(hdfsInStream, <span class="hljs-string">&quot;utf-8&quot;</span>);
        BufferedReader br = <span class="hljs-keyword">new</span> BufferedReader(isr);
        <span class="hljs-comment">// get first line k</span>
        String line =  br.readLine();
        <span class="hljs-keyword">int</span> k = Integer.parseInt(StringUtils.split(line, <span class="hljs-string">&quot;,&quot;</span>)[<span class="hljs-number">0</span>]);
        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;

        <span class="hljs-keyword">while</span> ((line = br.readLine()) != <span class="hljs-keyword">null</span> &amp;&amp; count &lt; k) {
            writeFileByline(meansPath, line);
            count++;
        }
        <span class="hljs-keyword">return</span> k;
    }


    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> IOException,
            InterruptedException, ClassNotFoundException, URISyntaxException </span>{

        Configuration conf = <span class="hljs-keyword">new</span> Configuration();
        String[] otherArgs = (<span class="hljs-keyword">new</span> GenericOptionsParser(conf, args)).getRemainingArgs();
        <span class="hljs-keyword">if</span> (otherArgs.length &lt; <span class="hljs-number">2</span>) {
            System.err.println(<span class="hljs-string">&quot;Usage: kmeans &lt;in&gt; &lt;out&gt;&quot;</span>);
            System.exit(<span class="hljs-number">2</span>);
        }

        <span class="hljs-keyword">int</span> code = <span class="hljs-number">0</span>;

        Path inputPath = <span class="hljs-keyword">new</span> Path(otherArgs[<span class="hljs-number">0</span>]);
        Path outputDir = <span class="hljs-keyword">new</span> Path(otherArgs[<span class="hljs-number">1</span>] + <span class="hljs-string">&quot;&quot;</span>);

        Path cacheMeansPath = <span class="hljs-keyword">new</span> Path(CACHED_MEANS);
        Path actualMeansPath = <span class="hljs-keyword">new</span> Path(ACTUAL_MEANS);

        Path cachePath = <span class="hljs-keyword">new</span> Path(CACHED_PATH);
        Path actualPath = <span class="hljs-keyword">new</span> Path(ACTUAL_PATH);

        <span class="hljs-keyword">int</span> k = readFileByLines(otherArgs[<span class="hljs-number">0</span>],ACTUAL_MEANS);
        <span class="hljs-keyword">int</span> maxIterations = <span class="hljs-number">500</span>;
        <span class="hljs-keyword">double</span> threshold = <span class="hljs-number">0.000001</span>;


        <span class="hljs-comment">// Delete output if exists</span>
        FileSystem hdfs = FileSystem.get(conf);
        <span class="hljs-keyword">if</span> (hdfs.exists(outputDir))
            hdfs.delete(outputDir, <span class="hljs-keyword">true</span>); <span class="hljs-comment">// recursive delete</span>

        <span class="hljs-keyword">boolean</span> changed = <span class="hljs-keyword">false</span>;
        <span class="hljs-keyword">int</span> counter = <span class="hljs-number">0</span>;

        <span class="hljs-keyword">while</span>(!changed &amp;&amp; counter &lt; maxIterations){

            <span class="hljs-comment">// Delete output if exists</span>
            <span class="hljs-keyword">if</span> (hdfs.exists(cachePath))
                hdfs.delete(cachePath, <span class="hljs-keyword">true</span>);
            <span class="hljs-comment">//moving the previous iteration file to the cache directory</span>
            hdfs.rename(actualPath, cachePath);

            conf.set(<span class="hljs-string">&quot;threshold&quot;</span>, threshold+<span class="hljs-string">&quot;&quot;</span>);
            <span class="hljs-comment">//passing K to the map reduce as a parameter</span>
            conf.set(<span class="hljs-string">&quot;k&quot;</span>, k+<span class="hljs-string">&quot;&quot;</span>);
            conf.set(<span class="hljs-string">&quot;mapreduce.output.textoutputformat.separator&quot;</span>, <span class="hljs-string">&quot;,&quot;</span>);


            Job kmeans = Job.getInstance(conf, <span class="hljs-string">&quot;Kmeans &quot;</span>+ (counter + <span class="hljs-string">&quot;&quot;</span>));

            <span class="hljs-comment">// add cache</span>
            kmeans.addCacheFile(cacheMeansPath.toUri());

            kmeans.setJarByClass(kmeansMapper.class);
            FileInputFormat.addInputPath(kmeans, inputPath);
            <span class="hljs-comment">// set out put path : output/means</span>
            FileOutputFormat.setOutputPath(kmeans, actualPath);

            kmeans.setMapperClass(kmeansMapper.class);
            kmeans.setMapOutputKeyClass(IntWritable.class);
            kmeans.setMapOutputValueClass(Text.class);

            kmeans.setReducerClass(kmeansReducer.class);
            kmeans.setOutputKeyClass(DoubleWritable.class);
            kmeans.setOutputValueClass(DoubleWritable.class);

            <span class="hljs-comment">// Execute job</span>
            code = kmeans.waitForCompletion(<span class="hljs-keyword">true</span>) ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>;

            <span class="hljs-comment">//checking if the mean is stable</span>
            BufferedReader file1Reader = <span class="hljs-keyword">new</span> BufferedReader(<span class="hljs-keyword">new</span> InputStreamReader(hdfs.open(cacheMeansPath)));
            BufferedReader file2Reader = <span class="hljs-keyword">new</span> BufferedReader(<span class="hljs-keyword">new</span> InputStreamReader(hdfs.open(actualMeansPath)));
            <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i&lt;k; i++){
                String[] keyValue1 = file1Reader.readLine().split(<span class="hljs-string">&quot;,&quot;</span>);
                String[] keyValue2 = file2Reader.readLine().split(<span class="hljs-string">&quot;,&quot;</span>);

                Point p1 = <span class="hljs-keyword">new</span> Point(Double.parseDouble(keyValue1[<span class="hljs-number">0</span>]),Double.parseDouble(keyValue1[<span class="hljs-number">1</span>]));
                Point p2 = <span class="hljs-keyword">new</span> Point(Double.parseDouble(keyValue2[<span class="hljs-number">0</span>]),Double.parseDouble(keyValue2[<span class="hljs-number">1</span>]));

                <span class="hljs-keyword">if</span>(p1.EuclideanDis(p2) &lt;= threshold){
                    changed = <span class="hljs-keyword">true</span>;
                }<span class="hljs-keyword">else</span>{
                    changed = <span class="hljs-keyword">false</span>;
                    <span class="hljs-keyword">break</span>;
                }
            }
            file1Reader.close();
            file2Reader.close();
            counter++;
            System.out.println(<span class="hljs-string">&quot;KMEANS finished iteration:&gt;&gt; &quot;</span>+counter + <span class="hljs-string">&quot; || means stable: &quot;</span>+ changed);

        }


        hdfs.rename(actualPath, outputDir);

        System.exit(code);

    }

}
</code></pre>
<h2 id="&#x8FD0;&#x884C;">&#x8FD0;&#x884C;</h2>
<p>&#x540C;&#x6837;&#x7684;&#x65B9;&#x6CD5;&#xFF0C;&#x6253;&#x5305;&#x6210;<code>JAR</code>&#x5305;&#x8FD0;&#x884C;&#xFF0C;</p>
<pre><code class="lang-shell">hadoop jar Kmeans.jar input/k-means.dat output/kmeans
</code></pre>
<p>&#x7ED3;&#x679C;&#x4E3A;&#xFF1A;</p>
<p><img src="../images/hadoop34.png" alt="hadoop34"></p>
<p>&#x8BF4;&#x660E;&#x53EA;&#x7528;&#x4E86;8&#x6B21;&#x8FED;&#x4EE3;&#x5C31;&#x8FBE;&#x5230;&#x7A33;&#x5B9A;&#x72B6;&#x6001;&#x4E86;&#x3002;</p>
<p>&#x67E5;&#x770B;&#x8FD0;&#x884C;&#x7ED3;&#x679C;&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x4E2D;&#x5FC3;&#x70B9;&#xFF1A;</p>
<pre><code class="lang-shell">hdfs dfs -cat output/kmeans/*
</code></pre>
<p><img src="../images/hadoop35.png" alt="hadoop35"></p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="hadoop2.html" class="navigation navigation-prev " aria-label="Previous page: hadoop编程实践（二）">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="hadoop-recap.html" class="navigation navigation-next " aria-label="Next page: Hadoop 编程总结">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"hadoop编程练习","level":"1.3.5","depth":2,"next":{"title":"Hadoop 编程总结","level":"1.3.6","depth":2,"path":"project/hadoop-recap.md","ref":"project/hadoop-recap.md","articles":[]},"previous":{"title":"hadoop编程实践（二）","level":"1.3.4","depth":2,"path":"project/hadoop2.md","ref":"project/hadoop2.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","disqus","katex-plus","ga","back-to-top-button","-lunr","-search","search-pro","expandable-chapters"],"ignores":["node_modules","_book"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"disqus":{"shortName":"techzealscott","useIdentifier":false},"github":{"url":"https://github.com/scottdyt"},"search-pro":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"katex-plus":{},"back-to-top-button":{},"ga":{"token":"UA-116370175-1","configuration":"auto"},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"expandable-chapters":{}},"theme":"default","author":"zealscott","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"sortedBy":" ","variables":{},"title":"Distribution System","links":{"sidebar":{"Homepage":"https://tech.zealscott.com"}},"gitbook":"*"},"file":{"path":"project/hadoop-coding.md","mtime":"2021-01-27T07:16:11.999Z","type":"markdown"},"gitbook":{"version":"3.0.0","time":"2021-03-04T09:58:50.487Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/URI.js/1.16.1/URI.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-disqus/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-ga/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/jquery.mark.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search-pro/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters/expandable-chapters.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/buttons.js"></script>
        
    

    </body>
</html>

