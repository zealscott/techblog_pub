{"./":{"url":"./","title":"Introduction","keywords":"","body":"Data Management Systems This GitBook notes are maintained by zealscott. Syllabus Date Description Notes 2.19 Introduction to OLAP 2.26 Kimball dimensional modeling 3.4 MDX&XMLA UDW云数据仓库 3.11 Dimensional model design 历史拉链表 3.18 Dimensional model/ Fact Table/ AG 日志分析 3.25 Procurement case study OLAP列存储 4.1 Order management 基准评测 4.8 Mid-exam 4.15 Accounting 数据仓库之表膨胀 4.22 MDX-1 数据仓库之Json格式 5.5 Customer Relationship Management Hive使用 5.12 Human Resources Management 建立数据仓库模型 5.19 Financial Services Sqoop抽取数据 5.26 Telecommunications 数据清洗 6.3 Transportation 定期装载 6.10 Electronic Commerce 初始装载 6.17 Insurance Hive 优化 Reading Material The Data Warehouse Toolkit:The Definitive Guide to Dimensional Modeling,Third Edition Ucloud document @Last updated at 1/27/2019 "},"sql server.html":{"url":"sql server.html","title":"SQL Server 和 SSBI 使用","keywords":"","body":" 熟悉SQL server使用，熟悉Analysis services环境，学会多维建模 SQL server使用 利用sql server 2008的用户界面创建数据库（Pm_dw）和五张表，即 日期表（Date_Key，Detail_Date, Year, Quarterly, Month）， 客户表(Customer_Key, Customer_Name, Sex, Age, P_Ientity(是否为学生), Income)， 地区表(Locate_Key, Detail_Address, Province, City, Area)， 商品表(Product_Key, Product_Name, Product_Unit_Price, Product_Class), 销售表(Date_Key, Customer_Key, Locate_Key, Product_Key, amount, total_fee)，并且能够合理标识主码和外码。 GUI 创建新的数据库 创建table 创建主键约束，注意，若这里出现无法保存修改，则需要更改SQL server设置。 SQL 首先使用SQL语句创建Table，注意，在SQL server中，必须指定constraint的关系名称。 use master go drop database pm_dw; create database pm_dw; use pm_dw; create table date ( date_key int primary key, detail_date char(10), year char(10), quarterly char(10), month char(10) ) create table customer ( customer_key int primary key, customer_name char(10), sex char(10), age char(10), p_ientity bit, income int ) -- modify table alter table customer add mobile int; create table location ( locate_key int primary key, detail_address char(10), province char(10), city char(10), area char(10) ) create table product ( product_key int primary key, product_name char(10), product_unit_price int, product_class char(10) ) create table sell ( date_key int, product_key int, customer_key int, locate_key int, amount int, total_fee int, CONSTRAINT FK_date foreign key(date_key) references date(date_key), CONSTRAINT FK_product foreign key(product_key) references product(product_key), CONSTRAINT FK_customer foreign key(customer_key) references customer(customer_key), CONSTRAINT FK_location foreign key(locate_key) references location(locate_key), CONSTRAINT PK_sell PRIMARY KEY (date_key,product_key,customer_key,locate_key) ) 尝试插入和修改数据： insert into customer values(1,'xiaowang','male',50,0,5000,17717); insert into customer values(2,'dage','male',25,0,5000,11742); insert into customer values(3,'xiaoxin','female',50,0,5000,11202); insert into customer values(4,'xiaoming','female',25,0,5000,71255); insert into customer values(5,'duyuntao','male',50,0,5000,1421); insert into customer values(6,'qwning','female',25,0,5000,13454); insert into location values(1,'minhang','shanghai','shanghai','ecnu'); insert into location values(2,'minhang','hangzhou','zhejiang','zheda'); insert into location values(3,'minhang','nanjing','jiangsu','nanda'); insert into location values(4,'minhang','beijing','beijing','qinghau'); insert into date values(1,'5:28','2018','4','12'); insert into date values(2,'12:28','2019','4','12'); insert into date values(3,'9:15','2019','3','9'); insert into date values(4,'1:15','2019','1','1'); insert into date values(5,'1:5','2018','1','1'); insert into date values(6,'8:5','2019','2','8'); insert into date values(7,'5:5','2018','2','5'); insert into date values(8,'3:2','2018','1','3'); insert into date values(9,'2:23','2019','1','2'); insert into date values(10,'5:10','2019','3','9'); insert into product values(1,'food1',20,'food'); insert into product values(2,'food2',10,'food'); insert into product values(3,'food3',60,'food'); insert into product values(4,'food4',23,'food'); insert into product values(5,'food5',52,'food'); insert into product values(6,'food6',12,'food'); insert into product values(7,'water1',2,'water'); insert into product values(8,'water2',7,'water'); insert into product values(9,'water3',8,'water'); insert into product values(10,'water4',12,'water'); insert into sell values(1,1,1,1,30,5000); insert into sell values(1,2,3,4,60,6763); insert into sell values(4,3,1,4,15,5346); insert into sell values(5,3,1,3,45,3453); insert into sell values(6,3,1,2,35,4362); insert into sell values(8,3,1,3,25,3456); insert into sell values(9,4,1,2,20,2354); -- update update customer set customer_name='xiaohua' where customer_key=1; update product set product_name='water' where product_class='food'; -- select select customer_name,sex,age from customer where customer_key=1; Analysis services环境 创建项目 打开SQL Server Business Intelligence（SSBI），创建analysis services项目。 定义数据源（数据库中上节课创建的数据库Pm_dw） 创建数据源视图 创建日期维度、客户维度、地区纬度、商品维度。 这里可以根据对应数据选择不同类型的属性 对四个维度表同样操作 创建多维数据集 在这里选择fact table 部署项目(若数据库改变，需要先点击纬度数据的refresh，再在cube中refresh) 分析多维数据集 将创建的项目部署到数据库，在多维数据集中打开浏览器页面，这里可以选择需要进行查询的属性进行操作： 分析每个省份每个季度的销售总金额 分析每个产品每年的总销售量 分析客户每个年龄段的购买情况 分析历年每类商品的销售情况 "},"udw.html":{"url":"udw.html","title":"UDW云数据仓库","keywords":"","body":"熟悉UDW环境 连接远程数据仓库 使用SSH连接远程云主机 Ucloud基于PostgreSQL，很多操作类似，参考文档 使用postgreSQL连接远程数据仓库 psql -U dyt -h udw.lurymd.m0.service.ucloud.cn -p 5432 -d db_dyt -W 查看UDW的表格设计 常用查询命令 列出所有数据库\\l 显示目前数据库的关系表\\d 导入SQL文件进行创建table 简单查询命令 运行JAR包 在本地写好JDBC程序，例如： //package com.database.jdbctest.entity; //import com.database.jdbctest.JDBCOperation; import java.sql.*; public class JDBCTest { public static void main(String args[]) { // Connection op = getAll(); getAll(\"customer\"); // op.insert(new S(\"S6\",\"徐瑞\", \"20\", \"上海\")); // op.insert(new S(\"S7\",\"胡\", \"30\", \"重庆\")); // op.getAll(); // op.update(new S(\"S7\",\"耀艺\", \"\", \"上海\")); // op.delete(\"S6\"); // op.getAll(); // op.SelectCity(\"上海\"); } public static Connection getConn() { String driver = \"org.postgresql.Driver\"; String url = \"jdbc:postgresql://udw.lurymd.m0.service.ucloud.cn:5432/db_dyt\"; String username = \"dyt\"; String password = \"dyt123\"; Connection conn = null; try { Class.forName(driver); // classLoader,加载对应驱动 conn = DriverManager.getConnection(url, username, password); } catch (ClassNotFoundException e) { e.printStackTrace(); } catch (SQLException e) { e.printStackTrace(); } return conn; } /** * @method Integer getAll() 查询并打印表中数据 * @return Integer 查询并打印表中数据 */ public static Integer getAll(String table) { Connection conn = getConn(); String sql = \"select * from \"+table; PreparedStatement pstmt; try { pstmt = (PreparedStatement)conn.prepareStatement(sql); ResultSet rs = pstmt.executeQuery(); int col = rs.getMetaData().getColumnCount(); System.out.println(\"============================\"); // 打印每一列 while (rs.next()) { for (int i = 1; i 放入服务器中并指定JDBC的jar包生成class javac -cp \".:postgresql-jdbc.jar\" JDBCTest.java 运行： java -cp \".:postgresql-jdbc.jar\" JDBCTest "},"history.html":{"url":"history.html","title":"历史拉链表","keywords":"","body":"介绍 历史拉链表 历史拉链表是一种数据模型，主要针对数据仓库设计中表存储数据的方式而定义的。它记录一个事物从开始到当前状态的所有变化的信息。拉链表可以避免按每一天存储所有记录造成的海量存储问题，同时也是处理缓慢变化数据的一种常见方式。 也就是说，对于表中的任何数据，不进行真正的删除，只记录操作和有效日期。 流程 其中，tmp0表有两个分区，表示历史数据和当前数据，使用tmp1对tmp0和事实表进行更新和交换。 操作 创建表 首先需要创建delta表，事实表，以及两个tmp表 -- 事实表 create table public.member_fatdt0 ( member_id varchar(64), -- 会员ID phoneno varchar(20), -- 电话号码 dw_beg_date date, -- 生效日期 dw_end_date date, -- 失效日期 dtype char(1), -- 类型（历史数据，当前数据） dw_status char(1), -- 数据操作类型（I，D，U） dw_ins_date date -- 数据仓库插入日期 )with(appendonly=true,compresslevel=5) -- 压缩级别 distributed by (member_id) PARTITION BY RANGE (dw_end_date) ( PARTITION p20111201 START (date '2011-12-01') INCLUSIVE, PARTITION p20111202 START (date '2011-12-02') INCLUSIVE, PARTITION p20111203 START (date '2011-12-03') INCLUSIVE, PARTITION p20111204 START (date '2011-12-04') INCLUSIVE, PARTITION p20111205 START (date '2011-12-05') INCLUSIVE, PARTITION p20111206 START (date '2011-12-06') INCLUSIVE, PARTITION p20111207 START (date '2011-12-07') INCLUSIVE, PARTITION p20111231 START (date '2011-12-31') INCLUSIVE END (date '3001-01-01') EXCLUSIVE ); -- 增量表 create table public.member_delta ( member_id varchar(64), phoneno varchar(20), action char(1), -- 数据操作类型（I，D，U） dw_ins_date date -- 类型（新增，删除，更新） )with(appendonly=true,compresslevel=5) -- 压缩级别 distributed by (member_id) -- 临时表 create table public.member_tmp0 ( member_id varchar(64), phoneno varchar(20), dw_beg_date date, dw_end_date date, dtype char(1), dw_status char(1), dw_ins_date date )with(appendonly=true,compresslevel=5) -- 压缩级别 distributed by (member_id) PARTITION BY LIST (dtype) ( PARTITION PHIS VALUES ('H'), -- 表示历史信息 PARTITION PCUR VALUES ('C'), -- 表示当前信息 DEFAULT PARTITION other ); -- 临时表1 create table public.member_tmp1 ( member_id varchar(64), phoneno varchar(20), dw_beg_date date, dw_end_date date, dtype char(1), dw_status char(1), dw_ins_date date )with(appendonly=true,compresslevel=5) -- 压缩级别 distributed by (member_id) 插入数据 -- 插入delta表 Insert into member_delta values('mem006','1310000006','I','2011-12-03'); Insert into member_delta values('mem002','1310000002','D','2011-12-03'); Insert into member_delta values('mem003','1310000003','U','2011-12-03'); 插入事实表 Insert into member_fatdt0 values('mem001','1310000001','2011-12-01','3000-12-31','C','I','2011-12-02'); Insert into member_fatdt0 values('mem002','1310000002','2011-12-01','3000-12-31','C','I','2011-12-02'); Insert into member_fatdt0 values('mem003','1310000003','2011-12-01','3000-12-31','C','I','2011-12-02'); Insert into member_fatdt0 values('mem004','1310000004','2011-12-01','3000-12-31','C','I','2011-12-02'); Insert into member_fatdt0 values('mem004','1310000004','2011-12-01','3000-12-31','C','I','2011-12-02'); Insert into member_fatdt0 values('mem005','1310000005','2011-12-01','3000-12-31','C','I','2011-12-02'); 数据刷新 将member_fatdt0表与member_delta左外连接，相关联的历史数据插入到member_tmp0历史分区，反之插入到member_tmp0的当前分区 这里主要处理update和delete操作，若能进行左连接，说明数据有更新，插入历史分区 truncate table public.member_tmp0; -- 清理临时表 insert into public.member_tmp0 ( member_id, phoneno, dw_beg_date, dw_end_date, dtype, dw_status, dw_ins_date ) select a.member_id,a.phoneno,a.dw_beg_date, case when b.member_id is null then a.dw_end_date else date '2011-12-02' end as dw_end_date, case when b.member_id is null then 'C' else 'H' end as dtype, case when b.member_id is null then a.dw_status else b.action end as dw_status, date '2011-12-03' from public.member_fatdt0 a left join public.member_delta b on a.member_id = b.member_id and b.action in('D','U') where a.dw_beg_date cast('2011-12-02' as date)-1; 将member_delta当前数据（更新，插入的新数据）插入到member_tmp0当前分区，end时间为无穷。 insert into public.member_tmp0 ( member_id, phoneno, dw_beg_date, dw_end_date, dtype, dw_status, dw_ins_date ) select member_id,phoneno, cast('2011-12-02' as date), cast('3000-12-31' as date), 'C', action, cast('2011-12-03' as date) from public.member_delta where action in ('I','U'); 将member_tmp0历史数据与member_fatdt0相应分区交换（通过member_tmp1表进行交换）。 alter table member_tmp1 drop constraint member_tmp0_1_prt_phis_check; truncate table public.member_tmp1; alter table public.member_tmp0 exchange partition for ('H') with table public.member_tmp1; alter table public.member_fatdt0 exchange partition for('2011-12-02') with table public.member_tmp1; 将member_tmp0当前数据与member_fatdt0相应分区交换（通过member_tmp1表进行交换）。 alter table member_tmp1 drop constraint member_fatdt0_1_prt_p20111202_check alter table member_tmp1 drop constraint member_tmp0_1_prt_pcur_check alter table public.member_tmp0 exchange partition for('C') with table public.member_tmp1; alter table public.member_fatdt0 exchange partition for('3000-12-31') with table public.member_tmp1; "},"log.html":{"url":"log.html","title":"日志分析","keywords":"","body":"创建外部表 首先启动gpfdist服务： nohup gpfdist -d /home/dyt/PJ4 -p 9058 -l /home/dyt/PJ4/gpfdist.log & 查看是否启动成功： ps -ef | grep gpfdist 创建外部表 1,123432423,2019-03-15 23:12:25,zsl 2,123657567,2019-03-15 23:12:26,sdf 3,123482825,2019-03-15 23:12:27,fgd 进入云数据库 psql -U dyt -h udw.lurymd.m0.service.ucloud.cn -p 5432 -d db_dyt -W 创建外部表 注意，这里端口号后，默认会转到之前确认的文件夹下，也就是/home/dyt/PJ4 drop external table test001_ext_1; create external table public.test001_ext_1( id integer, phoneno varchar(20), time date, name varchar(128) ) location( 'gpfdist://10.11.10.9:9058/test001_ext_1.txt' ) Format 'TEXT' (delimiter as E',' null as '' escape 'OFF'); 查看创建的表： 尝试在外部表中更新数据，并查看table： 发现table也同样变化了。 日志分析 日志分析是网站分析的基础，通过对网站浏览的日志进行分析，可以为网站优化提供数据支持，了解用户群以及用户浏览特性，对改进网站体验，提升流量有非常重要的意义。 创建外部表 我们已经有了一万行的网站数据data.txt，首先还是创建外部表导入数据 drop external table test001_ext_2; create external table public.test001_ext_2( log_time timestamp(0), -- 浏览时间 cookie_id varchar(256), -- 浏览的id url varchar(1024), -- 浏览的页面 ip varchar(64), -- 用户ip refer_url varchar(1024) -- 域名 ) location( 'gpfdist://10.11.10.9:9058/data.txt' ) Format 'TEXT' (delimiter as E',' null as '' escape 'OFF'); 创建内部表 创建一个新的内部表，将外部表的数据进行导入： drop table if exists log_path; create table log_path( log_time timestamp(0), -- 浏览时间 cookie_id varchar(256), -- 浏览的id url varchar(1024), -- 浏览的页面 ip varchar(64), -- 用户ip refer_url varchar(1024) -- 域名 )distributed by(cookie_id); insert into log_path select * from test001_ext_2; 查看数据是否成功导入： 查询PV、UV分布 cookie_id可以视为唯一的用户标识，故UV可视为去重后的cookie_id数。SQL如下： drop table if exists log_pv_uv_result; create table log_pv_uv_result( log_time varchar(1024), pv integer, uv integer )distributed by(log_time); insert into log_pv_uv_result select to_char(log_time,'yyyy-mm-dd HH24:mi:00'), COUNT(1) pv, COUNT(distinct cookie_id) uv from log_path group by 1 order by 1; 查看数据： 导出数据 使用copy命令导出 \\copy log_pv_uv_result to '/home/dyt/PJ4/log_pv_uv.csv' csv; 这样我们就可以用csv对数据进行操作并简单画图 解析URL 解析URL，是指通过substring对URL进行正则表达式匹配，正则表达式\\w+://([\\w.]+)可以将域名匹配出来。 同样的，可以将参数后面关键字（member_id或memberId）的值获取出来，作为字段member_id。 split_part函数可以将字符串按照某个字符串分割，然后获取其中一个子串。 regexp_split_to_array函数可以将字符串按照某个字符串分割，然后转换为数组变量。 主要熟悉数据仓库分析函数的使用 drop table if exists log_path_tmp1; create table log_path_tmp1 as (select log_time, cookie_id, substring(url,E'\\\\w+://([\\w.]+)') AS host, split_part(url,'?',1) AS url, substring(url,E'member[_]?[i|I]d=(\\\\w+)') AS member_id, regexp_split_to_array(split_part(url,'?',2),'&') AS paras, ip, refer_url from log_path) distributed by (cookie_id); 显示结果为： 用户浏览次数区间分析 select case when cnt>100 then '100+' when cnt>50 then '51-100' when cnt>10 then '11-50' when cnt>5 then '6-10' else ' "},"column store.html":{"url":"column store.html","title":"OLAP 列存储","keywords":"","body":"列存储 在常见的OLTP中，大部分都是按行存储的，在OLAP中，如果数据行特别多，可以考虑列存储。 应用场景 一个OLAP类型的查询可能需要访问几百万甚至几十亿个数据行，且该查询往往只关心少数几个数据列。例如，查询今年销量最高的前20个商品，这个查询只关心三个数据列：时间（date）、商品（item）以及销售量（sales amount）。 列式数据库只需要读取存储着“时间、商品、销量”的数据列，而行式数据库需要读取所有的数据列。 优点 同一个数据列的数据类型相同，数据相似性高，列式数据库压缩性比行存储好。 使用什么压缩算法？ 若字段属性并不多，可以替换；位图的压缩方法；具体参考这里 查询单列只需扫描单列，不必加载整行数据 缓存命中率提高，这是因为同一列的数据被高度压缩，常用的Page被频繁访问而变得异常活跃，Buffer Manager把活跃的数据页缓存到内存中，不常用的Page被换出（Page Out） 在UDW中使用列存储 首先进入UDW： psql -U dyt -h udw.lurymd.m0.service.ucloud.cn -p 5432 -d db_dyt -W 创建列存储表 create table test_column_ao( id bigint, name varchar(128), value varchar(128) )with(appendonly=true,ORIENTATION=column,compresslevel=5) distributed by (id); 查看表结构： 导入数据 使用同一个数据源，分别进行行存储和列存储： create table r_table ( id int, col1 int,col2 int,col3 int,col4 int,col5 int,col6 int,col7 int,col8 int,col9 int,col10 int, col11 varchar(10),col12 varchar(10),col13 varchar(10),col14 varchar(10),col15 varchar(10), col16 varchar(10),col17 varchar(10),col18 varchar(10),col19 varchar(10),col20 varchar(10), col21 varchar(20),col22 varchar(20),col23 varchar(20),col24 varchar(20),col25 varchar(20), col26 varchar(20),col27 varchar(20),col28 varchar(20),col29 varchar(20),col30 varchar(20), col31 varchar(30),col32 varchar(30),col33 varchar(30),col34 varchar(30),col35 varchar(30), col36 varchar(30),col37 varchar(30),col38 varchar(30),col39 varchar(30),col40 varchar(30), col41 varchar(40),col42 varchar(40),col43 varchar(40),col44 varchar(40),col45 varchar(40), col46 varchar(40),col47 varchar(40),col48 varchar(40),col49 varchar(40),col50 varchar(40), col51 varchar(50),col52 varchar(55),col53 varchar(60),col54 varchar(65),col55 varchar(128), col56 date,col57 text,col58 timestamp,col59 varchar (125),col60 bigint) with (appendonly=true,compresslevel =5) distributed by (col11); create table c_table ( id int, col1 int,col2 int,col3 int,col4 int,col5 int,col6 int,col7 int,col8 int,col9 int,col10 int, col11 varchar(10),col12 varchar(10),col13 varchar(10),col14 varchar(10),col15 varchar(10), col16 varchar(10),col17 varchar(10),col18 varchar(10),col19 varchar(10),col20 varchar(10), col21 varchar(20),col22 varchar(20),col23 varchar(20),col24 varchar(20),col25 varchar(20), col26 varchar(20),col27 varchar(20),col28 varchar(20),col29 varchar(20),col30 varchar(20), col31 varchar(30),col32 varchar(30),col33 varchar(30),col34 varchar(30),col35 varchar(30), col36 varchar(30),col37 varchar(30),col38 varchar(30),col39 varchar(30),col40 varchar(30), col41 varchar(40),col42 varchar(40),col43 varchar(40),col44 varchar(40),col45 varchar(40), col46 varchar(40),col47 varchar(40),col48 varchar(40),col49 varchar(40),col50 varchar(40), col51 varchar(50),col52 varchar(55),col53 varchar(60),col54 varchar(65),col55 varchar(128), col56 date,col57 text,col58 timestamp,col59 varchar (125),col60 bigint) with (appendonly=true,ORIENTATION=column,compresslevel =5) distributed by (col11); 导入数据 \\copy r_table from '/home/cl/columnstore/data.dat' with delimiter ','; \\copy c_table from '/home/cl/columnstore/data.dat' with delimiter ','; 查询比较 比较存储大小 select pg_relation_size('c_table') 首先用\\timing on开始计时 查询10个字段消耗的时间 select count(col1),count(col5),count(col10),count(col20),count(col25),count(col32),count(col42),count(col45),count(col56),count(col53) from r_table; r_table 消耗时间：55.743ms c_table 消耗时间：34.023ms 查询20个字段消耗时间 select count(col1),count(col2),count(col4), count(col5),count(col7),count(col8),count(col9),count(col10),count(col11),count(col14),count(col18),count(col19),count(col20),count(col23),count(col24),count(col25),count(col30),count(col31),count(col32),count(col39) from r_table r_table 消耗时间：62.746ms c_table 消耗时间：34.048ms 查询50个字段消耗时间 select count(col1),count(col2),count(col4), count(col5),count(col7),count(col8),count(col9),count(col10),count(col11),count(col13),count(col14),count(col18),count(col19),count(col20),count(col23),count(col24),count(col25),count(col28),count(col29),count(col30),count(col31),count(col32),count(col35),count(col36),count(col38),count(col39),count(col40),count(col41),count(col42),count(col45),count(col46),count(col48),count(col49),count(col50),count(col52),count(col53),count(col55),count(col56),count(col58),count(col60) from r_table; r_table 消耗时间：119.231ms c_table 消耗时间：25.986ms 查询全部字段消耗时间 select * from r_table r_table 消耗时间：175.872ms c_table 消耗时间：187.226ms 可以发现，如果查询全部字段，则基本上没有优势 可视化为折线图为： 扩大数据集为50w，重新进行测试，得到结果为： "},"tpch.html":{"url":"tpch.html","title":"基准评测","keywords":"","body":"基准评测工具 安装测试工具 三个重要考虑方面：数据如何生成，测量指标是什么，负载怎么测 首先下载TPC-H，修改makefile文件，在103行左右： # Current values for WORKLOAD are: TPCH DATABASE= TDAT MACHINE = LINUX WORKLOAD = TPCH 修改完成后直接运行make clean 和 make进行编译 测试数据生成：我们需要自动创建测试数据。编译之后会生成dbgen执行文件，用TPC-H即可方便地生成自定义大小的测试数据。在这里以0.1GB数据为例，采用命令./dbgen –s 0.1即可。 查看生成的数据集：ls -lrth *.tbl 测试数据生成：注意生成的数据后面每行会多出一个‘|’分隔符，所以需要通过以下脚本简单处理一下，生成对应的csv文件。 for i in `ls *.tbl`; do sed 's/|//′//' //′i > i/tbl/csv;echo{i/tbl/csv}; echo i/tbl/csv;echoi; done; 测试表创建 创建PART，REGION，NATION表： create table PART( P_PARTKEY BIGINT, P_NAME VARCHAR(55), P_MFGR CHAR(25), P_BRAND CHAR(10), P_TYPE VARCHAR(25), P_SIZE INTEGER, P_CONTAINER CHAR(10), P_RETAILPRICE DECIMAL, P_COMMENT VARCHAR(23) ) DISTRIBUTED BY (P_PARTKEY); create table region( R_REGIONKEY BIGINT, R_NAME CHAR(25), R_COMMENT VARCHAR(152) ) DISTRIBUTED BY (R_REGIONKEY); create table NATION( N_NATIONKEY BIGINT, N_NAME CHAR(25), N_REGIONKEY BIGINT NOT NULL, N_COMMENT VARCHAR(152) ) DISTRIBUTED BY (N_NATIONKEY); CREATE TABLE SUPPLIER ( S_SUPPKEY BIGINT, S_NAME CHAR(25), S_ADDRESS VARCHAR(40), S_NATIONKEY BIGINT NOT NULL, -- references N_NATIONKEY S_PHONE CHAR(15), S_ACCTBAL DECIMAL, S_COMMENT VARCHAR(101) ) DISTRIBUTED BY (s_suppkey); CREATE TABLE CUSTOMER ( C_CUSTKEY BIGINT, C_NAME VARCHAR(25), C_ADDRESS VARCHAR(40), C_NATIONKEY BIGINT NOT NULL, -- references N_NATIONKEY C_PHONE CHAR(15), C_ACCTBAL DECIMAL, C_MKTSEGMENT CHAR(10), C_COMMENT VARCHAR(117) ) DISTRIBUTED BY (c_custkey); CREATE TABLE PARTSUPP ( PS_PARTKEY BIGINT NOT NULL, -- references P_PARTKEY PS_SUPPKEY BIGINT NOT NULL, -- references S_SUPPKEY PS_AVAILQTY INTEGER, PS_SUPPLYCOST DECIMAL, PS_COMMENT VARCHAR(199) ) DISTRIBUTED BY (ps_partkey); CREATE TABLE ORDERS ( O_ORDERKEY BIGINT, O_CUSTKEY BIGINT NOT NULL, -- references C_CUSTKEY O_ORDERSTATUS CHAR(1), O_TOTALPRICE DECIMAL, O_ORDERDATE DATE, O_ORDERPRIORITY CHAR(15), O_CLERK CHAR(15), O_SHIPPRIORITY INTEGER, O_COMMENT VARCHAR(79) ) DISTRIBUTED BY (o_orderkey); CREATE TABLE LINEITEM ( L_ORDERKEY BIGINT NOT NULL, L_PARTKEY BIGINT NOT NULL, L_SUPPKEY BIGINT NOT NULL, L_LINENUMBER INTEGER, L_QUANTITY DECIMAL, L_EXTENDEDPRICE DECIMAL, L_DISCOUNT DECIMAL, L_TAX DECIMAL, L_RETURNFLAG CHAR(1), L_LINESTATUS CHAR(1), L_SHIPDATE DATE, L_COMMITDATE DATE, L_RECEIPTDATE DATE, L_SHIPINSTRUCT CHAR(25), L_SHIPMODE CHAR(10), L_COMMENT VARCHAR(44) ) DISTRIBUTED BY (l_orderkey); 导入测试数据 直接通过copy命令将csv数据导入响应表中： \\copy customer from '/data/dyt/tpch-dbgen-master/customer.csv' with delimiter '|' \\copy lineitem from '/data/dyt/tpch-dbgen-master/lineitem.csv' with delimiter '|' \\copy nation from '/data/dyt/tpch-dbgen-master/nation.csv' with delimiter '|' \\copy orders from '/data/dyt/tpch-dbgen-master/orders.csv' with delimiter '|' \\copy part from '/data/dyt/tpch-dbgen-master/part.csv' with delimiter '|' \\copy partsupp from '/data/dyt/tpch-dbgen-master/partsupp.csv' with delimiter '|' \\copy region from '/data/dyt/tpch-dbgen-master/region.csv' with delimiter '|' \\copy supplier from '/data/dyt/tpch-dbgen-master/supplier.csv' with delimiter '|' 测试脚本准备 TPC-H提供了22个测试SQL，位于/dss/queries目录下。通过以下脚本生成22条sql语句： for q in `seq 1 22` do DSS_QUERY=dss/templates ./qgen -s {% math_inline %}SF {% endmath_inline %}q > dss/queries/{% math_inline %}q.sql sed 's/^select/explain select/' dss/queries/{% endmath_inline %}q.sql > dss/queries/$q.explain.sql done 查看生成的SQL语句 测试执行 可以简单执行若干条SQL查询语句来查看在SF=0.1的情况下的执行时间。 \\timing \\i /data/dyt/tpch-dbgen-master/dss/queries/1.sql "},"mvcc.html":{"url":"mvcc.html","title":"数据仓库之表膨胀","keywords":"","body":"概念 多版本并发控制（MVCC） MVCC就是对同一份数据临时保留多版本的方式，实现并发控制。它可以避免读写事务之间的互相阻塞，相比通常的封锁技术可极大的提高业务的并发性能。 当一个MVCC 数据库需要更一个一条数据记录的时候，它不会直接用新数据覆盖旧数据，而是将旧数据标记为过时（obsolete）并在别处增加新版本的数据。这样就会有存储多个版本的数据，但是只有一个是最新的。 这种方式允许读者读取在他读之前已经存在的数据，即使这些在读的过程中半路被别人修改、删除了，也对先前正在读的用户没有影响。这种多版本的方式避免了填充删除操作在内存和磁盘存储结构造成的空洞的开销，但是需要系统周期性整理（sweepthrough）以真实删除老的、过时的数据。 表膨胀 根据MVCC的原理，数据库没有办法直接更新数据(更新操作(update)是通过先删除(delete)再插入(insert)实现的)，被更新之前的行数据仍然在数据文件中。这种现象叫做表膨胀。 PG中的MVCC 数据文件中存放同一逻辑行的多个行版本（称为Tuple）； 每个行版本的头部记录创建该版本的事务ID以及删除该行版本的事务的ID（分别称为xmin和xmax）； 每个事务的状态（运行中，中止或提交）记录在pg_clog文件中； 根据上面的数据并运用一定的规则每个事务只会看到一个特定的行版本。 实验 查看当前隔离级别： 新建一个简单的表： create table test(id int primary key, value char(10) ); insert into test values(1,'cc'); insert into test values(2,'dd'); insert into test values(3,'ee'); -- 开始一个事务，但不提交 begin;update test set value = 'ccc' where id = 1; -- 再开一个事物，查看 begin;select *, xmin,xmax,cmin,cmax from test; -- 最后再提交 commit; 在事务A中： 事务B中： 由于是分布式数据库，并不能根据事务的id来判断先后，可参考这里。 解决方法 vacuum full table 当一个过期的行不在被任何活跃事务引用时，它可以被移除从而腾出其所占用的空间进行重用。VACUUM命令会标记过期行所使用的空间可以被重用。 VACUUM命令（不带FULL）可以与其他查询并行运行。它会标记之前被过期行所占用的空间为空闲可用。如果剩余的空闲空间数量可观，它会把该页面加到该表的空闲空间映射中。当Greenplum数据库之后需要空间分配给新行时，它首先会参考该表的空闲空间映射以寻找有可用空间的页面。如果没有找到这样的页面，它会为该文件追加新的页面。 VACUUM（不带FULL）不会合并页面或者减小表在磁盘上的尺寸。它回收的空间只是放在空闲空间映射中表示可用。为了阻止磁盘文件大小增长，重要的是足够频繁地运行VACUUM。运行VACUUM的频率取决于表中更新和删除（插入只会增加新行）的频率。重度更新的表可能每天需要运行几次VACUUM来确保通过空闲空间映射能找到可用的空闲空间。在运行了一个更新或者删除大量行的事务之后运行VACUUM也非常重要。 VACUUM FULL命令会把表重写为没有过期行，并且将表减小到其最小尺寸。表中的每一页都会被检查，其中的可见行被移动到前面还没有完全填满的页面中。空页面会被丢弃。该表会被一直锁住直到VACUUM FULL完成。相对于常规的VACUUM命令来说，它是一种非常昂贵的操作，可以用定期的清理来避免或者推迟这种操作。最好是在一个维护期来运行VACUUM FULL。VACUUM FULL的一种替代方案是用一个CREATE TABLE AS语句重新创建该表并且删除掉旧表。 但是在索引存在的情况下，vacuum full table 只能删除之前行，不能回收索引。 因此使用vacuum full回收垃圾的建议操作流程： 记录下表的索引 删除索引 vacuum full 表 重建索引 -- 创建表 create table stu(id int, name varchar(20),gender varchar(10)); -- 生成数据 insert into stu select generate_series(1,100000),'Bob','man'; -- 查看表大小 select pg_size_pretty(pg_relation_size('stu')); -- 修改数据 update stu set name='Alice'; -- 再次查看表大小 select pg_size_pretty(pg_relation_size('stu')); -- 使用vacuum full stu回收空间，并查看空间大小。此时表空间应缩小一半。 vacuum full stu; -- 查看表大小 select pg_size_pretty(pg_relation_size('stu')); -- 在id字段上创建索引，并查看索引大小 create index idx_stu on stu(id); -- 查看表大小 select pg_size_pretty(pg_relation_size('stu')); -- 更新表数据，并查看表空间和索引空间大小 update stu set name = 'Jack'; select pg_size_pretty(pg_relation_size('idx_stu')); select pg_size_pretty(pg_relation_size('stu')); -- 删除索引，回收表空间。 drop index idx_stu; vacuum full stu; -- 重建索引，查看表空间和索引大小 create index idx_stu on stu(id); select pg_size_pretty(pg_relation_size('stu')); select pg_size_pretty(pg_relation_size('idx_stu')); 修改分布键释放空间 修改分布键可以回收索引的膨胀空间。修改分布键加载的锁是排它锁。建议在没有业务的时候执行，不要影响业务。 缺点：由于需要重新修改分布键，计算开销比较大，在表分布已经比较均匀时会影响性能。 -- 更新表stu的name字段，并查看表空间和索引大小 update stu set name = 'Jay'; select pg_size_pretty(pg_relation_size('idx_stu')); select pg_size_pretty(pg_relation_size('stu')); -- 查看表格分布 \\d stu; -- 按照原有的分布键重新分布。查看表空间和索引空间 alter table stu set with (reorganize=true) distributed by(id); select pg_size_pretty(pg_relation_size('stu')); select pg_size_pretty(pg_relation_size('idx_stu')); 复制表 创建新表，导入数据 CREATE TABLE…AS SELECT 命令把该表拷贝为一个新表，新建的表将不会出现膨胀现象。 -- 更新表，并创建复制表。 update stu set name = 'Bob'; create table copy_stu as select * from stu; -- 删除原表，并将复制表名更改为stu。 drop table stu; alter table copy_stu rename to stu; Reference Greenplum数据库概念 https://blog.csdn.net/hmxz2nn/article/details/82811404 CMU高级数据库课程：https://15721.courses.cs.cmu.edu/spring2019/schedule.html "},"json.html":{"url":"json.html","title":"数据仓库之 Json 格式","keywords":"","body":"Json基本操作 获取JSON数组元素，索引以0为开始 select '[{\"a\":\"foo\"},{\"b\":\"bar\"},{\"c\":\"foo\"}]'::json->2; 通过键来获取 JSON 对象的域（field） select '{\"a\":{\"b\":\"foo\"}}'::json->'a'; 获取 JSON 数组元素，然后以 text 形式返回它 select '[1,2,3]'::json->>2; 获取 JSON 对象的域，然后以 text 形式返回它 select '{\"a\":1,\"b\":2}'::json->>'b'; 获取指定路径上的 JSON 对象 select '{\"a\":{\"b\":{\"c\":\"foo\"}}}'::json#>'{a,b}'; 获取指定路径上的 JSON 对象，并以 text 形式返回它 select '{\"a\":[1,2,3],\"b\":[4,5,6]}'::json#>>'{a,2}'; 创建Json表： create table test_json(id int,name json) with (appendonly=true,orientation=column,compresslevel=5) distributed by(id); insert into test_json values (1,'{\"id\":1,\"sub\":{\"subid\":10,\"subsub\":{\"subsubid\":100}}}'), (2,'{\"id\":20,\"sub\":{\"subid\":200,\"subsub\":{\"subsubid\":2000}}}'), (1,'{\"id\":1,\"sub\":{\"subid\":\"test\",\"subsub\":{\"subsubid\":100}}}'), (3,'{\"id\":1,\"sub\":\"test\",\"name\":\"me\",\"ip\":\"10.10.10.10\"}'); SELECT * FROM test_json WHERE name->>'id'=1; SELECT * FROM test_json WHERE name->'sub'->>'subid'=10; SELECT * FROM test_json WHERE name->>'name'='me'; 创建函数 以 JSON 格式返回输入的值。 select to_json('a said \"Hi.\"'::text); 以 JSON 数组格式返回输入的数组。如果 pretty_bool的值为 true ， 那么则在维度-1元素之间添加换行符。 如下所示： select array_to_json(array_agg(q), false) from (select x as b,x*2 as c from generate_series(1,3) x)q; 以 JSON 对象格式返回行。 select row_to_json(row(1,'foo')); 建立一个可能不同类型的JSON数组，由可变参数列表组成 select json_build_array('a',1,'b',1.2,'c',true,'d',null,'e',json '{\"x\":3,\"y\":[1,2,3]}'); 输入的文本数组构建一个 JSON 对象。 select json_object('{{a,1},{b,2},{3,null},{\"d e f\",\"a b c\"}}'); select json_object('{a,b,c,\"d e f\"}','{1,2,3,\"a b c\"}'); 返回最外层的 JSON 数组的元素数量。 select json_array_length('[1,2,3,{\"f1\":1,\"f2\":[5,6]},4]'); 练习 创建一张普通数据表t1并且导入数据; 创建一张json数据表t1_json； 将普通表t1中的数据转换为json格式的数据并且存储在t1_json 中。 create table t1(c1 int,c2 int); insert into t1 values(1,1); insert into t1 values(2,2); insert into t1 values(3,3); select * from t1; create table t1_json(c1 json); insert into t1_json select row_to_json(row(q)) from (select * from t1) q; select * from t1_json; 创建一张json数据表t2_json并且导入数据； 创建一张普通数据表t2; 将t2_json表中数据进行转换并且存储在普通表中t2中。 create table t2_json(c1 json); insert into t2_json values ('{\"a\":\"gg\",\"b\":\"jj\"}'), ('{\"a\":\"ggg\",\"b\":\"jjj\"}'), ('{\"a\":\"gggg\",\"b\":\"jjjj\"}'); create table t2(a text,b text); insert into t2 select cast(c1->'a' as text),cast(c1->'b' as text) from t2_json; Reference UDW中Json类型 "},"hive.html":{"url":"hive.html","title":"Hive 使用","keywords":"","body":"首先，Hive是使用了MapReduce引擎和HDFS存储的中间键，其元数据存储在MySQL，Hive只是方便查询，其数据库中的数据都在HDFS中。 安装Hadoop和Hive 在之前的分布式系统中，已经安装好Hadoop，具体教程可参考这里。 需要注意的是，在Ubuntu下，如果把环境变量放到~/.bash_profile，并不是一个好的选择，因为每次新的terminal并不会取source，因此一般放到~/.profile即可。参考这里。 测试Hadoop 创建HDFS hdfs dfs -mkdir /user hdfs dfs -mkdir /user/test 拷贝input文件到HDFS目录下 hdfs dfs -put etc/hadoop /user/test/input 查看 hadoop fs -ls /user/test/input 执行mapreduce实例 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar grep /user/test/input output 'dfs[a-z.]+' 查看结果 hdfs dfs -cat output/ 修改配置 首先停止服务：sbin/stop-dfs.sh 修改mapreduce配置文件：cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml vi etc/hadoop/mapred-site.xml mapreduce.framework.name yarn vi etc/hadoop/yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle sbin/start-all.sh 这样就使用Yarn框架搭建Hadoop 访问ResourceManger的web页面：http://localhost:8088/ MySQL安装 sudo apt-get install mysql-server 会自动安装client等其他组件 sudo mysql_secure_installation 会自动执行初始化，包括设定密码等（注意，这里的密码是当前用户的密码，而不是root密码） 重置root密码： 首先需要找到debian的默认密码：sudo cat /etc/mysql/debian.cnf 使用这个密码登录：mysql -u debian-sys-maint -p 然后依次执行 USE mysql SELECT User, Host, plugin FROM mysql.user; UPDATE user SET plugin='mysql_native_password' WHERE User='root'; COMMIT; UPDATE mysql.user SET authentication_string=PASSWORD('new_password') where user='root'; COMMIT; FLUSH PRIVILEGES; COMMIT; EXIT 然后重启sudo service mysql restart 使用新密码登录：mysql -u root -p 启动mysql：service mysql start Hive安装 cd /usr/local/hive/conf cp hive-env.sh.template hive-env.sh cp hive-default.xml.template hive-site.xml vim hive-env.sh export HADOOP_HOME=/usr/local/hadoop export JAVA_HOME=/usr/local/java1.8 export HIVE_HOME=/usr/local/hive 在hdfs中创建目录，并授权，用于存储文件 hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -mkdir -p /user/hive/tmp hdfs dfs -mkdir -p /user/hive/log hdfs dfs -chmod -R 777 /user 添加mysql jdbc包 https://dev.mysql.com/downloads/connector/j/ 选择5.1.47版本 解压后将文件夹里的jar包复制到 hive/lib下 新建hive/conf/hive-site.xml 文件 hive.metastore.warehouse.dir /user/hive/warehouse hive.metastore.local true javax.jdo.option.ConnectionURL jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword 123 hive.cli.print.header true hive.cli.print.current.db true hive.support.concurrency true hive.exec.dynamic.partition.mode nonstrict hive.txn.manager org.apache.hadoop.hive.ql.lockmgr.DbTxnManager hive.compactor.initiator.on true hive.compactor.worker.threads 1 初始化hive： schematool-dbType mysql -initSchema 如果包冲突，删除/hive/lib下的划线jar包，删除hive数据库，重新执行 执行成功： 在MySQL中查看： 直接输入Hive启动hive： Hive 介绍 Hive 由Facebook 实现并开源 是基于Hadoop 的一个数据仓库工具 可以将结构化的数据映射为一张数据库表 并提供HQL(Hive SQL)查询功能 底层数据是存储在HDFS 上 Hive的本质是将SQL 语句转换为MapReduce 任务运行 使不熟悉MapReduce 的用户很方便地利用HQL 处理和计算HDFS 上的结构化的数据，适用于离线的批量数据计算。 测试 建立测试表 create table t1(id int,name string) clustered by (id) into 8 buckets stored as orc tblproperties('transactional'='true'); 说明：建表语句必须带有into buckets子句和stored as orc TBLPROPERTIES (‘transactional’=’true’)子句，并且不能带有sorted by子句。 测试insert、update、delete insert into t1 values (1,'aaa'); insert into t1 values (2,'bbb'); update t1 set name='ccc' where id=1; delete from t1 where id=2; 可以发现Hive是通过MapReduce框架进行执行： 文件格式 文件格式选择 如果数据有参数化的分隔符，那么可以选择textfile格式。 如果数据所在文件比块尺寸（hadoop默认128MB）小，可以选择sequencefile格式 如果执行数据分析，并高效存储数据，可以选择RCFILE格式。 如果希望减小数据所需存储空间并提升性能，可以选择ORCFILE格式。 TEXTFILE普通文本文件 Hive的默认文件格式，可以向表中装载以逗号、空格、TAB作为分隔符的数据，也可以导入json格式的数据。 文本文件除了支持普通字符串、数字、日期等简单数据类型外，还支持Struct、map、array三种集合类型。 ARRAY类型是由一系列相同数据类型的元素组成，这些元素可以通过下标来访问。比如有一个ARRAY类型的变量fruits，它是由[‘apple’,‘orange’,‘mango’]组成，那么我们可以通过fruits[1]来访问元素orange，因为ARRAY类型的下标是从0开始的。 MAP包含key->value键值对，可以通过key来访问元素。比如”userlist”是一个map类型，其中username是key，password是value；那么我们可以通过userlist['username']来得到这个用户对应的password； STRUCT可以包含不同数据类型的元素。类似于一个对象，这些元素可以通过”点语法”的方式来得到所需要的元素，比如user是一个STRUCT类型，那么可以通过user.address得到这个用户的地址。 以Tab为分隔符 创建文件data.csv，内容为： a1 1 b1 c1 a2 2 b2 c2 在Hive中导入： create database test; use test; create table t_textfile(c1 string,c2 int,c3 string,c4 string) row format delimited fields terminated by '\\t' stored as textfile; load data local inpath '/home/scott/Documents/data.csv' into table t_textfile; select * from t_textfile; Json格式文件 创建simple.json，内容如下： {\"foo\":\"abd\",\"ba\":\"12341234213\",\"a\":{\"aa\":123,\"bb\":\"dfg\"}} 在Hive中执行命令（需要添加Jar包路径） add jar /usr/local/hive/lib/hive-hcatalog-core-2.3.0.jar; create table my_table( foo string, ba string, a struct) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe' stored as textfile; load data local inpath '/home/scott/Documents/simple.json' into table my_table; select foo,ba,a.aa,a.bb from my_table; Array使用 创建文件array.txt，内容如下： 12,23,23,34 what,are,this 34,34,12,45,456 who,am,i,are 在Hive中执行命令： create table array_test(info1 array,info2 array) row format delimited fields terminated by '\\t' collection items terminated by ',' stored as textfile; load data local inpath '/home/scott/Documents/array.txt' overwrite into table array_test; select * from array_test; select size(info1),size(info2) from array_test; select info1[2],info2[0] from array_test; Map使用 json的结构是固定的，实际应用中结构可能动态变化，用map存储动态键值对 创建文件a.json，内容如下： {\"conflict\":{\"liveid\":123, \"zhuboid\":456, \"media\":789, \"proxy\":\"abc\", \"result\":1000}} {\"conflict\":{\"liveid\":123, \"zhuboid\":456, \"media\":789, \"proxy\":\"abc\"}} {\"conflict\":{\"liveid\":123, \"zhuboid\":456, \"media\":789}} {\"conflict\":{\"liveid\":123}} 在Hive中执行命令： create table json_tab( conflict map) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe' stored as textfile; load data local inpath '/home/scott/Documents/a.json' overwrite into table json_tab; select * from json_tab; select conflict[\"media\"] from json_tab; Sequence file 存储小文件，由二进制键值对组成，可分割的二进制格式，主要用途是联合多个小文件（小于块大小，默认128MB) 创建一个sequencefile类型的表： create table t_seq(c1 string,c2 int,c3 string,c4 string) 与textfile不同，sequencefile是二进制格式，需要从其他表导入。导入后观察执行状况并查询结果。（实际执行的是一个MapReduce任务） insert overwrite table t_seq select * from t_textfile; select * from t_seq; RCFILE 是高压缩率的二进制文件，采用列存储 创建一个RCFILE类型的表： create table t_rcfile(c1 string,c2 int,c3 string,c4 string) 导入数据： insert overwrite table t_rcfile select * from t_textfile; select * from t_rcfile; ORCFILE (Optimized Record Columnar)，相对其他格式，更优化的存储数据，提升数据处理速度，唯一支持事务的文件格式。 创建一个ORCFILE类型的表： create table t_orcfile(c1 string,c2 int,c3 string,c4 string) 导入数据 insert overwrite table t_orcfile select * from t_textfile; select * from t_orcfile; Hive表分类 Hive 中的表分为内部表、外部表、分区表和Bucket 表 内部表和外部表的区别： 删除内部表，删除表元数据和数据（就是数据库中常见的表） 删除外部表，删除元数据，不删除数据（源数据是不存储在Hive中） 内部表和外部表的使用选择： 如果数据的所有处理都在Hive 中进行，那么倾向于选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 使用外部表访问存储在HDFS 上的初始数据，然后通过Hive 转换数据并存到内部表中 分区表和 Bucket分区表区别： Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同时表和分区也可以进一步被划分为Buckets，分桶表的原理和MapReduce 编程中的HashPartitioner 的原理类似。 分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于Hive 是读模式，所以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash散列形成的多个文件，好处是可以获得更高的查询处理效率。 外部表 创建外部表（注意，这里指定的是hdfs中的路径） create external table fz_external_table(id int,name string,age int, tel string) row format delimited fields terminated by ',' stored as textfile location '/user/hive/test/external/fz_external_table'; 创建文件fz_external.txt： 1,fz,25,13188888888 2,test,20,132222222 3,dx,24,18934356563 4,test1,22,11111111 查询结果： load data local inpath '/home/scott/Documents/fz_external.txt' into table fz_external_table; select * from fz_external_table; 查看表，在hdfs中存在 hadoop dfs -ls /user/hive/test/external/fz_external_table 在hive中删除该表，用以上命令查看，观察结果。 可以发现外部数据并没有删除 分区表 创建分区表 create table page_view(viewtime int,userid bigint, page_url string,referrer_url string, ip string comment 'ip address of the user') comment 'this is the page view table' partitioned by (dt string,country string) row format delimited fields terminated by '\\001' stored as sequencefile; describe formatted page_view; 显示分区键。 创建外部分区表 创建文件a.txt： 1,a,us,ca 2,b,us,cb 3,c,ca,bb 4,d,ca,bc 创建非分区表t1，并导入数据，查看结果 drop table if exists t1; create table t1(id int,name string,cty string,st string) row format delimited fields terminated by ','; load data local inpath '/home/scott/Documents/a.txt' into table t1; select * from t1; create external table t2(id int, name string) partitioned by (country string, state string) clustered by (id) into 8 buckets stored as orcfile; insert into t2 partition(country, state) select * from t1; select * from t2; 所谓分区，只是在HDFS中创建了不同key 的文件夹，可以在HDFS中查看： 总结 clusterby key是根据key对数据进行分布，并且在每个bucket里面根据key进行排序。 distributeby key仅仅是根据key对数据进行分布，如果同时添加sort by子句，则会保证每个bucket数据是根据sort by的key有序。 如果distribute by和sort by的key是相同情况下，则等价于cluster by子句。即cluster by kesdistribute by keys sort by keys。 orderby则是对整个数据进行排序，没有任何分布概念。 Reference Hive表的分区与分桶 creating partition in external table in hive "},"model.html":{"url":"model.html","title":"建立数据仓库模型","keywords":"","body":"Hive 装载数据 load与load overwrite的区别是：load每次执行生成新的数据文件，文件中是本次装载的数据。load overwrite如表（或分区）的数据文件不存在则生成，存在则重新生成数据文件内容。 分区表比非分区表多了一种alter table ... add partition的数据装载方式。 对于分区表（无论内部还是外部），load与load overwrite会自动建立名为分区键值的目录，而alter table ... add partition，只要用location指定数据文件所在的目录即可。 对于外部表，除了在删除表时只删除元数据而保留表数据目录外，其数据装载行为与内部表相同。 load追加数据 首先生成数据并查看在warehouse中的数据 drop table if exists t1; create table t1(name string); load data local inpath '/home/scott/Documents/a.txt' into table t1; select * from t1; hdfs dfs -ls /user/hive/warehouse/test.db/t1; hdfs dfs -cat /user/hive/warehouse/test.db/t1/a.txt; 可以发现，文件被写入到HDFS中 追加数据 echo 'bbbb' >> a.txt; load data local inpath '/home/scott/Documents/a.txt' into table t1; select * from t1; hdfs dfs -ls /user/hive/warehouse/test.db/t1; hdfs dfs -cat /user/hive/warehouse/test.db/t1/a.txt; 可以发现，在HDFS中是直接追加了一个文件。 Load overwrite 使用overwrite关键字，会将直接的数据覆盖 drop table if exists t2; create table t2(name string); load data local inpath '/home/scott/Documents/a.txt' into table t2; select * from t2; hdfs dfs -ls /user/hive/warehouse/test.db/t2; hdfs dfs -cat /user/hive/warehouse/test.db/t2/a.txt; 追加数据 echo 'cccc' >> a.txt; load data local inpath '/home/scott/Documents/a.txt' overwrite into table t2; select * from t2; hdfs dfs -ls /user/hive/warehouse/test.db/t2; hdfs dfs -cat /user/hive/warehouse/test.db/t2/a.txt; 可以发现，overwrite之后依然只有一个文件，就是最后load 的文件。 动态分区插入 动态分区功能默认情况下是不开启的。分区以“严格”模式执行，在这种模式下要求至少有一个分区列是静态的。这有助于阻止因设计错误导致查询产生大量的分区。还有一些属性用于限制资源使用。 在本地文件a.txt中写入数据： aaa,US,CA aaa,US,CB bbb,CA,BB bbb,CA,BC 建立非分区表并装载数据 drop table if exists t1; create table t1(name string,city string,st string) row format delimited fields terminated by ','; load data local inpath '/home/scott/Documents/a.txt' overwrite into table t1; select * from t1; hdfs dfs -ls /user/hive/warehouse/test.db/t1; 此时数据未分区 建立外部分区表并动态装载数据 drop table if exists t2; create external table t2 (name string) partitioned by (country string, state string); set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.exec.max.dynamic.partitions.pernode=1000; insert into table t2 partition(country,state) select name,city,st from t1; insert into table t2 partition(country,state) select name,city,st from t1; select * from t2; hdfs dfs -ls /user/hive/warehouse/test.db/t2; 这里实际上插入了两次相同的数据，向外部分区表中装载了8条数据，动态建立了两个分区目录。 若在Hive中删除了外部表的元数据，但在HDFS中数据还在，因此需要恢复数据及分区。Hive提供了MSCK REPAIR TABLE的方法直接根据文件夹恢复分区。 create external table t2 (name string) partitioned by (country string, state string) row format delimited fields terminated by ',' LOCATION '/user/hive/warehouse/test.db/t2' ; MSCK REPAIR TABLE t2; 构建数据仓库模型 创建销售订单数据仓库中的表。 在这个场景中，源数据库表就是操作型系统的模拟。我们在MySQL中建立源数据库表。RDS存储原始数据，作为源数据到数据仓库的过渡，在Hive中建立RDS库表。TDS即为转化后的多维数据仓库，在Hive中建立TDS库表。 在MySQL中建立源数据库表：mysql -u root -p -- 建立源数据库 drop database if exists source; create database source; use source; -- 建立客户表 create table customer ( customer_number int not null auto_increment primary key comment '客户编号，主键', customer_name varchar(50) comment '客户名称', customer_street_address varchar(50) comment '客户住址', customer_zip_code int comment '邮编', customer_city varchar(30) comment '所在城市', customer_state varchar(2) comment '所在省份' ); -- 建立产品表 create table product ( product_code int not null auto_increment primary key comment '产品编码，主键', product_name varchar(30) comment '产品名称', product_category varchar(30) comment '产品类型' ); -- 建立销售订单表 create table sales_order ( order_number int not null auto_increment primary key comment '订单号，主键', customer_number int comment '客户编号', product_code int comment '产品编码', order_date datetime comment '订单日期', entry_date datetime comment '登记日期', order_amount decimal(10 , 2 ) comment '销售金额', foreign key (customer_number) references customer (customer_number) on delete cascade on update cascade, foreign key (product_code) references product (product_code) on delete cascade on update cascade ) 生成测试数据 use source; ##生成客户表测试数据 insert into customer (customer_name,customer_street_address,customer_zip_code,customer_city,customer_state) values ('really large customers', '7500 louise dr.',17050, 'mechanicsburg','pa'), ('small stores', '2500 woodland st.',17055, 'pittsburgh','pa'), ('medium retailers','1111 ritter rd.',17055,'pittsburgh','pa'), ('good companies','9500 scott st.',17050,'mechanicsburg','pa'), ('wonderful shops','3333 rossmoyne rd.',17050,'mechanicsburg','pa'), ('loyal clients','7070 ritter rd.',17055,'pittsburgh','pa'), ('distinguished partners','9999 scott st.',17050,'mechanicsburg','pa'); ##生成产品表测试数据 insert into product (product_name,product_category) values ('hard disk drive', 'storage'), ('floppy drive', 'storage'), ('lcd panel', 'monitor'); ##生成100条销售订单表测试数据 drop procedure if exists generate_sales_order_data; delimiter // create procedure generate_sales_order_data() begin drop table if exists temp_sales_order_data; create table temp_sales_order_data as select * from sales_order where 1=0; set @start_date := unix_timestamp('2016-03-01'); set @end_date := unix_timestamp('2016-07-01'); set @i := 1; while @i 在Hive中建立RDS库表： drop database if exists rds cascade; create database rds; use rds; -- 建立客户过渡表 CREATE TABLE customer ( customer_number INT comment 'number', customer_name VARCHAR(30) comment 'name', customer_street_address VARCHAR(30) comment 'address', customer_zip_code INT comment 'zipcode', customer_city VARCHAR(30) comment 'city', customer_state VARCHAR(2) comment 'state'); -- 建立产品过渡表 CREATE TABLE product ( product_code INT comment 'code', product_name VARCHAR(30) comment 'name', product_category VARCHAR(30) comment 'category' ); -- 建立销售订单过渡表 CREATE TABLE sales_order ( order_number INT comment 'order number', customer_number INT comment 'customer number', product_code INT comment 'product code', order_date TIMESTAMP comment 'order date', entry_date TIMESTAMP comment 'entry date', order_amount DECIMAL(10 , 2 ) comment 'order amount'); 在Hive中建立TDS库表 drop database if exists dw cascade; create database dw; use dw; -- 建立时间维度 create table date_dim( date_sk int, `date` date, month tinyint, month_name varchar(9), quarter tinyint, year smallint ) row format delimited fields terminated by ',' stored as textfile; -- 建立客户维度表 create table customer_dim ( customer_sk int, customer_number int, customer_name varchar(50), customer_street_address varchar(50), customer_zip_code int, customer_city varchar(30), customer_state varchar(2), version int, effective_date date, expiry_date date )clustered by(customer_sk) into 8 buckets stored as orc tblproperties('transactional' = 'true'); -- 建立产品维度表 create table product_dim ( product_sk int, product_code int, product_name varchar(30), product_category varchar(30), version int, effective_date date, expiry_date date )clustered by(product_sk) into 8 buckets stored as orc tblproperties('transactional' = 'true'); -- 建立订单维度表 create table order_dim ( order_sk int, order_number int, version int, effective_date date, expiry_date date )clustered by(order_sk) into 8 buckets stored as orc tblproperties('transactional' = 'true'); -- 建立销售订单事实表 create table sales_order_fact ( order_sk int, customer_sk int, product_sk int, order_date_sk int, order_amount decimal(10 , 2 ) )clustered by(order_sk) into 8 buckets stored as orc tblproperties('transactional' = 'true'); "},"sqoop.html":{"url":"sqoop.html","title":"Sqoop 抽取数据","keywords":"","body":"日期纬度装载 日期维度在数据仓库中是一个特殊角色。日期维度包含时间概念，而时间是最重要的，因为数据仓库的主要功能之一就是存储历史数据，所以每个数据仓库里的数据都有一个时间特征。 装载日期数据有三个常用方法： 预装载（最常见，最容易实现） 每日装载一天 从源数据装载日期 MySQL可以使用存储过程来插入数据，但是Hive不支持，所以使用shell脚本来执行日期的装载。 #!/bin/bash date1=\"{% math_inline %}1\" date2=\"{% endmath_inline %}2\" tempdate=`date -d \"{% math_inline %}date1\" +%F` tempdateSec=`date -d \"{% endmath_inline %}date1\" +%s` enddateSec=`date -d \"{% math_inline %}date2\" +%s` min=1 max=`expr \\( {% endmath_inline %}enddateSec - {% math_inline %}tempdateSec \\) / \\( 24 \\* 60 \\* 60 \\) + 1` cat /dev/null > ./date_dim.csv while [ {% endmath_inline %}min -le {% math_inline %}max ] do month=`date -d \"{% endmath_inline %}tempdate\" +%m` month_name=`date -d \"{% math_inline %}tempdate\" +%B` quarter=`echo {% endmath_inline %}month | awk '{print int(({% math_inline %}0-1)/3)+1}'` year=`date -d \"{% endmath_inline %}tempdate\" +%Y` echo {% math_inline %}{min}\",\"{% endmath_inline %}{tempdate}\",\"{% math_inline %}{month}\",\"{% endmath_inline %}{month_name}\",\"{% math_inline %}{quarter}\",\"{% endmath_inline %}{year} >> ./date_dim.csv tempdate=`date -d \"+{% math_inline %}min day {% endmath_inline %}date1\" +%F` tempdateSec=`date -d \"+{% math_inline %}min day {% endmath_inline %}date1\" +%s` min=`expr $min + 1` done 执行脚本，并生成日期纬度 ./date_dim_generate.sh 2000-01-01 2020-12-31 hdfs dfs -put -f date_dim.csv /user/hive/warehouse/dw.db/date_dim/ 数据抽取 逻辑数据映射 逻辑数据映射就是指源系统中的对象和目标数据仓库中的对象之间的对应关系，是建立ETL物理工作计划的指南。 包括三个组件： 目标组件。包括数据仓库中出现的物理表名称、表类型（事实表、维度表和子维度表等）、列名称、列的数据类型等。 源系统组件。包括数据源名称、源表名、源列名及其数据类型。 转换。源数据与期望的目标数据仓库格式对应所需的详细操作。 数据抽取方式 需要抽取哪部分源数据加载到数据仓库？有两种可选方式，完全抽取和变化数据捕获。 数据抽取的方向是什么？有两种方式， 拉模式，即数据仓库主动去源系统拉取数据； 推模式，由源系统将自己的数据推送给数据仓库。 对于方向，要改变或增加操作型业务系统的功能是非常困难，理论上讲，数据仓库不应该要求对源系统做任何改造。故选择拉模式。 数据量很小并且易处理，一般来说采取完全源数据抽取；如果源数据量很大，抽取全部数据不可行，只能抽取变化的源数据，这种数据抽取模式称为变化数据捕获，简称CDC。 CDC 基于时间戳的CDC 要求源数据里有相关的属性列，抽取过程可以利用这些属性列来判断哪些数据是增量数据。最常见的属性列有以下两种。 时间戳：这种方法至少需要一个更新时间戳，但最好有两个，一个插入时间戳，表示记录何时创建；一个更新时间戳，表示记录最后一次更新的时间。 序列：大多数数据库系统都提供自增功能。如果数据库表列被定义成自增的，就可以很容易地根据该列识别出新插入的数据。 缺点：不能区分插入和更新操作；不能记录删除记录的操作；无法识别多次更新；不具有实时能力。 基于触发器的CDC 当执行INSERT、UPDATE、DELETE这些SQL语句时，可以激活数据库里的触发器，并执行一些动作，就是说触发器可以用来捕获变更的数据并把数据保存到中间临时表里。然后这些变更的数据再从临时表中取出，被抽取到数据仓库的过渡区里。 缺点：大多数场合下，不允许向操作型数据库里添加触发器，降低系统的性能。 基于快照的CDC 通过比较源表和快照表来获得数据变化。快照就是一次性抽取源系统中的全部数据，把这些数据装载到数据仓库的过渡区中。 可以用全外连接比较源表与快照的差别。 缺点：需要大量的存储空间来保存快照。另外，当表很大时，这种查询会有比较严重的性能问题。 基于日志的CDC 实时从数据库日志中读取到所有数据库写操作，并使用这些操作来更新数据仓库中的数据。需要将日志的二进制格式转换为可以理解的格式（例如，mysqlbinlog） 缺点：实现复杂 实验 导出文本文件 show global variables like '%secure%'; use source; select * into outfile '/var/lib/mysql-files/product.txt' fields terminated by ',' from product; 使用mysqldump命令行工具，可以一次性导出多个表、多个库或所有库的数据，进入目标文件夹查看是否导入成功 sudo mysqldump -uroot -p123 source product -t -T /var/lib/mysql-files/ --fields-terminated-by=, // -t表示不导出create信息，-T参数指定导出文件的位置 Sqoop Sqoop是一个在Hadoop与结构化数据存储（如关系数据库）之间高效传输大批量数据的工具。 Sqoop的安装可参考这里。 Sqoop 显示数据库列表（需要MySQL的Jar包放入Sqoop/lib） sqoop list-datavases --connect jdbc:mysql://localhost:3306/ --username root --password 123 Sqoop显示数据库的表 sqoop list-tables --connect jdbc:mysql://localhost:3306/source --username root --password 123 例子 把MySQL中testdb.PERSON表的数据导入HDFS。 在mysql中创建数据库test，在test中创建表test1并插入两条数据。 create database test; use test; create table test1(id int,name varchar(10)); insert into test1 values(1,'aaa'); insert into test1 values(2,'bbb'); select * from test1; 使用sqoop导出数据 sqoop import --connect jdbc:mysql://localhost:3306/test --username root --password 123 --table test1 --target-dir /user/test/test1 --split-by id -m 1 查看结果 hdfs dfs -ls /user/test/test1 把HDFS的表导入mysql中 清空mysql 中test1表 truncate table test1; select * from test1; sqoop执行导入 sqoop export --connect jdbc:mysql://localhost:3306/test --username root --password 123 --table test1 --export-dir /user/test/test1 使用Sqoop抽取数据 首先将java-json.jar包加入sqoop/lib中，在这里下载 将hive/lib下的hive-common-2.3.0.jar 拷贝到sqoop/lib下。 建立sqoop增量导入作业，使用sqoop job -list 查看。 ##创建作业 sqoop job --create myjob_1 \\ -- \\ import \\ --connect \"jdbc:mysql://localhost:3306/source?useSSL=false&user=root&password=scott5183\" \\ --table sales_order \\ --columns \"order_number, customer_number, product_code, order_date, entry_date, order_amount\" \\ --where \"entry_date 查看此时作业中保存的last-value sqoop job --show myjob_1 | grep last.value 增量导入，执行sqoop job --exec myjob_1 查看此时作业中保存的last-value 在MySQL中执行（增加两条数据） use source; set @customer_number := floor(1 + rand()* 6); set @product_code := floor(1 + rand()* 2); set @order_date := from_unixtime(unix_timestamp('2016-07-03') + rand()*(unix_timestamp('2016-07-04') - unix_timestamp('2016-07-03'))); set @amount := floor(1000 + rand() * 9000); insert into sales_order values(101,@customer_number,@product_code,@order_date,@order_date,@amount); set @customer_number := floor(1 + rand()* 6); set @product_code := floor(1 + rand()* 2); set @order_date := from_unixtime(unix_timestamp('2016-07-04') + rand()*(unix_timestamp('2016-07-05') - unix_timestamp('2016-07-04'))); set @amount := floor(1000 + rand()* 9000); insert into sales_order values(102,@customer_number,@product_code,@order_date,@order_date,@amount); commit; 在hive的rds库里查询，前两行记录如下所示。 select * from sales_order where order_number in (101,102); 还原数据 use source; delete from sales_order where order_number in (101,102); alter table sales_order auto_increment=101; "},"data cleaning.html":{"url":"data cleaning.html","title":"数据清洗","keywords":"","body":"介绍 对大多数用户来说，ETL的核心价值在T所代表的转换部分。这个阶段要做很多工作，数据清洗就是其中一项重点任务。数据清洗是对数据进行重新审查和校验的过程，目的在于删除重复信息、纠正存在的错误，并提供数据一致性。 处理脏数据 数据仓库中的数据是面向某一主题数据的集合，这些数据从多个业务系统中抽取而来，并且包含历史数据，因此就不可避免地出现某些数据是错误的，或者数据相互之间存在冲突的情况。这些错误的或有冲突的数据显然不是我们想要的，被称为“脏数据”。我们要按照一定的规则处理脏数据，这个过程就是数据清洗。数据清洗的任务是过滤那些不符合要求的数据，将过滤的结果交给业务主管部门，确认是直接删除掉，还是修正之后再进行抽取。 不符合要求的数据主要是残缺的数据、错误的数据、重复的数据、差异的数据四大类。 残缺数据 这一类数据主要是一些应该有的信息缺失，如产品名称、客户名称、客户的区域信息，还包括业务系统中由于缺少外键约束所导致的主表与明细表不能匹配等。 错误数据 这一类错误产生的原因多是业务系统不够健全，在接收输入后没有进行合法性检查或检查不够严格，将有问题的数据直接写入后台数据库造成的，比如用字符串存储数字、超出合法的取值范围、日期格式不正确、日期越界等。 重复数据 源系统中相同的数据存在多份。 差异数据 本来具有同一业务含义的数据，因为来自不同的操作型数据源，造成数据不一致。这时需要将非标准的数据转化为在一定程度上的标准化数据。 数据清洗原则 保障数据清洗处理顺利进行的原则是优先对数据清洗处理流程进行分析和系统化的设计，针对数据的主要问题和特征，设计一系列数据对照表和数据清洗程序库的有效组合，以便面对不断变化的、形形色色的数据清洗问题。数据清洗流程通常包括如下内容。 预处理 对于大的数据加载文件，特别是新的文件和数据集合，要进行预先诊断和检测，不能贸然加载。有时需要临时编写程序进行数据清洁检查。 标准化处理 应用建于数据仓库内部的标准字典，对于地区名、人名、公司名、产品名、分类名以及各种编码信息进行标准化处理。 查重 应用各种数据库查询技术和手段，避免引入重复数据。 出错处理和修正 将出错的记录和数据写入到日志文件，留待进一步处理。 实例 身份证号码检查 身份证号码格式校验是很多系统在数据集成时的一个常见需求，我们以18位身份证为例，使用一个Hive查询实现身份证号码的合法性验证。该查询结果是所有不合规的身份证号码。按以下身份证号码的定义规则建立查询。 身份证18位分别代表的含义，从左到右方分别表示： 1～2，省级行政区代码。 3～4，地级行政区划分代码。 5～6，县区行政区分代码。 7～10、11～12、13～14，出生年、月、日。 15～17，顺序码，同一地区同年、同月、同日出生人的编号，奇数是男性，偶数是女性。 18校验码，如果是0～9则用0～9表示，如果是10则用X（罗马数字10）表示。 验证码计算方法 将前面的身份证号码17位数分别乘以不同的系数。从第1位到第17位的系数分别为：7－9－10－5－8－4－2－1－6－3－7－9－10－5－8－4－2。 将这17位数字和系数相乘的结果相加。 用加出来和除以11，看余数是多少。 余数只可能有0－1－2－3－4－5－6－7－8－9－10这11个数字。其分别对应的最后一位身份证的号码为1－0－X －9－8－7－6－5－4－3－2。 首先建立test表 create table test(idcard varchar(64)); insert into test values(\"453488808776763114\"); insert into test values(\"500101545645553119\"); insert into test values(\"437387322203063116\"); 简单的验证代码 select * from (select trim(upper(idcard)) idcard from test) t1 where length(idcard) <>18 or substr(idcard,1,2) not in ('11','12','13','14','15','21','22','23','31','32','33','34','35','36','37','41','42','43','44','45','46','50','51','52','53','54','61','62','63','64','65','71','81','82','91') or substr('10X98765432',pmod( (cast(substr(idcard,1,1) as int) + cast(substr(idcard,11,1) as int))*7 +(cast(substr(idcard,2,1) as int) + cast(substr(idcard,12,1) as int))*9 +(cast(substr(idcard,3,1) as int) + cast(substr(idcard,13,1) as int))*10 +(cast(substr(idcard,4,1) as int) + cast(substr(idcard,14,1) as int))*5 +(cast(substr(idcard,5,1) as int) + cast(substr(idcard,15,1) as int))*8 +(cast(substr(idcard,6,1) as int) + cast(substr(idcard,16,1) as int))*4 +(cast(substr(idcard,7,1) as int) + cast(substr(idcard,17,1) as int))*2 +cast(substr(idcard,8,1) as int)*1 +cast(substr(idcard,9,1) as int)*6 +cast(substr(idcard,10,1) as int)*3,11)+1,1) <>cast(substr(idcard,18,1) as int); 直接可检查出无效的身份证号码： 去除重复数据 有两个意义上的重复记录，一是完全重复的记录，也即所有字段均都重复，二是部分字段重复的记录。 对于第一种重复，比较容易解决，只需在查询语句中使用distinct关键字去重，几乎所有数据库系统都支持distinct操作。发生这种重复的原因主要是表设计不周，通过给表增加主键或唯一索引列即可避免。 use test; drop table if exists test; create table test(c1 int, c2 int); insert into test values(1,1); insert into test values(2,2); insert into test values(1,1); select * from test; select distinct * from test; 对于第二类重复问题，通常要求查询出重复记录中的任一条记录。假设表t有id、name、address三个字段，id是主键，有重复的字段为name、address，要求得到这两个字段唯一的结果集。 drop table if exists test; create table test(id int,name varchar(10),address varchar(20)); insert into test values(1,'abc','abcdefg'); insert into test values(2,'dfs','fgdgdfg'); insert into test values(3,'dfs','fgdgdfg'); select * from test t1 where t1.id = (select min(t2.id) from test t2 where t1.name = t2.name and t1.address = t2.address); Hive支持在FROM子句中使用子查询，子查询必须有名字，并且列必须唯一（也支持子查询） drop table if exists test; create table test(id int,name varchar(10),address varchar(20)); insert into test values(1,'abc','abcdefg'); insert into test values(2,'dfs','fgdgdfg'); insert into test values(3,'dfs','fgdgdfg'); select t1.* from test t1, (select name,address,min(id) id from test group by name,address) t2 where t1.id = t2.id; 还可以使用Hive的row_number()分析函数 select test.id,test.name,test.address from (select id,name,address,row_number() over (distribute by name,address sort by id)as rn from test) test where test.rn=1; 清洗日志 创建源表 use test; create table IF NOT EXISTS source_log ( remote_addr string, remote_user string, time_local string, request string, status string, body_bytes_sent string, request_body string, http_referer string, http_user_agent string, http_x_forwarded_for string, host string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( \"input.regex\" = \"(\\\"[^ ]*\\\") (\\\"[-|^ ]*\\\") (\\\"[^\\\"]*\\\") (\\\"[^\\\"]*\\\") (\\\"[0-9]*\\\") (\\\"[0-9]*\\\") ([-|^ ]*) (\\\"[^ ]*\\\") (\\\"[^\\\"]*\\\") (\\\"[-|^ ]*\\\") (\\\"[^ ]*\\\")\" ) STORED AS TEXTFILE; 加载数据，先创建本地文件source.log，插入以下数据： \"27.38.5.159\" \"-\" \"31/Aug/2015:00:04:37 +0800\" \"GET /course/view.php?id=27 HTTP/1.1\" \"303\" \"440\" - \"http://www.ibeifeng.com/user.php?act=mycourse\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36\" \"-\" \"learn.ibeifeng.com\" \"27.38.5.159\" \"-\" \"31/Aug/2015:00:04:37 +0800\" \"GET /login/index.php HTTP/1.1\" \"303\" \"465\" - \"http://www.ibeifeng.com/user.php?act=mycourse\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36\" \"-\" \"learn.ibeifeng.com\" 执行命令：load data local inpath '/home/scott/Documents/source.log' into table source_log; 过滤字段（去掉每个字段的双引号） create table clean_log( remote_addr string, time_local string, request string, http_referer string ) row format delimited fields terminated by '\\t' stored as parquet tblproperties(\"parquet.compress\"=\"SNAPPY\"); insert into clean_log select regexp_replace(remote_addr, '\"', ''), regexp_replace(time_local, '\"', ''), regexp_replace(request, '\"', ''), regexp_replace(http_referer, '\"', '') from source_log; 过滤字段（截取请求地址） 从 \"GET /course/view.php?id=27HTTP/1.1\" 中获取请求地址，即/course/view.php?id=27 select split(request,' ')[1] from clean_log; 过滤字段（截取referer主地址） 从\"http://www.ibeifeng.com/user.php?act=mycourse\"提取主地址，即\"http://www.ibeifeng.com\" select REGEXP_EXTRACT(http_referer, '^(?:.*://)?(?:www\\.)?([^:/]*).*$', 1) from source_log; "},"first load.html":{"url":"first load.html","title":"初始装载","keywords":"","body":"初始装载 在数据仓库可以使用前，需要装载历史数据。这些历史数据是导入进数据仓库的第一个数据集合。 首次装载被称为初始装载，一般是一次性工作。由最终用户来决定有多少历史数据进入数据仓库。例如，数据仓库使用的开始时间是2015年3月1日，而用户希望装载两年的历史数据，那么应该初始装载2013年3月1日到2015年2月28日之间的源数据。在2015年3月2日装载2015年3月1日的数据（假设执行频率是每天一次），之后周期性地每天装载前一天的数据。 在装载事实表前，必须先装载所有的维度表。因为事实表需要引用维度的代理键。这不仅针对初始装载，也针对定期装载。本次实验主要说明执行初始装载的步骤，包括标识源数据、维度历史的处理、使用HiveQL开发和验证初始装载过程。 标识源数据 设计开发初始装载步骤前需要识别数据仓库的每个事实表和每个维度表用到的并且是可用的源数据，还要了解数据源的特性，例如文件类型、记录结构和可访问性等。 下表显示的是销售订单示例数据仓库需要的源数据的关键信息，包括源数据表、对应的数据仓库目标表等属性。这类表格通常称作数据源对应图，因为它反映了每个从源数据到目标数据的对应关系。生成这个表格的过程就是逻辑数据映射。在本示例中，客户和产品的源数据直接与其数据仓库里的目标表customer_dim和product_dim表相对应，而销售订单事务表是多个数据仓库表的数据源。 纬度历史的数据 大多数维度值是随着时间改变的，如客户改变了姓名，产品的名称或分类变化等。当一个维度改变，比如当一个产品有了新的分类时，有必要记录维度的历史变化信息。在这种情况下，product_dim表里必须既存储产品老的分类，也存储产品当前的分类。并且，老的销售订单里的产品引用老的分类。 渐变维（SCD）即是一种在多维数据仓库中实现维度历史的技术。有三种不同的SCD技术：SCD类型1（SCD1），SCD类型2（SCD2），SCD类型3（SCD3）。 SCD1：通过更新维度记录直接覆盖已存在的值，它不维护记录的历史。SCD1一般用于修改错误的数据。 SCD2：在源数据发生变化时，给维度记录建立一个新的“版本”记录，从而维护维度历史。SCD2不删除、修改已存在的数据。 SCD3：通常用作保持维度记录的几个版本。它通过给某个数据单元增加多个列来维护历史。例如，为了记录客户地址的变化，customer_dim维度表有一个customer_address列和一个previous_customer_address列，分别记录当前和上一个版本的地址。SCD3可以有效维护有限的历史，而不像SCD2那样保存全部历史。SCD3很少使用。它只适用于数据的存储空间不足并且用户接受有限维度历史的情况。 纬度历史的处理 同一维度表中的不同字段可以有不同的变化处理方式。在本示例中，客户维度历史的客户名称使用SCD1，客户地址使用SCD2，产品维度的两个属性，产品名称和产品类型都使用SCD2保存历史变化数据。 多维数据仓库中的维度表和事实表一般都需要有一个代理键，作为这些表的主键，代理键一般由单列的自增数字序列构成。Hive没有关系数据库中的自增列，但它也有一些对自增序列的支持，通常有两种方法生成代理键：使用row_number()窗口函数或者使用一个名为UDFRowSequence的用户自定义函数（UDF）。 假设有维度表tbl_dim和过渡表tbl_stg，现在要将tbl_stg的数据装载到tbl_dim，装载的同时生成维度表的代理键。 用row_number()函数生成代理键 先建立测试所需的维度表和事实表，并插入数据： use test; drop table if exists tbl_dim; drop table if exists tbl_stg; create table tbl_stg(id int, sk int, address varchar(10)); insert into tbl_stg values(2,2,'qwe'); insert into tbl_stg values(3,3,'asd'); insert into tbl_stg values(4,4,'zxc'); create table tbl_dim(key int, id int,sk int, address varchar(10)); 用row_number()函数生成代理建 set hive.mapred.mode; set hive.mapred.mode=nonstrict; set hive.mapred.mode; set hive.strict.checks.cartesian.product; set hive.strict.checks.cartesian.product=false; set hive.strict.checks.cartesian.product; 先查询维度表中已有记录最大的代理键值，如果维度表中还没有记录，利用coalesce函数返回0。然后使用cross join连接生成过渡表和最大代理键值的笛卡尔集，最后使用row_number()函数生成行号，并将行号与最大代理键值相加的值，作为新装载记录的代理键。 insert into tbl_dim select row_number() over (order by tbl_stg.id) + t2.sk_max, tbl_stg.* from tbl_stg cross join (select coalesce(max(sk),0) sk_max from tbl_dim) t2; 此时，维度表中已经装载了数据： 用UDFRowSequence生成代理键 hive-contrib-2.0.0.jar中包含一个生成记录序号的自定义函数udfrowsequence。上面的语句先加载JAR包，然后创建一个名为row_sequence()的临时函数作为调用UDF的接口，这样可以为查询的结果集生成一个自增伪列。之后就和row_number()写法类似了，只不过将窗口函数row_number()替换为row_sequence()函数。 truncate table tbl_dim; add jar /usr/local/hive/lib/hive-contrib-2.3.0.jar; create temporary function row_sequence as 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence'; insert into tbl_dim select row_sequence() + t2.sk_max, tbl_stg.* from tbl_stg cross join(select coalesce(max(sk),0) sk_max from tbl_dim)t2; 结果与之前相同 初始数据抽取 建立sqoop导入作业，抽取sales_order数据到RDS库 use rds; truncate table customer; truncate table product; truncate table sales_order; ##创建作业 sqoop job --delete myjob_incremental_import sqoop job --create myjob_incremental_import \\ -- import \\ --connect \"jdbc:mysql://localhost:3306/source?useSSL=false&user=root&password=scott5183\" \\ --table sales_order \\ --columns \"order_number, customer_number, product_code, order_date, entry_date, order_amount\" \\ --hive-import \\ --hive-table rds.sales_order \\ --incremental append \\ --check-column order_amount \\ --last-value 0 sqoop job --exec myjob_incremental_import 全量抽取客户和产品数据到RDS库 sqoop import --connect jdbc:mysql://localhost:3306/source?useSSL=false --username=root --password scott5183 --table customer --hive-import --hive-table rds.customer --hive-overwrite sqoop import --connect jdbc:mysql://localhost:3306/source?useSSL=false --username=root --password scott5183 --table product --hive-import --hive-table rds.product --hive-overwrite 装载客户维度表 use dw; truncate table customer_dim; truncate table product_dim; truncate table order_dim; truncate table sales_order_fact; insert into customer_dim select row_number() over (order by t1.customer_number) + t2.sk_max, t1.customer_number, t1.customer_name, t1.customer_street_address,t1.customer_zip_code, t1.customer_city, t1.customer_state,1,'2016-03-01','2200-01-01' from rds.customer t1 cross join(select coalesce(max (customer_sk),0) sk_max from customer_dim) t2; 装载产品维度表 insert into product_dim select row_number() over (order by t1.product_code) + t2.sk_max, product_code, product_name, product_category,1, '2016-03-01','2200-01-01' from rds.product t1 cross join(select coalesce(max (product_sk),0) sk_max from product_dim) t2; 装载订单维度表 insert into order_dim select row_number() over (order by t1.order_number) + t2.sk_max, order_number,1, order_date,'2200-01-01' from rds.sales_order t1 cross join(select coalesce(max (order_sk),0) sk_max from order_dim) t2; 装载销售订单事实表 insert into sales_order_fact select order_sk,customer_sk,product_sk,date_sk,order_amount from rds.sales_order a,order_dim b, customer_dim c, product_dim d, date_dim e where a.order_number = b.order_number and a.customer_number = c.customer_number and a.product_code = d.product_code and to_date(a.order_date) = e.`date`; 验证初始装载的正确性 use dw; select * from customer_dim; select * from product_dim; select * from order_dim; select * from sales_order_fact; select * from date_dim; select order_number, customer_name, product_name,`date`, order_amount amount from sales_order_fact a, customer_dim b,product_dim c ,order_dim d, date_dim e where a.customer_sk=b.customer_sk and a.product_sk = c.product_sk and a.order_sk = d.order_sk and a.order_date_sk = e.date_sk order by order_number; "},"time load.html":{"url":"time load.html","title":"定期装载","keywords":"","body":"定期装载 初始装载只在开始数据仓库使用前执行一次，然而，必须要按时调度定期执行装载源数据的过程。与初始装载不同，定期装载一般都是增量的，需要捕获并且记录数据的变化历史。本节说明执行定期装载的步骤，包括识别源数据与装载类型、使用HiveQL开发和测试定期装载过程。 定期装载首先要识别数据仓库的每个事实表和每个维度表用到的并且是可用的源数据。然后要决定适合装载的抽取模式和维度历史装载类型。 源数据 RDS 数据仓库 抽取模式 维度历史装载类型 customer customer customer_dim 整体、拉取 address列上SCD2name列上SCD1 product product product_dim 整体、拉取 SCD2 sales_order sales_order order_dim CDC（每天）、拉取 唯一订单号 sales_order_fact CDC（每天）、拉取 n/a n/a n/a date_dim n/a 预装载 前期准备 order_dim维度表和sales_order_fact事实表使用基于时间戳的CDC装载模式。为此在rds库中建立一个名cdc_time的时间戳表。 USE rds; DROP TABLE IF EXISTS cdc_time ; CREATE TABLE cdc_time ( last_load date, current_load date ); SET hivevar:last_load = DATE_ADD(CURRENT_DATE(),-1); INSERT OVERWRITE TABLE cdc_time SELECT {% math_inline %}{hivevar:last_load}, {% endmath_inline %}{hivevar:last_load} ; 编写装载脚本 使用下面的regular_etl.sh脚本完成定期装载过程。 #!/bin/bash # 整体拉取customer、product表数据 sqoop import --connect jdbc:mysql://localhost:3306/source?useSSL=false --username root --password scott5183 --table customer --hive-import --hive-table rds.customer --hive-overwrite sqoop import --connect jdbc:mysql://localhost:3306/source?useSSL=false --username root --password scott5183 --table product --hive-import --hive-table rds.product --hive-overwrite # 执行增量导入 sqoop job --exec myjob_incremental_import # 调用 regular_etl.sql 文件执行定期装载 hive -f regular_etl.sql --hiveconf hive.mapred.mode=nonstrict 设置数据处理时间窗口 在脚本中设置三个变量，分别赋予起始时间点、终止时间点、最大时间点的值，并且将时间戳表rds.cdc_time的last_load和current_load字段分别设置为起始时间点和终止时间点。这些变量会在后面的脚本中多次引用。 -- 设置SCD的生效时间和过期时间 SET hivevar:cur_date = CURRENT_DATE(); SET hivevar:pre_date = DATE_ADD({% math_inline %}{hivevar:cur_date},-1); SET hivevar:max_date = CAST('2200-01-01' AS DATE); -- 设置CDC的上限时间 INSERT OVERWRITE TABLE rds.cdc_time SELECT last_load, {% endmath_inline %}{hivevar:cur_date} FROM rds.cdc_time; 装载客户维度表 客户维度表的customer_street_addresses字段值变化时采用SCD2，需要新增版本，customer_name字段值变化时采用SCD1，直接覆盖更新。 -- 装载customer维度 -- 设置已删除记录和customer_street_addresses列上SCD2的过期 UPDATE customer_dim SET expiry_date = {% math_inline %}{hivevar:pre_date} WHERE customer_dim.customer_sk IN (SELECT a.customer_sk FROM (SELECT customer_sk,customer_number,customer_street_address FROM customer_dim WHERE expiry_date = {% endmath_inline %}{hivevar:max_date}) a LEFT JOIN rds.customer b ON a.customer_number = b.customer_number WHERE b.customer_number IS NULL OR a.customer_street_address <> b.customer_street_address); -- 处理customer_street_addresses列上SCD2的新增行 INSERT INTO customer_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.customer_number) + t2.sk_max, t1.customer_number, t1.customer_name, t1.customer_street_address, t1.customer_zip_code, t1.customer_city, t1.customer_state, t1.version, t1.effective_date, t1.expiry_date FROM ( SELECT t2.customer_number customer_number, t2.customer_name customer_name, t2.customer_street_address customer_street_address, t2.customer_zip_code, t2.customer_city, t2.customer_state, t1.version + 1 version, {% math_inline %}{hivevar:pre_date} effective_date, {% endmath_inline %}{hivevar:max_date} expiry_date FROM customer_dim t1 INNER JOIN rds.customer t2 ON t1.customer_number = t2.customer_number AND t1.expiry_date = {% math_inline %}{hivevar:pre_date} LEFT JOIN customer_dim t3 ON t1.customer_number = t3.customer_number AND t3.expiry_date = {% endmath_inline %}{hivevar:max_date} WHERE t1.customer_street_address <> t2.customer_street_address AND t3.customer_sk IS NULL) t1 CROSS JOIN (SELECT COALESCE(MAX(customer_sk),0) sk_max FROM customer_dim) t2; 装载产品纬度表 - 装载product维度 -- 设置已删除记录和product_name、product_category列上SCD2的过期 UPDATE product_dim SET expiry_date = {% math_inline %}{hivevar:pre_date} WHERE product_dim.product_sk IN (SELECT a.product_sk FROM (SELECT product_sk,product_code,product_name,product_category FROM product_dim WHERE expiry_date = {% endmath_inline %}{hivevar:max_date}) a LEFT JOIN rds.product b ON a.product_code = b.product_code WHERE b.product_code IS NULL OR (a.product_name <> b.product_name OR a.product_category <> b.product_category)); 装载订单纬度表 -- 装载order维度 INSERT INTO order_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.order_number) + t2.sk_max, t1.order_number, t1.version, t1.effective_date, t1.expiry_date FROM ( SELECT order_number order_number, 1 version, order_date effective_date, '2200-01-01' expiry_date FROM rds.sales_order, rds.cdc_time WHERE entry_date >= last_load AND entry_date 装载销售事实表 -- 装载销售订单事实表 INSERT INTO sales_order_fact SELECT order_sk, customer_sk, product_sk, date_sk, order_amount FROM rds.sales_order a, order_dim b, customer_dim c, product_dim d, date_dim e, rds.cdc_time f WHERE a.order_number = b.order_number AND a.customer_number = c.customer_number AND a.order_date >= c.effective_date AND a.order_date = d.effective_date AND a.order_date = f.last_load AND a.entry_date 更新时间窗口 -- 更新时间戳表的last_load字段 INSERT OVERWRITE TABLE rds.cdc_time SELECT current_load, current_load FROM rds.cdc_time; 总脚本 -- 设置变量以支持事务 set hive.support.concurrency=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager; set hive.compactor.initiator.on=true; set hive.compactor.worker.threads=1; USE dw; -- 设置SCD的生效时间和过期时间 SET hivevar:cur_date = CURRENT_DATE(); SET hivevar:pre_date = DATE_ADD({% math_inline %}{hivevar:cur_date},-1); SET hivevar:max_date = CAST('2200-01-01' AS DATE); -- 设置CDC的上限时间 INSERT OVERWRITE TABLE rds.cdc_time SELECT last_load, {% endmath_inline %}{hivevar:cur_date} FROM rds.cdc_time; -- 装载customer维度 -- 设置已删除记录和customer_street_addresses列上SCD2的过期 UPDATE customer_dim SET expiry_date = {% math_inline %}{hivevar:pre_date} WHERE customer_dim.customer_sk IN (SELECT a.customer_sk FROM (SELECT customer_sk,customer_number,customer_street_address FROM customer_dim WHERE expiry_date = {% endmath_inline %}{hivevar:max_date}) a LEFT JOIN rds.customer b ON a.customer_number = b.customer_number WHERE b.customer_number IS NULL OR a.customer_street_address <> b.customer_street_address); -- 处理customer_street_addresses列上SCD2的新增行 INSERT INTO customer_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.customer_number) + t2.sk_max, t1.customer_number, t1.customer_name, t1.customer_street_address, t1.customer_zip_code, t1.customer_city, t1.customer_state, t1.version, t1.effective_date, t1.expiry_date FROM ( SELECT t2.customer_number customer_number, t2.customer_name customer_name, t2.customer_street_address customer_street_address, t2.customer_zip_code, t2.customer_city, t2.customer_state, t1.version + 1 version, {% math_inline %}{hivevar:pre_date} effective_date, {% endmath_inline %}{hivevar:max_date} expiry_date FROM customer_dim t1 INNER JOIN rds.customer t2 ON t1.customer_number = t2.customer_number AND t1.expiry_date = {% math_inline %}{hivevar:pre_date} LEFT JOIN customer_dim t3 ON t1.customer_number = t3.customer_number AND t3.expiry_date = {% endmath_inline %}{hivevar:max_date} WHERE t1.customer_street_address <> t2.customer_street_address AND t3.customer_sk IS NULL) t1 CROSS JOIN (SELECT COALESCE(MAX(customer_sk),0) sk_max FROM customer_dim) t2; -- 处理customer_name列上的SCD1 -- 因为hive里update的set子句还不支持子查询，所以这里使用了一个临时表存储需要更新的记录，用先delete再insert代替update，为简单起见也不考虑并发问题（数据仓库应用的并发操作基本都是只读的，很少并发写，所以并发导致的问题并不像OLTP那样严重）。 -- 因为SCD1本身就不保存历史数据，所以这里更新维度表里的所有customer_name改变的记录，而不是仅仅更新当前版本的记录 DROP TABLE IF EXISTS tmp; CREATE TABLE tmp AS SELECT a.customer_sk, a.customer_number, b.customer_name, a.customer_street_address, a.customer_zip_code, a.customer_city, a.customer_state, a.version, a.effective_date, a.expiry_date FROM customer_dim a, rds.customer b WHERE a.customer_number = b.customer_number AND (a.customer_name <> b.customer_name); DELETE FROM customer_dim WHERE customer_dim.customer_sk IN (SELECT customer_sk FROM tmp); INSERT INTO customer_dim SELECT * FROM tmp; -- 处理新增的customer记录 INSERT INTO customer_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.customer_number) + t2.sk_max, t1.customer_number, t1.customer_name, t1.customer_street_address, t1.customer_zip_code, t1.customer_city, t1.customer_state, 1, {% math_inline %}{hivevar:pre_date}, {% endmath_inline %}{hivevar:max_date} FROM ( SELECT t1.* FROM rds.customer t1 LEFT JOIN customer_dim t2 ON t1.customer_number = t2.customer_number WHERE t2.customer_sk IS NULL) t1 CROSS JOIN (SELECT COALESCE(MAX(customer_sk),0) sk_max FROM customer_dim) t2; -- 装载product维度 -- 设置已删除记录和product_name、product_category列上SCD2的过期 UPDATE product_dim SET expiry_date = {% math_inline %}{hivevar:pre_date} WHERE product_dim.product_sk IN (SELECT a.product_sk FROM (SELECT product_sk,product_code,product_name,product_category FROM product_dim WHERE expiry_date = {% endmath_inline %}{hivevar:max_date}) a LEFT JOIN rds.product b ON a.product_code = b.product_code WHERE b.product_code IS NULL OR (a.product_name <> b.product_name OR a.product_category <> b.product_category)); -- 处理product_name、product_category列上SCD2的新增行 INSERT INTO product_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.product_code) + t2.sk_max, t1.product_code, t1.product_name, t1.product_category, t1.version, t1.effective_date, t1.expiry_date FROM ( SELECT t2.product_code product_code, t2.product_name product_name, t2.product_category product_category, t1.version + 1 version, {% math_inline %}{hivevar:pre_date} effective_date, {% endmath_inline %}{hivevar:max_date} expiry_date FROM product_dim t1 INNER JOIN rds.product t2 ON t1.product_code = t2.product_code AND t1.expiry_date = {% math_inline %}{hivevar:pre_date} LEFT JOIN product_dim t3 ON t1.product_code = t3.product_code AND t3.expiry_date = {% endmath_inline %}{hivevar:max_date} WHERE (t1.product_name <> t2.product_name OR t1.product_category <> t2.product_category) AND t3.product_sk IS NULL) t1 CROSS JOIN (SELECT COALESCE(MAX(product_sk),0) sk_max FROM product_dim) t2; -- 处理新增的product记录 INSERT INTO product_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.product_code) + t2.sk_max, t1.product_code, t1.product_name, t1.product_category, 1, {% math_inline %}{hivevar:pre_date}, {% endmath_inline %}{hivevar:max_date} FROM ( SELECT t1.* FROM rds.product t1 LEFT JOIN product_dim t2 ON t1.product_code = t2.product_code WHERE t2.product_sk IS NULL) t1 CROSS JOIN (SELECT COALESCE(MAX(product_sk),0) sk_max FROM product_dim) t2; -- 装载order维度 INSERT INTO order_dim SELECT ROW_NUMBER() OVER (ORDER BY t1.order_number) + t2.sk_max, t1.order_number, t1.version, t1.effective_date, t1.expiry_date FROM ( SELECT order_number order_number, 1 version, order_date effective_date, '2200-01-01' expiry_date FROM rds.sales_order, rds.cdc_time WHERE entry_date >= last_load AND entry_date = c.effective_date AND a.order_date = d.effective_date AND a.order_date = f.last_load AND a.entry_date 测试执行脚本 执行MySQL脚本更新源数据 USE source; /*** 客户数据的改变如下： 客户6的街道号改为7777 Ritter Rd。（原来是7070 Ritter Rd） 客户7的姓名改为Distinguished Agencies。（原来是Distinguished Partners） 新增第八个客户。 ***/ UPDATE customer SET customer_street_address = '7777 Ritter Rd.' WHERE customer_number = 6 ; UPDATE customer SET customer_name = 'Distinguished Agencies' WHERE customer_number = 7 ; INSERT INTO customer (customer_name, customer_street_address, customer_zip_code, customer_city, customer_state) VALUES ('Subsidiaries', '10000 Wetline Blvd.', 17055, 'Pittsburgh', 'PA') ; /*** 产品数据的改变如下： 产品3的名称改为Flat Panel。（原来是LCD Panel） 新增第四个产品。 ***/ UPDATE product SET product_name = 'Flat Panel' WHERE product_code = 3 ; INSERT INTO product (product_name, product_category) VALUES ('Keyboard', 'Peripheral') ; /*** 新增订单日期为2016年7月4日的16条订单。 ***/ SET @start_date := unix_timestamp('2016-07-04'); SET @end_date := unix_timestamp('2016-07-05'); DROP TABLE IF EXISTS temp_sales_order_data; CREATE TABLE temp_sales_order_data AS SELECT * FROM sales_order WHERE 1=0; SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (101, 1, 1, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (102, 2, 2, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (103, 3, 3, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (104, 4, 4, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (105, 5, 2, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (106, 6, 2, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (107, 7, 3, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (108, 8, 4, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (109, 1, 1, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (110, 2, 2, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (111, 3, 3, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (112, 4, 4, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (113, 5, 1, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (114, 6, 2, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (115, 7, 3, @order_date, @order_date, @amount); SET @order_date := from_unixtime(@start_date + rand() * (@end_date - @start_date)); SET @amount := floor(1000 + rand() * 9000); INSERT INTO temp_sales_order_data VALUES (116, 8, 4, @order_date, @order_date, @amount); INSERT INTO sales_order SELECT NULL,customer_number,product_code,order_date,entry_date,order_amount FROM temp_sales_order_data ORDER BY order_date; COMMIT ; 在MySQ中执行： source /home/scott/Documents/update.sql; 查看 use dw; select * from customer_dim; select * from product_dim; select * from order_dim; select * from sales_order_fact; select * from date_dim; 执行regular_etl.sh脚本进行定期装载 ./regular_etl.sh 查看数据库变化 "},"optimization.html":{"url":"optimization.html","title":"Hive 优化","keywords":"","body":"Hive优化 Hive的执行依赖于底层的MapReduce作业，因此对Hadoop作业的优化或者对MapReduce作业的调整是提高Hive性能的基础。 大多数情况下，用户不需要了解Hive内部是如何工作的。但是当对Hive具有越来越多的经验后，学习一些Hive的底层实现细节和优化知识，会让用户更加高效地使用Hive。如果没有适当的调整，那么即使查询Hive中的一个小表，有时也会耗时数分钟才得到结果。也正是因为这个原因，Hive对于OLAP类型的应用有很大的局限性，它不适合需要立即返回查询结果的场景。然而，通过实施下面一系列的调优方法，Hive查询的性能会有大幅提高。 启用压缩 压缩可以使磁盘上存储的数据量变小，例如，文本文件格式能够压缩40%甚至更高比例，这样可以通过降低I/O来提高查询速度。 一个复杂的Hive查询在提交后，通常被转换为一系列中间阶段的MapReduce作业，Hive引擎将这些作业串联起来完成整个查询。可以将这些中间数据进行压缩。 在hive/conf/hive-site.xml中添加： hive.exec.compress.intermediate true hive.intermediate.compression.codec org.apache.hadoop.io.compress.SnappyCodec hive.intermediate.compression.type BLOCK 当Hive将输出写入到表中时，输出内容同样可以进行压缩。我们可以设置hive.exec.compress.output属性启用最终输出压缩。 hive.exec.compress.output true This controls whether the final outputs of a query (to a local/hdfs file or a Hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* 优化连接 可以通过配置Map连接和倾斜连接的相关属性提升连接查询的性能。 自动Map连接 当连接一个大表和一个小表时，自动Map连接是一个非常有用的特性。如果启用了该特性，小表将保存在每个节点的本地缓存中，并在Map阶段与大表进行连接。开启自动Map连接提供了两个好处。首先，将小表装进缓存将节省每个数据节点上的读取时间。其次，它避免了Hive查询中的倾斜连接，因为每个数据块的连接操作已经在Map阶段完成了。设置下面的属性启用自动Map连接属性。 hive.auto.convert.join true hive.auto.convert.join.noconditionaltask true hive.auto.convert.join.noconditionaltask.size 10000000 hive.auto.convert.join.use.nonstaged true hive.auto.convert.join：是否启用基于输入文件的大小，将普通连接转化为Map连接的优化机制。 hive.auto.convert.join.noconditionaltask：假设参与连接的表（或分区）有N个，如果打开这个参数，并且有N-1个表（或分区）的大小总和小于hive.auto.convert.join.noconditionaltask.size参数指定的值，那么会直接将连接转为Map连接。 hive.auto.convert.join.use.nonstaged：对于条件连接，如果从一个小的输入流可以直接应用于join操作而不需要过滤或者投影，那么不需要通过MapReduce的本地任务在分布式缓存中预存。 倾斜Map连接（某个连接键对应的行数过多的情况） hive.optimize.skewjoin true hive.skewjoin.key 100000 hive.skewjoin.mapjoin.map.tasks 10000 hive.skewjoin.mapjoin.min.split 3354432 hive.optimize.skewjoin：是否为连接表中的倾斜键创建单独的执行计划。 hive.skewjoin.key：决定如何确定连接中的倾斜键。 hive.skewjoin.mapjoin.map.tasks：指定倾斜连接中，用于Map连接作业的任务数。 hive.skewjoin.mapjoin.min.split：通过指定最小split的大小，确定Map连接作业的任务数。 桶Map连接（连接中使用的表是按特定列分桶） hive.optimize.bucketmapjoin true hive.optimize.bucketmapjoin.sortedmerge true hive.optimize.bucketmapjoin：是否尝试桶Map连接。 hive.optimize.bucketmapjoin.sortedmerge：是否尝试在Map连接中使用归并排序。 避免全局排序 Hive中使用order by子句实现全局排序。orderby只用一个Reducer产生结果，对于大数据集，这种做法效率很低。如果不需要全局有序，则可以使用sortby子句，该子句为每个reducer生成一个排好序的文件。如果需要控制一个特定数据行流向哪个reducer，可以使用distribute by子句。例如: Selectid,name, salary, dept fromemployee distribute by dept sort by id asc, name desc; 属于一个dept的数据会分配到同一个reducer进行处理，同一个dept的所有记录按照id、name列排序。最终的结果集是全局有序的。 优化limit操作 默认时limit操作仍然会执行整个查询，然后返回限定的行数。在有些情况下这种处理方式很浪费，因此可以通过设置下面的属性避免此行为。 hive.limit.optimize.enable true hive.limit.row.max.size 100000 hive.limit.optimize.limit.file 10 hive.limit.optimize.fetch.max 50000 hive.limit.optimize.enable：是否启用limit优化。当使用limit语句时，对源数据进行抽样。 hive.limit.row.max.size：在使用limit做数据的子集查询时保证的最小行数据量。 hive.limit.optimize.limit.file：在使用limit做数据子集查询时，采样的最大文件数。 hive.limit.optimize.fetch.max：使用简单limit数据抽样时，允许的最大行数。 启用并行执行 每条HiveQL语句都被转化成一个或多个执行阶段，可能是一个MapReduce阶段、采样阶段、归并阶段、限制阶段等。默认时，Hive在任意时刻只能执行其中一个阶段。如果组成一个特定作业的多个执行阶段是彼此独立的，那么它们可以并行执行，从而整个作业得以更快完成。通过设置下面的属性启用并行执行。 hive.exec.parallel true hive.exec.parallel.thread.number 8 hive.exec.parallel：是否并行执行作业。 hive.exec.parallel.thread.number：最多可以并行执行的作业数。 使用单一Reduce 通过为group by操作开启单一reduce任务属性，可以将一个查询中的多个group by操作联合在一起发送给单一MapReduce作业。 hive.multigroupby.singlereducer true 控制并行Reduce任务 Hive通过将查询划分成一个或多个MapReduce任务达到并行的目的。确定最佳的mapper个数和reducer个数取决于多个变量，例如输入的数据量以及对这些数据执行的操作类型等。如果有太多的mapper或reducer任务，会导致启动、调度和运行作业过程中产生过多的开销，而如果设置的数量太少，那么就可能没有充分利用好集群内在的并行性。对于一个Hive查询，可以设置下面的属性来控制并行reduce任务的个数。 hive.exec.reducers.bytes.per.reducer 256000000 hive.exec.reducers.max 1009 hive.exec.reducers.bytes.per.reducer：每个reducer的字节数，默认值为256MB。Hive是按照输入的数据量大小来确定reducer个数的。例如，如果输入的数据是1GB，将使用4个reducer。 hive.exec.reducers.max：将会使用的最大reducer个数。 启用向量化 通过查询执行向量化，使Hive从单行处理数据改为批量处理方式，具体来说是一次处理1024行而不是原来的每次只处理一行，这大大提升了指令流水线和缓存的利用率，从而提高了表扫描、聚合、过滤和连接等操作的性能。 hive.vectorized.execution.enabled true hive.vectorized.execution.reduce.enabled true hive.vectorized.execution.reduce.groupby.enabled true hive.vectorized.execution.enabled：如果该标志设置为true，则开启查询执行的向量模式，默认值为false。 hive.vectorized.execution.reduce.enabled：如果该标志设置为true，则开启查询执行reduce端的向量模式，默认值为true hive.vectorized.execution.reduce.groupby.enabled：如果该标志设置为true，则开启查询执行reduce端group by操作的向量模式，默认值为true。 启用基于成本的优化器 Hive的CBO也可以根据查询成本制定执行计划，例如确定表连接的顺序、以何种方式执行连接、使用的并行度等。设置下面的属性启用基于成本优化器。 hive.cbo.enable true hive.compute.query.using.stats true hive.stats.fetch.partition.stats true hive.stats.fetch.column.stats true hive.cbo.enable：控制是否启用基于成本的优化器，默认值是true。 hive.compute.query.using.stats：该属性的默认值为false。如果设置为true，Hive在执行某些查询时，例如selectcount(1)，只利用元数据存储中保存的状态信息返回结果。为了收集基本状态信息，需要将hive.stats.autogather属性配置为true。为了收集更多的状态信息，需要运行analyzetable查询命令。 hive.stats.fetch.partition.stats：该属性的默认值为true。操作树中所标识的统计信息，需要分区级别的基本统计，如每个分区的行数、数据量大小和文件大小等。分区统计信息从元数据存储中获取。如果存在很多分区，要为每个分区收集统计信息可能会消耗大量的资源。这个标志可被用于禁止从元数据存储中获取分区统计。当该标志设置为false时，Hive从文件系统获取文件大小，并根据表结构估算行数。 hive.stats.fetch.column.stats：该属性的默认值为false。操作树中所标识的统计信息，需要列统计。列统计信息从元数据存储中获取。如果存在很多列，要为每个列收集统计信息可能会消耗大量的资源。这个标志可被用于禁止从元数据存储中获取列统计。 Crontab cron是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 Linux下的任务调度分为两类，系统任务调度和用户任务调度。 系统任务调度：系统需要周期性执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 用户任务调度：用户要定期执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用crontab命令来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中，其文件名与用户名一致。 Crontab权限 Linux系统使用一对allow/deny文件组合判断用户是否具有执行crontab的权限。 如果用户名出现在/etc/cron.allow文件中，则该用户允许执行crontab命令。如果此文件不存在，那么如果用户名没有出现在/etc/cron.deny文件中，则该用户允许执行crontab命令。 如果只存在cron.deny文件，并且该文件是空的，则所有用户都可以使用crontab命令。 如果这两个文件都不存在，那么只有root用户可以执行crontab命令。allow/deny文件由每行一个用户名构成。 Crontab命令 crontab [-u user] file crontab [-u user] [-e | -l -r] -u user：用来设定某个用户的crontab服务，此参数一般由root用户使用。 file：file是命令文件的名字，表示将file作为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入，通常是键盘上键入的命令，并将它们载入crontab。 -e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。如果文件不存在，则创建一个。 -l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。 -r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。 注意： 如果不经意地输入了不带任何参数的crontab命令，不要使用Control-d退出，因为这会删除用户所对应的crontab文件中的所有条目。代替的方法是用Control-c退出。 Crontab文件 用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置。它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： 星号（*）：代表所有可能的值。 逗号（,）：指定一个列表范围，例“1,2,5,7,8,9”。 中杠（-）：表示一个整数范围，例如“2-6”表示“2,3,4,5,6”。 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。 执行 脚本中涉及文件路径时写绝对路径； 脚本执行要用到环境变量时，通过source命令显式引入 当手动执行脚本没问题，但是crontab不执行时，可以尝试在crontab中直接引入环境变量解决问题 可以将crontab执行任务的输出信息重定向到一个自定义的日志文件中 "},"recap.html":{"url":"recap.html","title":"OLAP 期末总结","keywords":"","body":"Ch1 蜘蛛网模型 从主体数据库中抽取部分数据作为一个小的数据库 缺乏可信度：基于不同的数据进行抽取 数据无时基：抽取时间不同 数据算法差异：抽取方式不同，可能会进行再加工 抽取的多层次 外部数据问题 无起始公共源 维度建模 以商业用户可理解的方式发布数据 提供高效的查询性能 并不需要满足范式要求 多维数据库种类 RLOAP 基于关系数据库的OLAP实现 按列存，查询效率更高 MOLAP 基于多维数据组织的OLAP实现 将细节数据和聚合后的数据均保存在cube中，以空间换效率 HOLAP 基于混合数据组织的OLAP实现 将细节数据保留在关系型数据库的事实表中，但聚合后的数据保存在cube中，聚合需要更多时间，但查询效率比ROLAP高 事实表 通常是连续值，不采用文本方式，主要的空间消耗 参照的完整性 事实表具有多个维度表的主键相关联的外键 粒度 事务（最基本的单位） 周期性快照（有聚集的信息） 累计快照 组合键 事实表通常有包含外键集合的主键，主键称为组合键。 维度表 一个数值元素是事实属性，还是维度属性？ 如果包含多个值并作为计算参与度量，则是事实（产品的价格经常变化，是事实属性） 若对具体值的描述，且表达常量和约束，则为维度属性 多为离散属性 可能会有数据冗余 这样设计的查询更方便 存储开销更小 数据不会经常改变 架构 DW/BI架构 操作性源系统 -> ETL（专为有意义，可展示的信息） -> 展现区/BI 独立数据集市 以部门为基础来部署，不考虑企业级别的信息共享与集成 短期有利于较低成本实现快速开发，长期由于数据冗余造成浪费和低效 Kimball架构 ETL -> 企业数据仓库（EDW） -> 展现区（企业数据仓库总线） Ch2 维度建模技术 维度设计过程 选择业务过程 声明粒度 用于确定某一事实表中的行表示什么。原子粒度是最低级别的粒度。 确认维度 如何描述来自业务过程度量事件的数据 描述环境的维度：包含BI应用所需要的用于过滤及分类实时的描述性属性（可以用文字） 确认事实 用于度量的事实（数字型表示） 星型模型 多个维度表围绕事实表，是部署在关系数据库系统上的多维结构 优点 简单易懂 性能优异：避免了小表的连接 适于变化：在事实/维度表中都很容易插入新数据 事实表 可加度量：可以按照与事实表关联的任意维度汇总（销售量） 半可加度量：可以对某些维度汇总，但不是所有维度（库存量可以对除时间维度进行汇总） 不可家度量：比率、利润率 空值 可以存在空值度量，聚合函数可以对空值进行操作（不要填0） 维度值外键不能存在空值，用代理键（unknown） 一致性事实 如果某些度量出现在不同的事实表中，定义需要相同 例如身高都以cm为单位 事实表类型 事务事实表 一行对应空间或时间上某点的度量事件 周期快照事实表 每行汇总来发生在某一周期内的多个度量时间，粒度是周期性的 例如：某个周期内的库存量 累积快照事实表 每行汇总了发生在过程开始和结束之间步骤内的度量事件，也就是有很多度量值，在不同时间发生的 例如：申请学校的流程、入库流程 无事实事实表 不带度量的事实，可以与累积快照事实表结合， 统计每个时间段发生的事件数（count=1） 例如：某一天发生的学生参加课程的事件 聚合事实表或OLAP多维数据库 对原子粒度事实表上钻操作，提高查询性能 合并事实表 粒度相同的事实可以合并 维度表 结构：通常比较宽，扁平型非规范表 代理键是唯一主键 不使用自然键（有意义的字符串或日期），会造成多源系统的兼容性问题 改进性能，代理键占空间小 下钻 按照多个字段进行group by，粒度变细 退化维度 除了主键，没有其他维度（订单号） 非规范化扁平维度 简洁、高效，对应于星型模型 多层次维度 不止一个自然层次。例如日期、地理，容易引起数据的不规范（改了上层而没改下层） 空值属性 用Unkonwn等描述性字符串代替，增加一行，可以join 日期维度 主键为整数（代理键），但是有意义的，例如20180923 扮演角色维度 单个维度可以被事实表多次使用 杂项维度 不同的维度混合在一起，最多是笛卡尔积个个数，可以避免蜈蚣模式 雪花维度 多级层级结构，更加规范化，但复杂低效 减少数据量，join变多，效率变低 蜈蚣模型 增加了数据量（外键变多） 支架维度 包含对其他维度的引用 例如：银行账户维度引用开户日期维度 一致性维度 不同的维度表的属性具有相同列名和内容，则维度表具有一致性 处理缓慢变化的维度 类型0：保持原样 类型1：重写 原地修改，不能反映历史信息 类型2：增加新行（多行描述统一成员，增加新行的同时，修改原来行的日期、指示器属性） 行有效的日期/时间戳 行戒指日期/时间冲 当前行标识 类型3：增加新属性（不常用） 类型4：增加微型维度 当维度中的一组属性快速变化并划分为微型维度时采用 例如人口统计微型维度：购物的用户信息不断改变，反映购物者当时的消费水平 类型5：微型维度+类型1支架 精确保存历史属性值，按照当前属性值增加报表的历史事实 在客户维度中再增加一个当前人口统计维度 类型6：类型1到类型2属性（加一列和一行） 保存历史和当前维度属性值 增加新行的同时，记录之前的历史部门名称，修改当前部门名称 类型7：双类型1和2 两个都不是微型维度 事实表连接两个产品维度 一个是产品维度，采用类型2增加新行的方式 一个是当前产品维度，即产品维度中当前行标识=current的行 处理维度层次关系 可变深度层次：直辖市/省 具有路径字符属性的可变深度层次：每个路径加入有意义的字符串表示深度 Ch3 零售业务 日期维度 主键一般为代理键，但具有日期含义yyyymmdd 如何处理当前时间概念？ 可以添加某些每天更新的字段：IsCurrentDay 添加滞后属性：0表示今天，-1表示昨天 处理更细的粒度 不将日期和时间放在一个维度 促销维度 为每一种促销手段建立一个维度 促销数据稀疏（只有很少的条件组合能影响产品） 为所有促销手段建立统一的维度 空外键、空事实 某些东西有促销也有非促销 使用unknown代替，促销维度中也加入unknown，便于连接 退化维度（DD） POS Transaction号码，没有外键，没有维度表，但是也是一个维度，可用于购买商品的分组，如相同的 POS Transaction 可以表明这些物品时在同一个交易中购买的 对频繁购物者建模 建立频繁购物者维度表，在事实表上增加外键 哪些商品参与促销但未被购买？ 构建促销包含事实表，无论是否卖出，每天加一行（count=1） 用该促销包含事实表和零售销售事实表求差集即可（except） 事实表的主键 使用组合键，包括表外键的子集和退化维度 使用代理键 多个源系统中不会造成歧义 直接的唯一表示 插入删除的替换更新：避免执行复杂的事务 雪花模型 复杂，查询效率低 无法实现位图索引 可以很方便的对多个属性进行查询（使用AND/OR/NOT操作） 每个属性有N位bit，每一位表示在当前record上是否存在该属性 位图索引适合属性不多，record不多的情况 蜈蚣模式 包含大量维度信息 导致事实表需要更多的磁盘空间 Ch4 库存 半可加事实 在库存快照中，当前数量可以进行汇总，但不能在日期维度可加 库存累计快照 事实表定义过程开始、结束以及期间的可区分的里程碑 如接收日期维度、验收日期维度、入库日期维度等 适合处理业务用户开展对工作流或流水线的分析 Ch5 采购（看作业SQL） 处理缓慢变化维度 列举出不同年份推出的产品的数量 请按照年龄范围、收入级别列 举出每年发生的事实数量 请按照收入级别列举出所有客户的人数 请按照当前部门名称列举出所有产品的改变部门的次数 Ch6 订单管理 角色扮演维度 例如日期维度，在同一个事实表中会使用多次，可以保证全局的一致性 如果不使用角色维度，则会导致JOIN的时候造成混乱，因为日期维度都在一张表上，修改生产日期可能会修改成销售日期 杂项维度 是多个属性的组合，而不是多个属性集的并集 避免了蜈蚣模式，减少了维度数量，从而减少了存储开销 避免表头/明细模式 订单头维度很大，会导致与事实表连接时效率很低，连接后空间很大 此外，维度表不应该与事实表以同样的速率增长，如订单明细事务事实增加的同时订单头维度也会增加一项 将订单头维度放入事实表中 也需要避免拆分为两个事实表，会导致连接后数据量大，性能差的问题，这是两个大表的连接操作 多币种 本地货币+通用货币（直接在当前事实表） 使用货币兑换事实 飞机票的多维模型？ Ch7 会计 维度属性层次 固定层次：好处理 不整齐可变深度层次（直辖市/省） 递归指针 映射桥接表（里面有距离父指针的层次，最高层父节点标识，最低层子节点表示标识） 每个父节点，都有对于其所有的子节点 -- 使用 7-11 Select 子组织键 From 事实表，组织映射桥接表 Where 事实表.组织id == 组织映射桥接表.子组织键 And 组织映射桥接表.父组织 = 9 给各个节点用字符进行命名 组织结构变迁 将 4\\5\\6 整体迁移到 9 下面 -- 将4,5,6与父结点1,2的关联关系删除 Delete from Org_Map where child_org in (4,5,6) and parent_org not in(4,5,6) -- 将4，5，6分别与1，7，9建立父子关系 Insert into Org_Map (parent_org, child_org) select parent_org, 4 from Org_Map where parent_org in (1,7,9) -- 5，6类似4 案例分析 采用固定层次模式和动态模式设计多维模型 教育学部下属的各个单位2018年新购置的各类资产分别有几项？ 数据学院成立于2016年，分年度列出该学院购置的资产数量 计算机系原隶属于信息学院，现隶属于计软学院，需要对原始数据仓库做何调整？ Ch8 客户关系管理 客户维度 规模大，属性多，变化快 通常表示的是融合了多个内部和外部源系统的集成数据 使用支架维度，如首次购买日期维度 将聚集事实作为维度属性提供给用户，跟踪某类客户，如上一年度花费超过一定数额的客户 分段属性，将客户分类，如按照其购买行为 多值维度的桥接表 借款申请事实 - 申请公布维度 - 申请公布桥接表 - 公布项维度 客户维度- 联系组维度 - 联系组桥接表 - 联系维度 连续行为的步骤维度 通过记录各个事实所属的步骤，来分析到底在哪个步骤会被终止 时间分为事实表 在2013年处于欺诈警告的客户 begin_eff_datetime = 1/1/2013 计算每个客户在2013年处于欺诈警告的天数 sum(least(12/31/2013,end_eff_datetime) - greatest(1/1/2013,begin_eff_datetimie)) group by customer_name 找出还剩下1个步骤没有完成，且这种事情发生了3次以上的客户 找出申请项目最多的100个客户 研究对比分组001和002的销售情况变化 如果一个客户可以属于多个分组的话，如何进行查询。 Ch9 人力资源管理 递归式雇员层次 方法一：在事实表中雇员维度扮演雇员和经理双重角色 方法二：经理维度作为雇员维度的一个支架表 经理维度 - 雇员维度 - 雇员离职事实 雇员职位变迁时，自然键ID不会变，Key会改变 经理本身既是雇员又是经理 -- 求老板手下的员工总数 Select Count(*)-1 From 雇员维度 Where current row indicator = true 方法三：管理层次桥接表，适用于需要记录直接和间接管理的情况，且不知道管理具体有多少层次 经理维度 - 管理层次桥接表 - 雇员离职事实 Select Count(*)-1 From 管理层次桥接表，雇员维度 Where Manager Key = 001 And 雇员维度.current row indicator = ‘true’ And join sentence 相比方法二，桥接表产生的数据量更大，每个人与自己的所有上级都有一条记录，如1boss+10经理+100员工=1+20+300=321，自己和自己之间也有一条记录，距离层级为0 多技能关键词 雇员维度 - 雇员技能分组桥接表 - 技能维度 Ch10 金融服务 账户可以有一个或多个客户 月账户快照事实 - 账户维度 - 账户/客户桥接表（weighting factor） - 客户维度 使用多微型维度 客户维度/客户人口统计维度/客户风险概要维度 - 事实表 能减少存储开销 表示事实发生的状态，不会改变 在桥接表中增加微型维度 事实表 - 账户维度 - 账户/客户维度桥接表 - 客户维度/人口统计维度 查询在1月份共有多少位年龄层次在21-30之间的用户参与了事实 查询在1月份由年龄层次在21-30之间的客户参加发起的账户数量 只要该账户在一个客户属于这个年龄层就算 动态值范围事实 不在维度表中预先定义范围，SQL无法泛化Group by子句，增加范围定义表，其中动态划定范围lower value & upper value，将band group key加到事实表中 查询余额范围，及其对应的账户数量和总余额 Ch11 电信 可以修改模型的方面 账单维度表示为退化维度 粒度不应该是每个话单每个月一行，而是话单上每个服务列表项一行 添加日期维度外键，尽量不要使用具体日期 没必要使用雪花模式，进行合并 将比率规划类型代码当成文本放在事实表中不合适（不使用雪花模型） 在维度上没有描述性信息（不能是缩写） 维度表的主键最好是代理键 确定数据的一致性，从而保证数据集成的正确性和健壮性 Ch12 交通运输 以区段为基本粒度的建模方式 对比不同乘客级别每年的平均获得的里程数 ​ 分析2018年虹桥机场每个月的航班数量 构建基于起降操作的多维模型 航班号和起降结合在一起，用于分析其出发和降落的机场，转机的机场不需要太过关注 引旅行始发地和目的地的机场角色扮演维度 构建基于旅行的多维模型 事实应该包含旅行总基准票价、总税费等聚集度量 旅行区段数，在需要将区段作为上卷报表时才考虑 船运模式 港口维度角色扮演，表明中转情况 分析素有从最初港口装货并在最终港口卸货的商品数量 分析跨洋/ 非跨洋飞行的线路最近一年来的变化趋势 分析国内航线或者国际航线中，分别是哪两个城市之间的航线最为密集 多时区的日期和时间，包括本地时间和GMT时间，角色视图 比较当前时间上午/下午起飞的航班的起降次数 列举出每天最晚飞起的航班和起飞时间 分析最近10 年来参加校园访问活动的同学中最终会入学的比率走势 分析最近 10 年来最终入学的同学中，有过初始问询，但是并未访问过校园的同学的比率走势 多值诊断 桥接表使用两个外键组合作为主键，为诊断分组建立一个维度主键更符合认知，外键必然是指向另一个表的主键是 列出拥有大于等于3个诊断记录的报销单据 当检查项数据稀疏时，采用测试度量类型维度来制定某个测试项 列举出所有病人在Type1和Type2上做检查的总次数 行列翻转 Ch13 保单 如何利用类型 2 设计缓慢变化维度？ 每次保留之前的记录，新增当前记录，更新日期和状态 如果去除退化维度，会发生什么情况？ 用于关联多条记录 一个用户可能关联多个行业，多值维度怎么设计 ？ 使用桥接表 异构的超类和子类产品 每个子类快照事实是超类事实表某一段的拷贝，仅包含属于特定业务线的保险项目或保险项的键 使用超类是为了方便开展分析工作，而不需要同时访问两个大型事实表 保费周期快照 汇总2018年2月和3月销售的保费和赚到的保费 列举出销售保费小于赚到保费的所有月份 多驾驶员的桥接表 列举出不同投保驾驶员数量的保单数量 习题 PPT 5 PPT 8 期中考试试卷 SQL with 语句的有效期为该查询语句，create view的结果在查询后仍然存在 with table_name[columns] as (select * from another_table where xxxx) create view view_name as (select column_name(s) from table_name when condition) 差集 Except -- 查询未促销的商品 Select xxxxx From 零售销售事实 Where xxxxxx Except Select xxxxx From 促销包含事实 Where xxxxxx 组合多个属性列 Union 每张表获取各自的内容然后进行 Union 一般用于where语句中有一个字段不同的情况 对于缓慢变化维度中的微型维度 若按照收入级别列举出所有客户的人数，这是无法写SQL语句的，因为一个客户的收入级别可能会改变 使用 Count(Distinct()) 来消除冗余 时间范围 -- 在2013年曾处于欺诈警告 where begin_eff_time = 1/1/2013:0:0:0 -- 2013年一整年都处于欺诈警告 where begin_eff_time = 12/31/2013:23:59:59 Group by 条件语句使用 Having 在 select 中选择多内容一定要在 group by 中出现，否则只能使用聚集函数 least\\greatest 求最小值最大值，范围查找时可能会用到 累积快照事实表中的count=1表明每个阶段的人数，因此使用sum而不是count进行查询 Case 语句使用 Case case_value When when_value Then statement_list [When when_value Then statement_list] ... [Ekse statement_list] End -- 结合sum使用 sum(case sex when 'F' then 1 else 0 end) female "}}