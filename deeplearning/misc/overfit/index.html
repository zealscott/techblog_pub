<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>防止神经网络过拟合的基本方法 | Scott&#39;Log</title>
<meta name="keywords" content="deep learning" />
<meta name="description" content="过拟合是训练神经网络中常见的问题，本文讨论了产生过拟合的原因，如何发现过拟合，以及简单的解决方法">
<meta name="author" content="Scott Du">
<link rel="canonical" href="https://tech.zealscott.com/deeplearning/misc/overfit/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.fd126033c3455a688b2479431fed44510433c36ce61093336d94a5681fad5866.css" integrity="sha256-/RJgM8NFWmiLJHlDH&#43;1EUQQzw2zmEJMzbZSlaB&#43;tWGY=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://tech.zealscott.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tech.zealscott.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tech.zealscott.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tech.zealscott.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://tech.zealscott.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body, 
        {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '\\[', right: '\\]', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false}
                ]
            }
        );"></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-116370175-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<meta property="og:title" content="防止神经网络过拟合的基本方法" />
<meta property="og:description" content="过拟合是训练神经网络中常见的问题，本文讨论了产生过拟合的原因，如何发现过拟合，以及简单的解决方法" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tech.zealscott.com/deeplearning/misc/overfit/" /><meta property="og:image" content="https://tech.zealscott.com/papermod-cover.png"/><meta property="article:section" content="deeplearning" />
<meta property="article:published_time" content="2018-05-15T19:58:58&#43;08:00" />
<meta property="article:modified_time" content="2018-05-15T19:58:58&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://tech.zealscott.com/papermod-cover.png"/>

<meta name="twitter:title" content="防止神经网络过拟合的基本方法"/>
<meta name="twitter:description" content="过拟合是训练神经网络中常见的问题，本文讨论了产生过拟合的原因，如何发现过拟合，以及简单的解决方法"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deep Learning",
      "item": "https://tech.zealscott.com/deeplearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "防止神经网络过拟合的基本方法",
      "item": "https://tech.zealscott.com/deeplearning/misc/overfit/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "防止神经网络过拟合的基本方法",
  "name": "防止神经网络过拟合的基本方法",
  "description": "过拟合是训练神经网络中常见的问题，本文讨论了产生过拟合的原因，如何发现过拟合，以及简单的解决方法",
  "keywords": [
    "deep learning"
  ],
  "articleBody": "发现过拟合问题 在训练神经网络时，我们常常有训练集、测试集和验证集三种数据集。\n有时候我们会发现，训练出来的神经网络在训练集上表现很好（准确率很高），但在测试集上的准确率比较差。这种现象一般被认为是过拟合，也就是过度学习了训练集上的特征，导致泛化能力较差。\nhold out 方法 那么如何发现是否存在过拟合方法呢？一种简单的思路就是把训练集分为训练集和验证集，其中训练集用来训练数据，验证集用来检测准确率。\nEarly stop 我们在每个迭代期的最后都计算在验证集上的分类准确率，一旦分类准确率已经饱和，就停止训练。这个策略被称为提前停止。\n示例 以MNIST数据集为例，这里使用1000个样本作为训练集，迭代周期为400，使用交叉熵代价函数，随机梯度下降，我们可以画出其损失值与准确率。\n训练集上的损失值和准确率：\n验证集上的损失值和准确率：\n对比测试集与验证集的准确率：\n可以发现：训练集上的损失值越来越小，正确率已经达到了100%，而验证集上的损失会突然增大，正确率没有提升。这就产生了过拟合问题。\n增大训练量 一个最直观，也是最有效的方式就是增大训练量。有了足够的训练数据，就算是一个规模很大的网络也不太容易过拟合。\n例如，如果我们将MNIST的训练数据增大到50000（扩大了50倍），则可以发现训练集和测试集的正确率差距不大，且一直在增加（这里只迭代了30次）：\n但很不幸，一般来说，训练数据时有限的，这种方法不太实际。\n人为扩展训练数据 当我们缺乏训练数据时，可以使用一种巧妙的方式人为构造数据。\n例如，对于MNIST手写数字数据集，我们可以将每幅图像左右旋转15°。这应该还是被识别成同样的数字，但对于我们的神经网络来说（像素级），这就是完全不同的输入。\n因此，将这些样本加入到训练数据中很可能帮助我们的网络学习更多如何分类数字。\n这个想法很强大并且已经被广泛应用了，更多讨论可以查看这篇论文。\n再举个例子，当我们训练神经网络进行语音识别时，我们可以对这些语音随机加上一些噪音–加速或减速。\n规范化（regularization） 除了增大训练样本，另一种能减轻过拟合的方法是**降低网络的规模。**但往往大规模的神经网络有更强的潜力，因此我们想使用另外的技术。\n规范化是神经网络中常用的方法，虽然没有足够的理论，但规范化的神经网络往往能够比非规范化的泛化能力更强。\n一般来说，我们只需要对$w$进行规范化，而几乎不对$b$进行规范化。\nL2规范化 学习规则 最常用的规范化手段，也称为权重衰减（weight decay）。\nL2规范化的想法是增加一个额外的项到代价函数上，这个项被称为**规范化项。**例如，对于规范化的交叉熵：\n$$C= -\\frac{1}{n}\\sum\\limits_{x}[y_j\\ln a_j^L+ (1-y_j)\\ln (1-a_j^L)]+ \\frac{\\lambda}{2n}\\sum\\limits_ww^2$$\n对于其他形式的代价函数，都可以写成：\n$$C =C_0+\\frac{\\lambda}{2n}\\sum\\limits_ww^2$$\n由于我们的目的是使得代价函数越小越好，因此直觉的看，规范化的效果是让网络倾向于学习小一点的权重。\n换言之，规范化可以当做一种寻找小的权重和最小化原始代价函数之间的折中。\n现在，我们再对$w$和$b$求偏导：\n$$\\frac{\\partial C}{\\partial w} =\\frac{\\partial C_0}{\\partial w}+\\frac{\\lambda}{n}w $$\n$$\\frac{\\partial C}{\\partial w} =\\frac{\\partial C_0}{\\partial b} $$\n因此，我们计算规范化的代价函数的梯度是很简单的：仅仅需要反向传播，然后加上$\\frac{\\lambda}{n}w$得到所有权重的偏导数。而偏置的偏导数不需要变化。所以权重的学习规则为：\n$$w\\to (1-\\frac{\\lambda\\eta}{n})w-\\frac{\\eta}{m}\\sum\\limits_x\\frac{\\partial C_x}{\\partial w}$$\n$$b\\to b-\\frac{\\eta}{m}\\sum\\limits_x\\frac{\\partial C_x}{\\partial b}$$\n这里也表明，我们倾向于使得权重更小一点。\n那这样，是否会让权重不断下降变为0呢？但实际上不是这样的，因为如果在原始代价函数中的下降会造成其他项使得权重增加。\n示例 我们依然来看MNIST的例子。这里，我使用$\\lambda = 0.1$的规范化项进行学习。\n训练集上的准确率和损失值和之前一样：\n测试集上的损失值不断减少，准确率不断提高，符合预期：\nL1规范化 学习规则 这个方法是在未规范化的代价函数上加一个权重绝对值的和：\n$$C = C_0+ \\frac{\\lambda}{n}\\sum\\limits_w|w|$$\n对其进行求偏导得：\n$$\\frac{\\partial C}{\\partial w} =\\frac{\\partial C_0}{\\partial w}+\\frac{\\lambda}{n}sgn(w ) $$\n其中$sgn()$就是$w$的正负号。\n与L2规范化的联系 我们将L1规范化与L2规范化进行对比（Mini-Batch = $m$）：\n$$w\\to w - \\frac{\\lambda\\eta}{nm}\\sum sgn(w )-\\frac{\\eta}{m}\\sum\\limits_x\\frac{\\partial C_x}{\\partial w}$$\n$$w\\to (1-\\frac{\\lambda\\eta}{n})w-\\frac{\\eta}{m}\\sum\\limits_x\\frac{\\partial C_x}{\\partial w}$$\n两种规范化都惩罚大的权重，但权重缩小的方式不同。\n在L1规范化中，权重通过一个常量向0进行收缩；\n而L2规范化中，权重通过一个和$w$成比例的量进行收缩。\n所以，当一个特定的权重绝对值$|w|$很大时，L1规范化的权重缩小远比L2小很多；而当$|w|$很小时，L1规范化的缩小又比L2大很多。\n因此，L1规范化倾向于聚集网络的权值在相对少量的高重要连接上，而其他权重就会被趋向于0。\nDropout Dropout是一种相当激进的技术，和之前的规范化技术不同，它不改变网络本身，而是会随机地删除网络中的一般隐藏的神经元，并且让输入层和输出层的神经元保持不变。\n我们每次使用梯度下降时，只使用随机的一般神经元进行更新权值和偏置，因此我们的神经网络时再一半隐藏神经元被丢弃的情况下学习的。\n而当我们运行整个网络时，是两倍的神经元会被激活。因此，我们将从隐藏神经元的权重减半。\n这种技术的直观理解为：**当我们Dropout不同的神经元集合时，有点像我们在训练不同的神经网络。**而不同的神经网络会以不同的方式过拟合，所以Dropout就类似于不同的神经网络以投票的方式降低过拟合。\n对于不同的技术，其实都可以理解为：**我们在训练网络的健壮性。**无论是L1、L2规范化倾向于学习小的权重，还是Dropout强制学习在神经元子集中更加健壮的特征，都是让网络对丢失个体连接的场景更加健壮。\nwhy works? 在论文中由这样一段话解释Dropout方法：\n This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n 参考  What is regularization in machine learning? Improving neural networks by preventing co-adaptation of feature detectors best practices for convolutional neural networks applied to visual  ",
  "wordCount" : "192",
  "inLanguage": "en",
  "datePublished": "2018-05-15T19:58:58+08:00",
  "dateModified": "2018-05-15T19:58:58+08:00",
  "author":{
    "@type": "Person",
    "name": "Scott Du"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tech.zealscott.com/deeplearning/misc/overfit/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Scott'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tech.zealscott.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tech.zealscott.com" accesskey="h" title="Scott&#39;Log (Alt + H)">Scott&#39;Log</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tech.zealscott.com/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/deeplearning/" title="Deep Learning">
                    <span>Deep Learning</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/curriculum/" title="Curriculum">
                    <span>Curriculum</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/misc/" title="Misc">
                    <span>Misc</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://tech.zealscott.com/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      防止神经网络过拟合的基本方法
    </h1>
    <div class="post-description">
      过拟合是训练神经网络中常见的问题，本文讨论了产生过拟合的原因，如何发现过拟合，以及简单的解决方法
    </div>
    <div class="post-meta"><span title='2018-05-15 19:58:58 +0800 CST'>May 15, 2018</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Scott Du

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%8f%91%e7%8e%b0%e8%bf%87%e6%8b%9f%e5%90%88%e9%97%ae%e9%a2%98" aria-label="发现过拟合问题">发现过拟合问题</a><ul>
                        
                <li>
                    <a href="#hold-out-%e6%96%b9%e6%b3%95" aria-label="hold out 方法">hold out 方法</a><ul>
                        
                <li>
                    <a href="#early-stop" aria-label="Early stop">Early stop</a></li></ul>
                </li>
                <li>
                    <a href="#%e7%a4%ba%e4%be%8b" aria-label="示例">示例</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%a2%9e%e5%a4%a7%e8%ae%ad%e7%bb%83%e9%87%8f" aria-label="增大训练量">增大训练量</a><ul>
                        
                <li>
                    <a href="#%e4%ba%ba%e4%b8%ba%e6%89%a9%e5%b1%95%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae" aria-label="人为扩展训练数据">人为扩展训练数据</a></li></ul>
                </li>
                <li>
                    <a href="#%e8%a7%84%e8%8c%83%e5%8c%96regularization" aria-label="规范化（regularization）">规范化（regularization）</a><ul>
                        
                <li>
                    <a href="#l2%e8%a7%84%e8%8c%83%e5%8c%96" aria-label="L2规范化">L2规范化</a><ul>
                        
                <li>
                    <a href="#%e5%ad%a6%e4%b9%a0%e8%a7%84%e5%88%99" aria-label="学习规则">学习规则</a></li>
                <li>
                    <a href="#%e7%a4%ba%e4%be%8b-1" aria-label="示例">示例</a></li></ul>
                </li>
                <li>
                    <a href="#l1%e8%a7%84%e8%8c%83%e5%8c%96" aria-label="L1规范化">L1规范化</a><ul>
                        
                <li>
                    <a href="#%e5%ad%a6%e4%b9%a0%e8%a7%84%e5%88%99-1" aria-label="学习规则">学习规则</a></li>
                <li>
                    <a href="#%e4%b8%8el2%e8%a7%84%e8%8c%83%e5%8c%96%e7%9a%84%e8%81%94%e7%b3%bb" aria-label="与L2规范化的联系">与L2规范化的联系</a></li></ul>
                </li>
                <li>
                    <a href="#dropout" aria-label="Dropout">Dropout</a><ul>
                        
                <li>
                    <a href="#why-works" aria-label="why works?">why works?</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83" aria-label="参考">参考</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="发现过拟合问题">发现过拟合问题<a hidden class="anchor" aria-hidden="true" href="#发现过拟合问题">#</a></h1>
<p>在训练神经网络时，我们常常有<strong>训练集、测试集和验证集</strong>三种数据集。</p>
<p>有时候我们会发现，训练出来的神经网络在训练集上表现很好（准确率很高），但在测试集上的准确率比较差。这种现象一般被认为是<strong>过拟合</strong>，也就是过度学习了训练集上的特征，导致泛化能力较差。</p>
<h2 id="hold-out-方法">hold out 方法<a hidden class="anchor" aria-hidden="true" href="#hold-out-方法">#</a></h2>
<p>那么如何发现是否存在过拟合方法呢？一种简单的思路就是把训练集分为训练集和验证集，其中训练集用来训练数据，验证集用来检测准确率。</p>
<h3 id="early-stop">Early stop<a hidden class="anchor" aria-hidden="true" href="#early-stop">#</a></h3>
<p>我们在每个迭代期的最后都计算在<strong>验证集</strong>上的分类准确率，一旦分类准确率已经饱和，就停止训练。这个策略被称为<strong>提前停止</strong>。</p>
<h2 id="示例">示例<a hidden class="anchor" aria-hidden="true" href="#示例">#</a></h2>
<p>以MNIST数据集为例，这里使用1000个样本作为训练集，迭代周期为400，使用交叉熵代价函数，随机梯度下降，我们可以画出其损失值与准确率。</p>
<p><strong>训练集上的损失值和准确率：</strong></p>
<p><img loading="lazy" src="/images/nndl/overfit1.png" alt="overfit1"  />
</p>
<p><img loading="lazy" src="/images/nndl/overfit2.png" alt="overfit2"  />
</p>
<p><strong>验证集上的损失值和准确率：</strong></p>
<p><img loading="lazy" src="/images/nndl/overfit3.png" alt="overfit3"  />
</p>
<p><img loading="lazy" src="/images/nndl/overfit4.png" alt="overfit4"  />
</p>
<p><strong>对比测试集与验证集的准确率：</strong></p>
<p><img loading="lazy" src="/images/nndl/overfit5.png" alt="overfit5"  />
</p>
<p>可以发现：训练集上的损失值越来越小，正确率已经达到了100%，而验证集上的损失会突然增大，正确率没有提升。这就产生了过拟合问题。</p>
<h1 id="增大训练量">增大训练量<a hidden class="anchor" aria-hidden="true" href="#增大训练量">#</a></h1>
<p>一个最直观，也是最有效的方式就是增大训练量。有了足够的训练数据，就算是一个规模很大的网络也不太容易过拟合。</p>
<p>例如，如果我们将MNIST的训练数据增大到50000（扩大了50倍），则可以发现训练集和测试集的正确率差距不大，且一直在增加（这里只迭代了30次）：</p>
<p><img loading="lazy" src="/images/nndl/overfit6.png" alt="overfit6"  />
</p>
<p>但很不幸，一般来说，训练数据时有限的，这种方法不太实际。</p>
<h2 id="人为扩展训练数据">人为扩展训练数据<a hidden class="anchor" aria-hidden="true" href="#人为扩展训练数据">#</a></h2>
<p>当我们缺乏训练数据时，可以使用一种巧妙的方式人为构造数据。</p>
<p>例如，对于MNIST手写数字数据集，我们可以将每幅图像左右旋转15°。这应该还是被识别成同样的数字，但对于我们的神经网络来说（像素级），这就是完全不同的输入。</p>
<p>因此，将这些样本加入到训练数据中很可能帮助我们的网络学习更多如何分类数字。</p>
<p>这个想法很强大并且已经被广泛应用了，更多讨论可以查看<a href="https://pdfs.semanticscholar.org/7b1c/c19dec9289c66e7ab45e80e8c42273509ab6.pdf">这篇论文</a>。</p>
<p>再举个例子，当我们训练神经网络进行语音识别时，我们可以对这些语音随机加上一些噪音&ndash;加速或减速。</p>
<h1 id="规范化regularization">规范化（regularization）<a hidden class="anchor" aria-hidden="true" href="#规范化regularization">#</a></h1>
<p>除了增大训练样本，另一种能减轻过拟合的方法是**降低网络的规模。**但往往大规模的神经网络有更强的潜力，因此我们想使用另外的技术。</p>
<p>规范化是神经网络中常用的方法，虽然没有足够的理论，<strong>但规范化的神经网络往往能够比非规范化的泛化能力更强。</strong></p>
<p>一般来说，我们只需要对$w$进行规范化，而几乎不对$b$进行规范化。</p>
<h2 id="l2规范化">L2规范化<a hidden class="anchor" aria-hidden="true" href="#l2规范化">#</a></h2>
<h3 id="学习规则">学习规则<a hidden class="anchor" aria-hidden="true" href="#学习规则">#</a></h3>
<p>最常用的规范化手段，也称为权重衰减（weight decay）。</p>
<p>L2规范化的想法是增加一个额外的项到代价函数上，这个项被称为**规范化项。**例如，对于规范化的交叉熵：</p>
<p>$$C= -\frac{1}{n}\sum\limits_{x}[y_j\ln a_j^L+ (1-y_j)\ln (1-a_j^L)]+ \frac{\lambda}{2n}\sum\limits_ww^2$$</p>
<p>对于其他形式的代价函数，都可以写成：</p>
<p>$$C =C_0+\frac{\lambda}{2n}\sum\limits_ww^2$$</p>
<p>由于我们的目的是使得代价函数越小越好，因此直觉的看，<strong>规范化的效果是让网络倾向于学习小一点的权重。</strong></p>
<p>换言之，<strong>规范化可以当做一种寻找小的权重和最小化原始代价函数之间的折中。</strong></p>
<p>现在，我们再对$w$和$b$求偏导：</p>
<p>$$\frac{\partial C}{\partial w} =\frac{\partial C_0}{\partial w}+\frac{\lambda}{n}w   $$</p>
<p>$$\frac{\partial C}{\partial w} =\frac{\partial C_0}{\partial b} $$</p>
<p>因此，我们计算规范化的代价函数的梯度是很简单的：仅仅需要反向传播，然后加上$\frac{\lambda}{n}w$得到所有权重的偏导数。而偏置的偏导数不需要变化。所以权重的学习规则为：</p>
<p>$$w\to (1-\frac{\lambda\eta}{n})w-\frac{\eta}{m}\sum\limits_x\frac{\partial C_x}{\partial w}$$</p>
<p>$$b\to b-\frac{\eta}{m}\sum\limits_x\frac{\partial C_x}{\partial b}$$</p>
<p>这里也表明，我们倾向于使得权重更小一点。</p>
<p>那这样，是否会让权重不断下降变为0呢？但实际上不是这样的，因为如果在原始代价函数中的下降会造成其他项使得权重增加。</p>
<h3 id="示例-1">示例<a hidden class="anchor" aria-hidden="true" href="#示例-1">#</a></h3>
<p>我们依然来看MNIST的例子。这里，我使用$\lambda = 0.1$的规范化项进行学习。</p>
<p><strong>训练集上的准确率和损失值和之前一样：</strong></p>
<p><img loading="lazy" src="/images/nndl/overfit7.png" alt="overfit7"  />
</p>
<p><img loading="lazy" src="/images/nndl/overfit8.png" alt="overfit8"  />
</p>
<p><strong>测试集上的损失值不断减少，准确率不断提高，符合预期：</strong></p>
<p><img loading="lazy" src="/images/nndl/overfit9.png" alt="overfit9"  />
</p>
<p><img loading="lazy" src="/images/nndl/overfit10.png" alt="overfit10"  />
</p>
<h2 id="l1规范化">L1规范化<a hidden class="anchor" aria-hidden="true" href="#l1规范化">#</a></h2>
<h3 id="学习规则-1">学习规则<a hidden class="anchor" aria-hidden="true" href="#学习规则-1">#</a></h3>
<p>这个方法是在未规范化的代价函数上加一个权重绝对值的和：</p>
<p>$$C = C_0+ \frac{\lambda}{n}\sum\limits_w|w|$$</p>
<p>对其进行求偏导得：</p>
<p>$$\frac{\partial C}{\partial w} =\frac{\partial C_0}{\partial w}+\frac{\lambda}{n}sgn(w )  $$</p>
<p>其中$sgn()$就是$w$的正负号。</p>
<h3 id="与l2规范化的联系">与L2规范化的联系<a hidden class="anchor" aria-hidden="true" href="#与l2规范化的联系">#</a></h3>
<p>我们将L1规范化与L2规范化进行对比（Mini-Batch = $m$）：</p>
<p>$$w\to w - \frac{\lambda\eta}{nm}\sum sgn(w )-\frac{\eta}{m}\sum\limits_x\frac{\partial C_x}{\partial w}$$</p>
<p>$$w\to (1-\frac{\lambda\eta}{n})w-\frac{\eta}{m}\sum\limits_x\frac{\partial C_x}{\partial w}$$</p>
<p>两种规范化都惩罚大的权重，但权重缩小的方式不同。</p>
<p>在L1规范化中，权重通过一个常量向0进行收缩；</p>
<p>而L2规范化中，权重通过一个和$w$成比例的量进行收缩。</p>
<p>所以，当一个特定的权重绝对值$|w|$很大时，L1规范化的权重缩小远比L2小很多；而当$|w|$很小时，L1规范化的缩小又比L2大很多。</p>
<p><strong>因此，L1规范化倾向于聚集网络的权值在相对少量的高重要连接上，而其他权重就会被趋向于0。</strong></p>
<h2 id="dropout">Dropout<a hidden class="anchor" aria-hidden="true" href="#dropout">#</a></h2>
<p>Dropout是一种相当激进的技术，和之前的规范化技术不同，它不改变网络本身，<strong>而是会随机地删除网络中的一般隐藏的神经元，并且让输入层和输出层的神经元保持不变。</strong></p>
<p>我们每次使用梯度下降时，只使用随机的一般神经元进行更新权值和偏置，因此我们的神经网络时再一半隐藏神经元被丢弃的情况下学习的。</p>
<p>而当我们运行整个网络时，是两倍的神经元会被激活。因此，我们将从隐藏神经元的权重减半。</p>
<p>这种技术的直观理解为：**当我们Dropout不同的神经元集合时，有点像我们在训练不同的神经网络。**而不同的神经网络会以不同的方式过拟合，所以Dropout就类似于不同的神经网络以投票的方式降低过拟合。</p>
<p>对于不同的技术，其实都可以理解为：**我们在训练网络的健壮性。**无论是L1、L2规范化倾向于学习小的权重，还是Dropout强制学习在神经元子集中更加健壮的特征，都是让网络对丢失个体连接的场景更加健壮。</p>
<h3 id="why-works">why works?<a hidden class="anchor" aria-hidden="true" href="#why-works">#</a></h3>
<p>在<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">论文中</a>由这样一段话解释Dropout方法：</p>
<blockquote>
<p>This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.</p>
</blockquote>
<h1 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h1>
<ol>
<li><a href="https://www.quora.com/What-is-regularization-in-machine-learning">What is regularization in machine learning?</a></li>
<li><a href="https://arxiv.org/pdf/1207.0580.pdf">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
<li><a href="https://pdfs.semanticscholar.org/7b1c/c19dec9289c66e7ab45e80e8c42273509ab6.pdf">best practices for convolutional neural networks applied to visual</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tech.zealscott.com/tags/deep-learning/">deep learning</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://tech.zealscott.com/deeplearning/misc/parameters/">
    <span class="title">«</span>
    <br>
    <span>如何选择神经网络中的超参数</span>
  </a>
  <a class="next" href="https://tech.zealscott.com/deeplearning/misc/softmax/">
    <span class="title">»</span>
    <br>
    <span>交叉熵与 softmax </span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 防止神经网络过拟合的基本方法 on twitter"
        href="https://twitter.com/intent/tweet/?text=%e9%98%b2%e6%ad%a2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%96%b9%e6%b3%95&amp;url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fmisc%2foverfit%2f&amp;hashtags=deeplearning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 防止神经网络过拟合的基本方法 on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fmisc%2foverfit%2f&amp;title=%e9%98%b2%e6%ad%a2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%96%b9%e6%b3%95&amp;summary=%e9%98%b2%e6%ad%a2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%96%b9%e6%b3%95&amp;source=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fmisc%2foverfit%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 防止神经网络过拟合的基本方法 on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fmisc%2foverfit%2f&title=%e9%98%b2%e6%ad%a2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%96%b9%e6%b3%95">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 防止神经网络过拟合的基本方法 on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fmisc%2foverfit%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 防止神经网络过拟合的基本方法 on whatsapp"
        href="https://api.whatsapp.com/send?text=%e9%98%b2%e6%ad%a2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%96%b9%e6%b3%95%20-%20https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fmisc%2foverfit%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 防止神经网络过拟合的基本方法 on telegram"
        href="https://telegram.me/share/url?text=%e9%98%b2%e6%ad%a2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%87%e6%8b%9f%e5%90%88%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%96%b9%e6%b3%95&amp;url=https%3a%2f%2ftech.zealscott.com%2fdeeplearning%2fmisc%2foverfit%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://tech.zealscott.com">Scott&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
