{"./":{"url":"./","title":"Introduction","keywords":"","body":"Guideline for CMU Deep Learning This GitBook notes are maintained by zealscott. Course matrial 11-785 Introduction to Deep Learning Videos Notes L02 What can a network represent As an universal Boolean function / classifiers / approximators Discuss the depth and width in network L03 Learning the network Empirical Risk Optimization problem statement L03.5 A brief note on derivatives Multiple variables Minimization L04 Backpropagation Chain rule / Subgradient Backpropagation / Vector formulation L05 Convergence Backpropagation prefers consistency over perfection(which is good) Second-order method problem / learning rate choose L06 Optimization Rprop / Quickprop Momentum / Nestorov’s Accelerated Gradient Batch / Stochastic / Mini-batch gradient descent L07 Optimizers and regularizers Second moments: RMS Prop / Adam Batch normalization Regularizer / dropout L08 Motivation of CNN The need for shift invariance Scan network / Why distributing scan / Receptive Field / Stride / Pooling L09 Cascade Correlation Why Is Backprop So Slow? The advantages of cascade correlation L10 CNN architecture Architecture / size of parameters / convolution layer / maxpooling L11 Using CNNs to understand the neural basis of vision (guest lecture) L12 Backpropagation in CNNs Computing ∇Z(l)Div\\nabla_{Z(l)} D i v∇Z(l)​Div / ∇Y(l−1)Div\\nabla_{Y(l-1)} D i v∇Y(l−1)​Div / ∇w(l)Div\\nabla_{w(l)} D i v∇w(l)​Div Regular convolution running on shifted derivative maps using flipped filter Derivative of Max pooling / Mean pooling Transposed Convolution / Depth-wise convolution Le-net 5 / AlexNet / VGGNet / Googlenet / Resnet / Densenet L13 Recurrent Networks Model / Architecture Back Propagation Through Time Bidirectional RNN L14 Stability analysis and LSTMs Stability: memory ability / saturate / different activation Vanishing gradient LSTM: architecture / forward / backward Gated Recurrent Units (GRU) L15 Divergence of RNN One to one / Many to many / Many to one / Seq2seq divergence Language modelling: Representing words L16 Connectionist Temporal Classification Sequence to sequence model / time synchronous / order synchronous Iterative estimate output table: viterbi algorithm / expected divergence Repetitive decoding problem / Beam search L17 Seq2seq & Attention Autoencoder / attention weight / beam search L18 Representation Autoencoder / non-linear manifold L19 Hopfield network Loopy network / energy / content-addressable memory Store a specific pattern / orthogonal patterns L20 Boltzmann machines 1 Training hopfield nets: Geometric approach / Optimization Boltzmann Distribution L21 Boltzmann machines 2 Stochastic system: Boltzmann machines Training / Sampling of this model , as well as Restricted Boltzmann Machines L22 Variational Autoencoders 1 Generative models: PCA, Mixture Gaussian, Factor analysis, Autoencoder EM algorithm for generative model L23 Variational Autoencoders 1 Non-linear Gaussian Model VAEs Ref 优化器对比 BGD / SGD / MBGD / Momentum / NAG / Adagrad / Adadelta / RMSprop / Adam Activation Functions Sigmoid / Relu / Leaky ReLU Vanishing gradient problem CS224d The Vanishing Gradient Problem A good illustration of NN KL散度与交叉熵区别与联系 @Last updated at 5/11/2021 "},"L02 Network represent.html":{"url":"L02 Network represent.html","title":"1 Network Represent","keywords":"","body":"Preliminary Perceptron Threshold unit “Fires” if the weighted sum of inputs exceeds a threshold Soft perceptron Using sigmoid function instead of a threshold at the output Activation: The function that acts on the weighted combination of inputs (and threshold) Affine combination Different from Linear combination: the result of mapping zero is not zero. Multi-layer perceptron Depth Is the length of the longest path from a source to a sink Deep: Depth greater than 2 Inputs/Outputs are real or Boolean stimuli What can this network compute? Universal Boolean functions A perceptron can model any simple binary Boolean gate Using weight 1 or -1 to model function The universal AND gate: (⋀i=1LXi)∧(⋀i=L+1NXˉi)(\\bigwedge_{i=1}^{L} X_{i}) \\wedge(\\bigwedge_{i=L+1}^{N} \\bar{X}_{i})(⋀i=1L​Xi​)∧(⋀i=L+1N​Xˉi​) The universal OR gate: (⋁i=1LXi)∨(⋁i=L+1NXˉi)(\\bigvee_{i=1}^{L} X_{i}) \\vee(\\bigvee_{i=L+1}^{N} \\bar{X}_{i})(⋁i=1L​Xi​)∨(⋁i=L+1N​Xˉi​) Cannot compute an XOR MLPs can compute the XOR MLPs are universal Boolean functions Can compute any Boolean function A Boolean function is just a truth table So expressed the result in disjunctive normal form, like Y=Xˉ1Xˉ2X3X4Xˉ5+Xˉ1X2Xˉ3X4X5+Xˉ1X2X3Xˉ4Xˉ5+X1Xˉ2Xˉ3Xˉ4X5+X1Xˉ2X3X4X5+X1X2Xˉ3Xˉ4X5 \\begin{aligned} Y=& \\bar{X}_1 \\bar{X}_2 X_3 X_4 \\bar{X}_5+\\bar{X}_1 X_2 \\bar{X}_3 X_4 X_5+\\bar{X}_1 X_2 X_3 \\bar{X}_4 \\bar{X}_5+X_1 \\bar{X}_2 \\bar{X}_3 \\bar{X}_4 X_5+X_1 \\bar{X}_2 X_3 X_4 X_5+X_1 X_2 \\bar{X}_3 \\bar{X}_4 X_5 \\end{aligned} Y=​Xˉ1​Xˉ2​X3​X4​Xˉ5​+Xˉ1​X2​Xˉ3​X4​X5​+Xˉ1​X2​X3​Xˉ4​Xˉ5​+X1​Xˉ2​Xˉ3​Xˉ4​X5​+X1​Xˉ2​X3​X4​X5​+X1​X2​Xˉ3​Xˉ4​X5​​ In this case, need 5 neurons in the hidden layer. Need for depth A one-hidden-layer MLP is a Universal Boolean Function But the largest number of perceptrons is expontial: 2N2^N2N How about depth? Will require 3(N−1)3(N-1)3(N−1) perceptrons, linear in NNN to express the same function Using associatable rules, can be arranged in 2log⁡2N2\\log_2 N2log2​N layers eg. model O=W⊕X⊕Y⊕ZO=W \\oplus X \\oplus Y \\oplus ZO=W⊕X⊕Y⊕Z The challenge of depth Using only KKK hidden layers will require O(2CN)O(2^{CN})O(2CN) neurons in the KKKth layer, where C=2−(k−1)/2C = 2^{-(k-1)/2}C=2−(k−1)/2 A network with fewer than the minimum required number of neurons cannot model the function Universal classifiers Composing complicated “decision” boundaries Using OR to create more decision boundaries Can compose arbitrarily complex decision boundaries Even using one-layer MLP Need for depth A naïve one-hidden-layer neural network will required infinite hidden neurons Construct basic unit and add more layers to decrese #neurons The number of neurons required in a shallow network is potentially exponential in the dimensionality of the input Universal approximators A one-layer MLP can model an arbitrary function of a single input MLPs can actually compose arbitrary functions in any number of dimensions Even without \"activation\" Activation A universal map from the entire domain of input values to the entire range of the output activation Optimal depth and width Deeper networks will require far fewer neurons for the same approximation error Sufficiency of architecture Not all architectures can represent any function Continuous activation functions result in graded output at the layer To capture information \"missed\" by the lower layer Width vs. Activations vs. Depth Narrow layers can still pass information to subsequent layers if the activation function is sufficiently graded But will require greater depth, to permit later layers to capture patterns Capacity of the network Information or Storage: how many patterns can it remember VC dimension: bounded by the square of the number of ..weights.. in the network Straight forward: largest number of disconnected convex regions it can represent A network with insufficient capacity cannot exactly model a function that requires a greater minimal number of convex hulls than the capacity of the network "},"L03 A brief note on derivatives.html":{"url":"L03 A brief note on derivatives.html","title":"2 A Brief Note On Derivatives","keywords":"","body":"What is derivatives? A derivative of a function at any point tells us how much a minute increment to the argument of the function will increment the value of the function To be clear, what we want is not differentiable, but how the change effects the outputs. Based on the fact that at a fine enough resolution, any smooth, continuous function is locally linear at any point. So we can express like this Δy=αΔx \\Delta y=\\alpha \\Delta x Δy=αΔx Multivariate scalar function Δy=α1Δx1+α2Δx2+⋯+αDΔxD \\Delta y=\\alpha_{1} \\Delta x_{1}+\\alpha_{2} \\Delta x_{2}+\\cdots+\\alpha_{D} \\Delta x_{D} Δy=α1​Δx1​+α2​Δx2​+⋯+αD​ΔxD​ The partial derivative αi\\alpha_iαi​ gives us how yyy increments when only xix_ixi​ is incremented It can be expressed as: Δy=∇xyΔx \\Delta y=\\nabla_{x} y \\Delta x Δy=∇x​yΔx where ∇xy=[∂y∂x1⋯∂y∂xD] \\nabla_{\\mathrm{x}} y=\\left[\\frac{\\partial y}{\\partial x_{1}} \\quad \\cdots \\quad \\frac{\\partial y}{\\partial x_{D}}\\right] ∇x​y=[∂x1​∂y​⋯∂xD​∂y​] Optimization Single variable Three different critical point with zero derivative The second derivative is ≥0\\ge 0≥0 at minima ≤0\\le 0≤0 at maxima =0=0=0 at inflection points multiple variables df(X)=∇Xf(X)dX d f(X)=\\nabla_{X} f(X) d X df(X)=∇X​f(X)dX The gradient is the transpose of the derivative ∇Xf(X)T\\nabla_{X} f(X)^{T}∇X​f(X)T(give us the change in f(x)f(x)f(x) for tiny variations in XXX) This is a vector inner product df(x)d f(x)df(x) is max if dXdXdX is aligned with ∇Xf(X)T\\nabla_{X} f(X)^{\\mathrm{T}}∇X​f(X)T ∠(∇Xf(X)T,dX)=0\\angle\\left(\\nabla_{X} f(X)^{\\mathrm{T}}, d X\\right)=0∠(∇X​f(X)T,dX)=0 The gradient is the direction of fastest increase in f(x)f(x)f(x) Hessian Unconstrained Minimization of function Solve for ZZZ where the derivative equals to zero: ∇Xf(X)=0 \\nabla_{X} f(X)=0 ∇X​f(X)=0 Compute the Hessian Matrix at the candidate solution and verify that Hessian is positive definite (eigenvalues positive) -> to identify local minima Hessian is negative definite (eigenvalues negative) -> to identify local maxima Closed form are not available To find a maximum move in the direction of the gradient xk+1=xk+ηk∇xf(xk)T x^{k+1}=x^{k}+\\eta^{k} \\nabla_{x} f\\left(x^{k}\\right)^{T} xk+1=xk+ηk∇x​f(xk)T To find a minimum move exactly opposite the direction of the gradient xk+1=xk−ηk∇xf(xk)T x^{k+1}=x^{k}-\\eta^{k} \\nabla_{x} f\\left(x^{k}\\right)^{T} xk+1=xk−ηk∇x​f(xk)T Choose steps fixed step size iteration-dependent step size: critical for fast optimization Convergence For convex functions gradient descent will always find the minimum. For non-convex functions it will find a local minimum or an inflection point "},"L03 Learning the network.html":{"url":"L03 Learning the network.html","title":"3 Learning The Network","keywords":"","body":" How do we compose the network that performs the requisite function? Preliminary The bias can also be viewed as the weight of another input component that is always set to 1 z=∑iwixiz=\\sum_{i} w_{i} x_{i}z=∑i​wi​xi​ What we learn: The ..parameters.. of the network Learning the network: Determining the values of these parameters such that the network computes the desired function How to learn a network? W^=argmin⁡W∫Xdiv⁡(f(X;W),g(X))d \\widehat{\\boldsymbol{W}}=\\underset{W}{\\operatorname{argmin}} \\int_{X} \\operatorname{div}(f(X ; W), g(X)) d W=Wargmin​∫X​div(f(X;W),g(X))d div() is a divergence function thet goes to zero when f(X;W)=g(X)f(X ; W)=g(X)f(X;W)=g(X) But in practice g(x)g(x)g(x) will not have such specification Sample g(x)g(x)g(x): just gather training data Learning Simple perceptron do For i=1..Ntraini = 1.. N_{train}i=1..Ntrain​ O(xi)=sign(WTXi)O(x_i) = sign(W^TX_i)O(xi​)=sign(WTXi​) if O(xi)≠yiO(x_i) \\neq y_iO(xi​)≠yi​ W=W+YiXiW = W+Y_iX_iW=W+Yi​Xi​ until no more classification errors A more complex problem This can be perfectly represented using an MLP But perveptron algorithm require linearly separated labels to be learned in lower-level neurons An exponential search over inputs So we need differentiable function to compute the change in the output for ..small.. changes in either the input or the weights Empirical Risk Minimization Assuming XXX is a random variable: W^=argmin⁡W∫Xdiv⁡(f(X;W),g(X))P(X)dX=argmin⁡WE[div⁡(f(X;W),g(X))] \\begin{aligned} \\widehat{\\boldsymbol{W}}=& \\underset{W}{\\operatorname{argmin}} \\int_{X} \\operatorname{div}(f(X ; W), g(X)) P(X) d X \\\\\\\\ &=\\underset{W}{\\operatorname{argmin}} E[\\operatorname{div}(f(X ; W), g(X))] \\end{aligned} W=​Wargmin​∫X​div(f(X;W),g(X))P(X)dX=Wargmin​E[div(f(X;W),g(X))]​ Sample g(X)g(X)g(X), where di=g(Xi)+noised_{i}=g\\left(X_{i}\\right)+ noisedi​=g(Xi​)+noise, estimate function from the samples The empirical estimate of the expected error is the average error over the samples E[div⁡(f(X;W),g(X))]≈1N∑i=1Ndiv⁡(f(Xi;W),di) E[\\operatorname{div}(f(X ; W), g(X))] \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\operatorname{div}\\left(f\\left(X_{i} ; W\\right), d_{i}\\right) E[div(f(X;W),g(X))]≈N1​i=1∑N​div(f(Xi​;W),di​) Empirical average error (Empirical Risk) on all training data Loss⁡(W)=1N∑idiv⁡(f(Xi;W),di) \\operatorname{Loss}(W)=\\frac{1}{N} \\sum_{i} \\operatorname{div}\\left(f\\left(X_{i} ; W\\right), d_{i}\\right) Loss(W)=N1​i∑​div(f(Xi​;W),di​) Estimate the parameters to minimize the empirical estimate of expected error W^=argmin⁡WLoss⁡(W) \\widehat{\\boldsymbol{W}}=\\underset{W}{\\operatorname{argmin}} \\operatorname{Loss}(W) W=Wargmin​Loss(W) Problem statement Given a training set of input-output pairs (X_1,d_1),(X_2,d_2),…,(X_N,d_N) \\left(\\boldsymbol{X}\\_{1}, \\boldsymbol{d}\\_{1}\\right),\\left(\\boldsymbol{X}\\_{2}, \\boldsymbol{d}\\_{2}\\right), \\ldots,\\left(\\boldsymbol{X}\\_{N}, \\boldsymbol{d}\\_{N}\\right) (X_1,d_1),(X_2,d_2),…,(X_N,d_N) Minimize the following function Loss⁡(W)=1N∑idiv⁡(f(Xi;W),di) \\operatorname{Loss}(W)=\\frac{1}{N} \\sum_{i} \\operatorname{div}\\left(f\\left(X_{i} ; W\\right), d_{i}\\right) Loss(W)=N1​i∑​div(f(Xi​;W),di​) This is problem of function minimization An instance of optimization "},"L04 Backpropagation.html":{"url":"L04 Backpropagation.html","title":"4 Backpropagation","keywords":"","body":" a way to solve minimization problem Problem setup Input-output pairs: not to mention Representing the output: one-hot vector yi=exp⁡(zi)∑jexp⁡(zj) y_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j} \\exp \\left(z_{j}\\right)} yi​=∑j​exp(zj​)exp(zi​)​ two classes of softmax = sigmoid Divergence: must be differentiable For real-valued output vectors, the (scaled) L2L_2L2​ divergence Div⁡(Y,d)=12∥Y−d∥2=12∑i(yi−di)2 \\operatorname{Div}(Y, d)=\\frac{1}{2}\\|Y-d\\|^{2}=\\frac{1}{2} \\sum_{i}\\left(y_{i}-d_{i}\\right)^{2} Div(Y,d)=21​∥Y−d∥2=21​i∑​(yi​−di​)2 For binary classifier Div⁡(Y,d)=−dlog⁡Y−(1−d)log⁡(1−Y) \\operatorname{Div}(Y, d)=-\\operatorname{dlog} Y-(1-d) \\log (1-Y) Div(Y,d)=−dlogY−(1−d)log(1−Y) Note: the derivative is not zero even d=Yd = Yd=Y, but it can converge very quickly For multi-class classification Div⁡(Y,d)=−∑idilog⁡yi=−log⁡yc \\operatorname{Div}(Y, d)=-\\sum_{i} d_{i} \\log y_{i}=-\\log y_{c} Div(Y,d)=−i∑​di​logyi​=−logyc​ If yc1y_c yc​1 , the slope is negative w.r.t. ycy_cyc​, indicates increasing ycy_cyc​ will reduce divergence Train the network Distributed Chain rule y=f(g1(x),g1(x),…,gM(x)) y=f\\left(g_{1}(x), g_{1}(x), \\ldots, g_{M}(x)\\right) y=f(g1​(x),g1​(x),…,gM​(x)) dydx=∂f∂g1(x)dg1(x)dx+∂f∂g2(x)dg2(x)dx+⋯+∂f∂gM(x)dgM(x)dx \\frac{d y}{d x}=\\frac{\\partial f}{\\partial g_{1}(x)} \\frac{d g_{1}(x)}{d x}+\\frac{\\partial f}{\\partial g_{2}(x)} \\frac{d g_{2}(x)}{d x}+\\cdots+\\frac{\\partial f}{\\partial g_{M}(x)} \\frac{d g_{M}(x)}{d x} dxdy​=∂g1​(x)∂f​dxdg1​(x)​+∂g2​(x)∂f​dxdg2​(x)​+⋯+∂gM​(x)∂f​dxdgM​(x)​ Backpropagation For each layer: we caculate ∂Div∂yi\\frac{\\partial D i v}{\\partial y_{i}}∂yi​∂Div​,∂Dicv∂z\\frac{\\partial Dicv}{\\partial z}∂z∂Dicv​, and ∂Div∂wij\\frac{\\partial Div}{\\partial w_{ij}}∂wij​∂Div​ For ouput layer It is easy to caculate ∂Div∂yi(N)\\frac{\\partial D i v}{\\partial y_{i}^{(N)}}∂yi(N)​∂Div​ So: ∂Div∂zi(N)=fN′(zi(N))∂Div∂yi(N)\\frac{\\partial D i v}{\\partial z_{i}^{(N)}}=f_{N}^{\\prime}\\left(z_{i}^{(N)}\\right) \\frac{\\partial D i v}{\\partial y_{i}^{(N)}}∂zi(N)​∂Div​=fN′​(zi(N)​)∂yi(N)​∂Div​ ∂Div∂wij(N)=∂zj(N)∂wij(N)∂Div∂zj(N)\\frac{\\partial D i v}{\\partial w_{ij}^{(N)}}=\\frac{\\partial z_{j}^{(N)}}{\\partial w_{ij}^{(N)}} \\frac{\\partial D i v}{\\partial z_{j}^{(N)}}∂wij(N)​∂Div​=∂wij(N)​∂zj(N)​​∂zj(N)​∂Div​, where ∂zj(N)∂wij(N)=yi(N)\\frac{\\partial z_{j}^{(N)}}{\\partial w_{ij}^{(N)}} = y_i^{(N)}∂wij(N)​∂zj(N)​​=yi(N)​ Pass on zj(N)=∑iwij(2)yi(v−1)z_{j}^{(N)}=\\sum_{i} w_{i j}^{(2)} y_{i}^{(v-1)}zj(N)​=∑i​wij(2)​yi(v−1)​, so ∂zj(N)∂y1(N−1)=wij(N)\\frac{\\partial z_{j}^{(N)}}{\\partial y_{1}^{(N-1)}} = w_{ij}^{(N)}∂y1(N−1)​∂zj(N)​​=wij(N)​ ∂Div∂yi(N−1)=∑jwij(N)∂Div∂zj(N)\\frac{\\partial D i v}{\\partial y_{i}^{(N-1)}}=\\sum_{j} w_{i j}^{(N)} \\frac{\\partial D i v}{\\partial z_{j}^{(N)}}∂yi(N−1)​∂Div​=∑j​wij(N)​∂zj(N)​∂Div​ ∂Div∂zi(N−1)=fN−1′(zi(N−1))∂Div∂yi(N−1)\\frac{\\partial D i v}{\\partial z_{i}^{(N-1)}}=f_{N-1}^{\\prime}(z_{i}^{(N-1)}) \\frac{\\partial D i v}{\\partial y_{i}^{(N-1)}}∂zi(N−1)​∂Div​=fN−1′​(zi(N−1)​)∂yi(N−1)​∂Div​ ∂Div∂wij(N−1)=yi(N−2)∂Div∂zj(N−1)\\frac{\\partial D i v}{\\partial w_{i j}^{(N-1)}}=y_{i}^{(N-2)} \\frac{\\partial D i v}{\\partial z_{j}^{(N-1)}}∂wij(N−1)​∂Div​=yi(N−2)​∂zj(N−1)​∂Div​ Special case Vector activations Vector activations: all outputs are functions of all inputs So the derivatives need to change a little ∂Div∂zi(k)=∑j∂Div∂yj(k)∂yj(k)∂zi(k) \\frac{\\partial D i v}{\\partial z_{i}^{(k)}}=\\sum_{j} \\frac{\\partial D i v}{\\partial y_{j}^{(k)}} \\frac{\\partial y_{j}^{(k)}}{\\partial z_{i}^{(k)}} ∂zi(k)​∂Div​=j∑​∂yj(k)​∂Div​∂zi(k)​∂yj(k)​​ Note: derivatives of scalar activations are just a special case of vector activations: ∂yj(k)∂zi(k)=0 for i≠j \\frac{\\partial y_{j}^{(k)}}{\\partial z_{i}^{(k)}}=0 \\text { for } i \\neq j ∂zi(k)​∂yj(k)​​=0 for i≠j For example, Softmax: yi(k)=exp⁡(zi(k))∑jexp⁡(zj(k)) y_{i}^{(k)}=\\frac{\\exp \\left(z_{i}^{(k)}\\right)}{\\sum_{j} \\exp \\left(z_{j}^{(k)}\\right)} yi(k)​=∑j​exp(zj(k)​)exp(zi(k)​)​ ∂Div∂zi(k)=∑j∂Div∂yj(k)∂yj(k)∂zi(k) \\frac{\\partial D i v}{\\partial z_{i}^{(k)}}=\\sum_{j} \\frac{\\partial D i v}{\\partial y_{j}^{(k)}} \\frac{\\partial y_{j}^{(k)}}{\\partial z_{i}^{(k)}} ∂zi(k)​∂Div​=j∑​∂yj(k)​∂Div​∂zi(k)​∂yj(k)​​ ∂yj(k)∂zi(k)={yi(k)(1−yi(k)) if i=j−yi(k)yj(k) if i≠j \\frac{\\partial y_{j}^{(k)}}{\\partial z_{i}^{(k)}}=\\left\\{\\begin{array}{c} y_{i}^{(k)}\\left(1-y_{i}^{(k)}\\right) \\quad \\text { if } i=j \\\\ -y_{i}^{(k)} y_{j}^{(k)} \\quad \\text { if } i \\neq j \\end{array}\\right. ∂zi(k)​∂yj(k)​​={yi(k)​(1−yi(k)​) if i=j−yi(k)​yj(k)​ if i≠j​ Using Keonecker delta δij=1\\delta_{i j}=1δij​=1 if i=j,0i=j, \\quad 0i=j,0 if i≠ji \\neq ji≠j ∂Div∂zi(k)=∑j∂Div∂yj(k)yi(k)(δij−yj(k)) \\frac{\\partial D i v}{\\partial z_{i}^{(k)}}=\\sum_{j} \\frac{\\partial D i v}{\\partial y_{j}^{(k)}} y_{i}^{(k)}\\left(\\delta_{i j}-y_{j}^{(k)}\\right) ∂zi(k)​∂Div​=j∑​∂yj(k)​∂Div​yi(k)​(δij​−yj(k)​) Multiplicative networks Some types of networks have multiplicative combination(instead of additive combination) Seen in networks such as LSTMs, GRUs, attention models, etc. So the derivatives need to change ∂Div∂oi(k)=∑jwij(k+1)∂Div∂zj(k+1) \\frac{\\partial D i v}{\\partial o_{i}^{(k)}}=\\sum_{j} w_{i j}^{(k+1)} \\frac{\\partial D i v}{\\partial z_{j}^{(k+1)}} ∂oi(k)​∂Div​=j∑​wij(k+1)​∂zj(k+1)​∂Div​ ∂Div∂yj(k−1)=∂oi(k)∂yj(k−1)∂Div∂oi(k)=yl(k−1)∂Div∂oi(k) \\frac{\\partial D i v}{\\partial y_{j}^{(k-1)}}=\\frac{\\partial o_{i}^{(k)}}{\\partial y_{j}^{(k-1)}} \\frac{\\partial D i v}{\\partial o_{i}^{(k)}}=y_{l}^{(k-1)} \\frac{\\partial D i v}{\\partial o_{i}^{(k)}} ∂yj(k−1)​∂Div​=∂yj(k−1)​∂oi(k)​​∂oi(k)​∂Div​=yl(k−1)​∂oi(k)​∂Div​ A layer of multiplicative combination is a special case of vector activation Non-differentiable activations Activation functions are sometimes not actually differentiable The RELU (Rectified Linear Unit) And its variants: leaky RELU, randomized leaky RELU The “max” function Subgradient (f(x)−f(x0))≥vT(x−x0) \\left(f(x)-f\\left(x_{0}\\right)\\right) \\geq v^{T}\\left(x-x_{0}\\right) (f(x)−f(x0​))≥vT(x−x0​) The subgradient is a direction in which the function is guaranteed to increase If the function is differentiable at xxx , the subgradient is the gradient But gradient is not always the subgradient though Vector formulation Define the vectors: Forward pass Backward pass Chain rule y=f(g(x))\\mathbf{y}=\\boldsymbol{f}(\\boldsymbol{g}(\\mathbf{x}))y=f(g(x)) Let z=g(x)z = g(x)z=g(x),y=f(z)y = f(z)y=f(z) So Jy(x)=Jy(z)Jz(x)J_{\\mathbf{y}}(\\mathbf{x})=J_{\\mathbf{y}}(\\mathbf{z}) J_{\\mathbf{z}}(\\mathbf{x})Jy​(x)=Jy​(z)Jz​(x) For scalar functions: D=f(Wy+b)D = f(Wy + b)D=f(Wy+b) Let z=Wy+bz = Wy + bz=Wy+b, D=f(z)D = f(z)D=f(z) ∇xD=∇z(D)Jz(x)\\nabla_{x} D = \\nabla_z(D)J_z(x)∇x​D=∇z​(D)Jz​(x) So for backward process ∇ZNDiv=∇YDiv∇ZNY\\nabla_{Z_N} Div = \\nabla_Y Div \\nabla_{Z_N}Y∇ZN​​Div=∇Y​Div∇ZN​​Y ∇yN−1Div=∇ZNDiv∇yN−1zN\\nabla_{y_{N-1}}Div = \\nabla_{Z_N} Div \\nabla_{y_{N-1}} z_N ∇yN−1​​Div=∇ZN​​Div∇yN−1​​zN​ ∇WNDiv=yN−1∇ZNDiv\\nabla_{W_N} Div = y_{N-1} \\nabla_{Z_N} Div∇WN​​Div=yN−1​∇ZN​​Div ∇bNDiv=∇ZNDiv\\nabla_{b_N} Div = \\nabla_{Z_N} Div∇bN​​Div=∇ZN​​Div For each layer First compute ∇yDiv\\nabla_{y} Div∇y​Div Then compute ∇zDiv\\nabla_{z}Div∇z​Div Finally ∇WDiv\\nabla_{W} Div∇W​Div, ∇bDiv\\nabla_{b} Div∇b​Div Training Analogy to forward pass "},"L05 Convergence.html":{"url":"L05 Convergence.html","title":"5 Convergence","keywords":"","body":" Convergence issue of gradient descent Backpropagation The divergence function minimized is only a proxy for classification error(like Softmax) Minimizing divergence may not minimize classification error Does not separate the points even though the points are linearly separable This is because the separating solution is not a feasible optimum for the loss function Compare to perceptron Perceptron rule has low bias(makes no errors if possible) But high variance(swings wildly in response to small changes to input) Backprop is minimally changed by new training instances Prefers consistency over perfection(which is good) Convergence Univariate inputs For quadratic surfaces Minimize E=12aw2+bw+c \\text {Minimize } E=\\frac{1}{2} a w^{2}+b w+c Minimize E=21​aw2+bw+c w(k+1)=w(k)−ηdE(w(k))dw \\mathrm{w}^{(k+1)}=\\mathrm{w}^{(k)}-\\eta \\frac{d E\\left(\\mathrm{w}^{(k)}\\right)}{d \\mathrm{w}} w(k+1)=w(k)−ηdwdE(w(k))​ Gradient descent with fixed step size η\\etaη to estimate scalar parameter www Using Taylor expansion E(w)=E(w(k))+E′(w(k))(w−w(k))+E′′(w(k))(w−w(k))2 E(w)=E\\left(\\mathbf{w}^{(k)}\\right)+E^{\\prime}\\left(\\mathbf{w}^{(k)}\\right)\\left(w-\\mathbf{w}^{(k)}\\right)+E^{\\prime\\prime}\\left(\\mathbf{w}^{(k)}\\right)\\left(w-\\mathbf{w}^{(k)}\\right)^2 E(w)=E(w(k))+E′(w(k))(w−w(k))+E′′(w(k))(w−w(k))2 So we can get the optimum step size ηopt=E′′(w(k))−1\\eta_{opt} = E^{\\prime\\prime}(w^{(k)})^{-1}ηopt​=E′′(w(k))−1 For ηηopt\\eta ηηopt​ the algorithm will converge monotonically For 2ηopt>η>ηopt2\\eta_{opt} > \\eta > \\eta_{opt}2ηopt​>η>ηopt​, we have oscillating convergence For η>2ηopt\\eta > 2\\eta_{opt}η>2ηopt​, we get divergence For generic differentiable convex objectives also can use Taylor expansion to estimate Using Newton's method ηopt=(d2E(w(k))dw2)−1 \\eta_{o p t}=\\left(\\frac{d^{2} E\\left(\\mathrm{w}^{(k)}\\right)}{d w^{2}}\\right)^{-1} ηopt​=(dw2d2E(w(k))​)−1 Multivariate inputs Quadratic convex function E=12wTAw+wTb+c E=\\frac{1}{2} \\mathbf{w}^{T} \\mathbf{A} \\mathbf{w}+\\mathbf{w}^{T} \\mathbf{b}+c E=21​wTAw+wTb+c If AAA is diagonal E=12∑i(aiiwi2+biwi)+c E=\\frac{1}{2} \\sum_{i}\\left(a_{i i} w_{i}^{2}+b_{i} w_{i}\\right)+c E=21​i∑​(aii​wi2​+bi​wi​)+c We can optimize each coordinate independently Like η1,opt=a11−1\\eta_{1,opt} = a^{-1}_{11}η1,opt​=a11−1​, η2,opt=a22−1\\eta_{2,opt} = a^{-1}_{22}η2,opt​=a22−1​ But Optimal learning rate is different for the different coordinates If updating gradient descent for entire vector, need to satisfy η2min⁡iηi,opt \\eta η2imin​ηi,opt​ This, however, makes the learning very slow if max⁡iηi,optmin⁡iηi,opt\\frac{\\max_i \\eta_{i,opt}}{\\min_i\\eta_{i,opt}}mini​ηi,opt​maxi​ηi,opt​​ is large Solution: Normalize the objective to have identical eccentricity in all directions Then all of them will have identical optimal learning rates Easier to find a working learning rate Target E=12w^Tw^+b^Tw^+c E=\\frac{1}{2} \\widehat{\\mathbf{w}}^{T} \\widehat{\\mathbf{w}}+\\hat{\\mathbf{b}}^{T} \\widehat{\\mathbf{w}}+c E=21​wTw+b^Tw+c So let w^=Sw\\widehat{\\mathbf{w}}=\\mathbf{S} \\mathbf{w}w=Sw, and S=A0.5S = A^{0.5}S=A0.5, b^=A−0.5b\\hat{b} = A^{-0.5}bb^=A−0.5b ,w^=A0.5w\\widehat{\\mathbf{w}} = A^{0.5} \\mathbf{w}w=A0.5w Gradient descent rule w^(k+1)=w^(k)−η∇w^E(w^(k))T \\widehat{\\mathbf{w}}^{(k+1)}=\\widehat{\\mathbf{w}}^{(k)}-\\eta \\nabla_{\\widehat{\\mathbf{w}}} E\\left(\\widehat{\\mathbf{w}}^{(k)}\\right)^{T} w(k+1)=w(k)−η∇w​E(w(k))T w(k+1)=w(k)−ηA−1∇wE(w(k))T \\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}-\\eta \\mathbf{A}^{-1} \\nabla_{\\mathbf{w}} E\\left(\\mathbf{w}^{(k)}\\right)^{T} w(k+1)=w(k)−ηA−1∇w​E(w(k))T So we just need to caculate A−1\\mathbf{A}^{-1} A−1, and the step size of each direction is all the same(1) For generic differentiable multivariate convex functions Also use Taylor expansion E(w)≈E(w(k))+∇wE(w(k))(w−w(k))+12(w−w(k))THE(w(k))(w−w(k))+⋯ E(\\mathbf{w}) \\approx E\\left(\\mathbf{w}^{(k)}\\right)+\\nabla_{\\mathbf{w}} E\\left(\\mathbf{w}^{(k)}\\right)\\left(\\mathbf{w}-w^{(k)}\\right)+\\frac{1}{2}\\left(\\mathbf{w}-w^{(k)}\\right)^{T} H_{E}\\left(w^{(k)}\\right)\\left(\\mathbf{w}-w^{(k)}\\right)+\\cdots E(w)≈E(w(k))+∇w​E(w(k))(w−w(k))+21​(w−w(k))THE​(w(k))(w−w(k))+⋯ We get the normalized update rule w(k+1)=w(k)−ηHE(w(k))−1∇wE(w(k))T \\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}-\\eta H_{E}\\left(\\boldsymbol{w}^{(k)}\\right)^{-1} \\nabla_{\\mathbf{w}} E\\left(\\mathbf{w}^{(k)}\\right)^{T} w(k+1)=w(k)−ηHE​(w(k))−1∇w​E(w(k))T Use quadratic approximations to get the maximum Issues Hessian For complex models such as neural networks, with a very large number of parameters, the Hessian is extremely difficult to compute For non-convex functions, the Hessian may not be positive semi-definite, in which case the algorithm can diverge Learning rate For complex models such as neural networks the loss function is often not convex η>2ηopt\\eta > 2\\eta_{opt}η>2ηopt​ can actually help escape local optima However always having η>2ηopt\\eta > 2\\eta_{opt}η>2ηopt​ will ensure that you never ever actually find a solution Using Decaying learning rate "},"L06 Optimization.html":{"url":"L06 Optimization.html","title":"6 Optimization","keywords":"","body":"Problems Decaying learning rates provide googd compromise between escaping poor local minima and convergence Many of the convergence issues arise because we force the same learning rate on all parameters Try to releasing the requirement that a fixed step size is used across all dimensions To be clear, backpropagation is a way to compute derivative, not a algorithm The below is NOT grident desent algorithm Better convergence strategy Derivative-inspired algorithms RProp Resilient propagation Simple first-order algorithm, to be followed independently for each component Steps in different directions are not coupled At each time If the derivative at the current location recommends continuing in the same direction as before (i.e. has not changed sign from earlier): Increase the step(α\\alphaα > 1), and continue in the same direction If the derivative has changed sign (i.e. we’ve overshot a minimum) Reduce the step(β1\\beta β1) and reverse direction Features It is frequently much more efficient than gradient descent No convexity assumption QuickProp Newton updates w(k+1)=w(k)−ηA−1∇wE(w(k))T \\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}-\\eta \\mathbf{A}^{-1} \\nabla_{\\mathbf{w}} E\\left(\\mathbf{w}^{(k)}\\right)^{T} w(k+1)=w(k)−ηA−1∇w​E(w(k))T Quickprop employs the Newton updates with two modifications It treats each dimension independently wik+1=wik−E′′(wik∣wjk,j≠i)−1E′(wik∣wjk,j≠i) w_{i}^{k+1}=w_{i}^{k}-E^{\\prime \\prime}\\left(w_{i}^{k} | w_{j}^{k}, j \\neq i\\right)^{-1} E^{\\prime}\\left(w_{i}^{k} | w_{j}^{k}, j \\neq i\\right) wik+1​=wik​−E′′(wik​∣wjk​,j≠i)−1E′(wik​∣wjk​,j≠i) It approximates the second derivative through finite differences wl,ij(k+1)=wl,ij(k)−Δwl,ij(k−1)Err′(wl,ij(k))−Err′(wl,ij(k−1))Err⁡′(wl,ij(k)) w_{l, i j}^{(k+1)}=w_{l, i j}^{(k)}-\\frac{\\Delta w_{l, i j}^{(k-1)}}{E r r^{\\prime}\\left(w_{l, i j}^{(k)}\\right)-E r r^{\\prime}\\left(w_{l, i j}^{(k-1)}\\right)} \\operatorname{Err}^{\\prime}\\left(w_{l, i j}^{(k)}\\right) wl,ij(k+1)​=wl,ij(k)​−Err′(wl,ij(k)​)−Err′(wl,ij(k−1)​)Δwl,ij(k−1)​​Err′(wl,ij(k)​) Features Employs Newton updates with empirically derived derivatives Prone to some instability for non-convex objective functions But is still one of the fastest training algorithms for many problems Momentum methods Insight In the direction that converges, it keeps pointing in the same direction Need keep track of oscillations Emphasize steps in directions that converge smoothly In the direction that overshoots, it steps back and forth Shrink steps in directions that bounce around Maintain a running average of all past steps Get longer in directions where gradient stays in the same sign Become shorter in directions where the sign keep flipping Update with the running average, rather than the current gradient Emphasize directions of steady improvement are demonstrably superior to other methods Momentum Update ΔW(k)=βΔW(k−1)−η∇WLoss⁡(W(k−1))T \\Delta W^{(k)}=\\beta \\Delta W^{(k-1)}-\\eta \\nabla_{W} \\operatorname{Loss}\\left(W^{(k-1)}\\right)^{T} ΔW(k)=βΔW(k−1)−η∇W​Loss(W(k−1))T W(k)=W(k−1)+ΔW(k) {W^{(k)}=W^{(k-1)}+\\Delta W^{(k)}} W(k)=W(k−1)+ΔW(k) First computes the gradient step at the current location: −η∇WLoss⁡(W(k−1))T-\\eta \\nabla_{W} \\operatorname{Loss}\\left(W^{(k-1)}\\right)^{T}−η∇W​Loss(W(k−1))T Then adds in the scaled previous step: βΔW(k−1)\\beta \\Delta W^{(k-1)}βΔW(k−1) Nestorov’s Accelerated Gradient Change the order of operations ΔW(k)=βΔW(k−1)−η∇WLoss⁡(W(k−1)+βΔW(k−1))T \\Delta W^{(k)}=\\beta \\Delta W^{(k-1)}-\\eta \\nabla_{W} \\operatorname{Loss}\\left(W^{(k-1)}+\\beta \\Delta W^{(k-1)}\\right)^{T} ΔW(k)=βΔW(k−1)−η∇W​Loss(W(k−1)+βΔW(k−1))T W(k)=W(k−1)+ΔW(k) {W^{(k)}=W^{(k-1)}+\\Delta W^{(k)}} W(k)=W(k−1)+ΔW(k) First extend the previous step: βΔW(k−1)\\beta \\Delta W^{(k-1)}βΔW(k−1) Then compute the gradient step at the resultant position: −η∇WLoss⁡(W(k−1)+βΔW(k−1))T-\\eta \\nabla_{W} \\operatorname{Loss}\\left(W^{(k-1)}+\\beta \\Delta W^{(k-1)}\\right)^{T}−η∇W​Loss(W(k−1)+βΔW(k−1))T Add the two to obtain the final step Converges much faster than momentum Summary Try the step size for all dimension is bad Treat each dimension independently Try to normalize curvature in all directions Second order methods, e.g. Newton's method Too expensive: require inversion of a giant Hessian Treat each dimension independently RProp / QucikProp Works, but ignores dependence between dimensions Can still be too slow Momentum methods which emphasize directions of steady improvement are demonstrably superior to other methods Incremental updates Batch gradient descent Try to simultaneously adjust the function at all training points We must process all training points before making a single adjustment Stochastic gradient descent Adjust the function at one training point at a time A single pass through the entire training data is called an “epoch” An epoch over a training set with TTT samples result in TTT updates of parameters We must go through them randomly to get more convergent behavior Otherwise we may get cyclic behavior (hard to converge) Learning rate Correcting the function for individual instances will lead to never-ending, non-convergent updates (correct one, and miss the other) The learning will continuously “chase” the latest sample Correction for individual instances with the eventual miniscule learning rates will not modify the function Drewbacks Batch / Stochastic gradient descent is an unbiased estimate of the expected loss E[Loss⁡(f(X;W),g(X))]=E[div⁡(f(X;W),g(X))] E[\\operatorname{Loss}(f(X ; W), g(X))]=E[\\operatorname{div}(f(X ; W), g(X))] E[Loss(f(X;W),g(X))]=E[div(f(X;W),g(X))] But the variance of the empirical risk in batch gradient is {% math_inline %}\\frac{1}{N}{% endmath_inline %} times compared to stochastic gradient descent Like using {% math_inline %}\\frac{1}{N}\\sum{X}{% endmath_inline %} and {% math_inline %}X_i{% endmath_inline %} to estimate {% math_inline %}\\bar{X}{% endmath_inline %} Mini-batch gradient descent Adjust the function at a small, randomly chosen subset of points Also an unbiased estimate of the expected error, and the variance is relatively small compared to SGD The mini-batch size is a hyper parameter to be optimized Convergence depends on learning rate Simple technique: fix learning rate until the error plateaus, then reduce learning rate by a fixed factor (e.g. 10) Advanced methods: Adaptive updates, where the learning rate is itself determined as part of the estimation "},"L07 Optimizater.html":{"url":"L07 Optimizater.html","title":"7 Optimizater","keywords":"","body":"Optimizers Momentum and Nestorov’s method improve convergence by normalizing the mean (first moment) of the derivatives Considering the second moments RMS Prop / Adagrad / AdaDelta / ADAM1 Simple gradient and momentum methods still demonstrate oscillatory behavior in some directions2 Depends on magic step size parameters (learning rate) Need to dampen step size in directions with high motion Second order term (use variation to smooth it) Scale down updates with large mean squared derivatives scale up updates with small mean squared derivatives RMS Prop Notion The squared derivative is ∂w2D=(∂wD)2\\partial_{w}^{2} D=\\left(\\partial_{w} D\\right)^{2}∂w2​D=(∂w​D)2 The mean squared derivative is E[∂W2D]E\\left[\\partial_{W}^{2} D\\right]E[∂W2​D] This is a variant on the basic mini-batch SGD algorithm Updates are by parameter E[∂w2D]k=γE[∂w2D]k−1+(1−γ)(∂w2D)k E\\left[\\partial_{w}^{2} D\\right]_{k}=\\gamma E\\left[\\partial_{w}^{2} D\\right]_{k-1}+(1-\\gamma)\\left(\\partial_{w}^{2} D\\right)_{k} E[∂w2​D]k​=γE[∂w2​D]k−1​+(1−γ)(∂w2​D)k​ wk+1=wk−ηE[∂w2D]k+ϵ∂wD w_{k+1}=w_{k}-\\frac{\\eta}{\\sqrt{E\\left[\\partial_{w}^{2} D\\right]_{k}+\\epsilon}} \\partial_{w} D wk+1​=wk​−E[∂w2​D]k​+ϵ​η​∂w​D If using the same step over a long period, E[∂w2D]k+ϵ≈∣∂wD∣\\sqrt{E\\left[\\partial_{w}^{2} D\\right]_{k}+\\epsilon} \\approx |\\partial_{w} D|E[∂w2​D]k​+ϵ​≈∣∂w​D∣ So wk+1=wk−sign(∂wD)ηw_{k+1}=w_{k}-\\text{sign} (\\partial_{w} D )\\etawk+1​=wk​−sign(∂w​D)η Only the sign remain, similar to RProp Adam RMS prop only considers a second-moment normalized version of the current gradient ADAM utilizes a smoothed version of the momentum-augmented gradient Considers both first and second moments mk=δmk−1+(1−δ)(∂wD)k m_{k}=\\delta m_{k-1}+(1-\\delta)\\left(\\partial_{w} D\\right)_{k} mk​=δmk−1​+(1−δ)(∂w​D)k​ vk=γvk−1+(1−γ)(∂w2D)k v_{k}=\\gamma v_{k-1}+(1-\\gamma)\\left(\\partial_{w}^{2} D\\right)_{k} vk​=γvk−1​+(1−γ)(∂w2​D)k​ mk^=mk1−δk,v^k=vk1−γk \\hat{m_k}=\\frac{m_{k}}{1-\\delta^{k}}, \\quad \\quad \\hat{v}_{k}=\\frac{v_{k}}{1-\\gamma^{k}} mk​^​=1−δkmk​​,v^k​=1−γkvk​​ wk+1=wk−ηv^k+ϵm^k w_{k+1}=w_{k}-\\frac{\\eta}{\\sqrt{\\hat{v}_{k}+\\epsilon}} \\hat{m}_{k} wk+1​=wk​−v^k​+ϵ​η​m^k​ Typically δ≈1\\delta \\approx 1δ≈1, initalize mk−1,vk−1≈0m_{k-1}, v_{k-1} \\approx 0 mk−1​,vk−1​≈0, so 1−δ≈01- \\delta \\approx 01−δ≈0, will be very slow to update in the beginning So we need mk^=mk1−δk\\hat{m_k}=\\frac{m_{k}}{1-\\delta^{k}}mk​^​=1−δkmk​​ term to scale up in the beginning Tricks To make the network converge better, we can consider the following aspects The Divergence Dropout Batch normalization Gradient clipping Data augmentation Divergence What shape do we want the divergence function would be? Must be smooth and not have many poor local optima The best type of divergence is steep far from the optimum, but shallow at the optimum But not too shallow(hard to converge to minimum) The choice of divergence affects both the learned network and results Common choices L2 divergence Div=12∑i(yi−di)2 D i v=\\frac{1}{2} \\sum_{i}\\left(y_{i}-d_{i}\\right)^{2} Div=21​i∑​(yi​−di​)2 KL divergence Div=∑idilog⁡(di)−∑idilog⁡(yi) \tD i v=\\sum_{i} d_{i} \\log \\left(d_{i}\\right)-\\sum_{i} d_{i} \\log \\left(y_{i}\\right) \tDiv=i∑​di​log(di​)−i∑​di​log(yi​) L2 is particularly appropriate when attempting to perform regression Numeric prediction For L2 divergence the derivative w.r.t. the pre-activation of the output layer is : ∇z12∥y−d∥2=(y−d)Jy(z)\\nabla_{z} \\frac{1}{2}\\|y-d\\|^{2}=(y-d) J_{y}(z)∇z​21​∥y−d∥2=(y−d)Jy​(z) We literally “propagate” the error (y−d)(y-d)(y−d) backward Which is why the method is sometimes called “error backpropagation” The KL divergence is better when the intent is classification The output is a probability vector Batch normalization Covariate shifts problem Training assumes the training data are all similarly distributed (So as mini-batch) In practice, each minibatch may have a different distribution Which may occur in each layer of the network Minimize one batch cannot give the correction of other batches Solution Move all batches to have a mean of 0 and unit standard deviation Eliminates covariate shift between batches Batch normalization is a covariate adjustment unit that happens after the weighted addition of inputs (affine combination) but before the application of activation 5 Steps Covariate shift to standard position ui=zi−μBσB2+ϵ u_{i}=\\frac{z_{i}-\\mu_{B}}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}} ui​=σB2​+ϵ​zi​−μB​​ Shift to right position zi^=γui+β \t\\hat{z_i} = \\gamma u_i + \\beta \tzi​^​=γui​+β Backpropagation The outputs are now functions of μB\\mu_BμB​ and σB2\\sigma_B^2σB2​ which are functions of the entire minibatch Div⁡(MB)=1B∑tDiv⁡(Yt(Xt,μB,σB2),dt(Xt)) \\operatorname{Div}(M B)=\\frac{1}{B} \\sum_{t} \\operatorname{Div}\\left(Y_{t}\\left(X_{t}, \\mu_{B}, \\sigma_{B}^{2}\\right), d_{t}\\left(X_{t}\\right)\\right) Div(MB)=B1​t∑​Div(Yt​(Xt​,μB​,σB2​),dt​(Xt​)) The divergence for each YtY_tYt​ depends on all the XtX_tXt​ within the mini-batch Is a vector function over the mini-batch Using influence diagram to caculate derivatives3 Goal We need to caculate the learnable parameters dDivγ,dDivβ\\frac{d D i v}{\\gamma}, \\frac{d D i v}{\\beta}γdDiv​,βdDiv​, and the affine combination dDivzi\\frac{d D i v}{z_i}zi​dDiv​ ∂Div∂zi=∂Div∂ui⋅∂ui∂zi+∂Div∂σB2⋅∂σB2∂zi+∂Div∂μB⋅∂μB∂zi \\frac{\\partial D i v}{\\partial z_{i}}=\\frac{\\partial D i v}{\\partial u_{i}} \\cdot \\frac{\\partial u_{i}}{\\partial z_{i}}+\\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}} \\cdot \\frac{\\partial \\sigma_{B}^{2}}{\\partial z_{i}}+\\frac{\\partial D i v}{\\partial \\mu_{B}} \\cdot \\frac{\\partial \\mu_{B}}{\\partial z_{i}} ∂zi​∂Div​=∂ui​∂Div​⋅∂zi​∂ui​​+∂σB2​∂Div​⋅∂zi​∂σB2​​+∂μB​∂Div​⋅∂zi​∂μB​​ So we need extra ∂Div∂ui,∂Div∂σB2,∂Div∂μB\\frac{\\partial D i v}{\\partial u_{i}},\\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}},\\frac{\\partial D i v}{\\partial \\mu_{B}} ∂ui​∂Div​,∂σB2​∂Div​,∂μB​∂Div​ Preparation μB=1B∑i=1BziσB2=1B∑i=1B(zi−μB)2 \\mu_{B}=\\frac{1}{B} \\sum_{i=1}^{B} z_{i}\\quad \\quad \\sigma_{B}^{2}=\\frac{1}{B} \\sum_{i=1}^{B}\\left(z_{i}-\\mu_{B}\\right)^{2} μB​=B1​i=1∑B​zi​σB2​=B1​i=1∑B​(zi​−μB​)2 ui=zi−μBσB2+ϵzi^=γui+β u_{i}=\\frac{z_{i}-\\mu_{B}}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}} \\quad \\quad \\hat{z_i} = \\gamma u_i + \\beta ui​=σB2​+ϵ​zi​−μB​​zi​^​=γui​+β For the first term ∂Div∂ui⋅∂ui∂zi\\frac{\\partial D i v}{\\partial u_{i}} \\cdot \\frac{\\partial u_{i}}{\\partial z_{i}}∂ui​∂Div​⋅∂zi​∂ui​​ First caculate dDivγ,dDivβ\\frac{d D i v}{\\gamma}, \\frac{d D i v}{\\beta}γdDiv​,βdDiv​ dDivdβ=dDivdz^dDivdγ=udDivdz^ \\frac{d D i v}{d \\beta}=\\frac{d D i v}{d \\hat{z}} \\quad \\quad \\frac{d D i v}{d \\gamma}=u \\frac{d D i v}{d \\hat{z}} dβdDiv​=dz^dDiv​dγdDiv​=udz^dDiv​ ∂ui∂zi=1σB2+ϵ\\frac{\\partial u_{i}}{\\partial z_{i}} = \\frac{1}{\\sqrt{\\sigma^2_B +\\epsilon}}∂zi​∂ui​​=σB2​+ϵ​1​, so the first term = ∂Div∂ui⋅1σB2+ϵ\\frac{\\partial D i v}{\\partial u_{i}} \\cdot \\frac{1}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}}∂ui​∂Div​⋅σB2​+ϵ​1​ For the second term ∂Div∂σB2⋅∂σB2∂zi\\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}} \\cdot \\frac{\\partial \\sigma_{B}^{2}}{\\partial z_{i}}∂σB2​∂Div​⋅∂zi​∂σB2​​ Caculate ∂Div∂σB2\\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}}∂σB2​∂Div​ ∂Div∂σB2=∑∂Div∂ui∂ui∂σB2 \\frac{\\partial Div}{\\partial \\sigma_{B}^{2}}=\\sum \\frac{\\partial Div}{\\partial u_{i}} \\frac{\\partial u_{i}}{\\partial \\sigma_{B}^{2}} ∂σB2​∂Div​=∑∂ui​∂Div​∂σB2​∂ui​​ ∂Div∂σB2=−12(σB2+ϵ)−3/2∑i=1B∂Div∂ui(zi−μB) \\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}}=\\frac{-1}{2}\\left(\\sigma_{B}^{2}+\\epsilon\\right)^{-3 / 2} \\sum_{i=1}^{B} \\frac{\\partial D i v}{\\partial u_{i}}\\left(z_{i}-\\mu_{B}\\right) ∂σB2​∂Div​=2−1​(σB2​+ϵ)−3/2i=1∑B​∂ui​∂Div​(zi​−μB​) And ∂σB2∂zi\\frac{\\partial \\sigma_{B}^{2}}{\\partial z_{i}}∂zi​∂σB2​​ ∂σB2∂zi=2(zi−μB)B \\frac{\\partial \\sigma_{B}^{2}}{\\partial z_{i}}=\\frac{2\\left(z_{i}-\\mu_{B}\\right)}{B} ∂zi​∂σB2​​=B2(zi​−μB​)​ So the second term = ∂Div∂σB2⋅2(zi−μB)B\\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}} \\cdot \\frac{2\\left(z_{i}-\\mu_{B}\\right)}{B}∂σB2​∂Div​⋅B2(zi​−μB​)​ Finally for the third term ∂Div∂μB⋅∂μB∂zi\\frac{\\partial D i v}{\\partial \\mu_{B}} \\cdot \\frac{\\partial \\mu_{B}}{\\partial z_{i}}∂μB​∂Div​⋅∂zi​∂μB​​ Caculate ∂Div∂μB\\frac{\\partial D i v}{\\partial \\mu_{B}}∂μB​∂Div​ ∂Div∂μB=∑∂Div∂μi∂μi∂μB+∂Div∂σB2∂σB2∂μB \\frac{\\partial D i v}{\\partial \\mu_B}=\\sum \\frac{\\partial Div}{\\partial \\mu_{i}} \\frac{\\partial \\mu_{i}}{\\partial \\mu_{B}}+\\frac{\\partial Div}{\\partial\\sigma_{B}^{2} } \\frac{\\partial \\sigma_{B}^{2}}{\\partial \\mu_{B}} ∂μB​∂Div​=∑∂μi​∂Div​∂μB​∂μi​​+∂σB2​∂Div​∂μB​∂σB2​​ ∂Div∂μB=(∑i=1B∂Div∂ui⋅−1σB2+ϵ)+∂Div∂σB2⋅∑i=1B−2(zi−μB)B \\frac{\\partial D i v}{\\partial \\mu_{B}}=\\left(\\sum_{i=1}^{B} \\frac{\\partial D i v}{\\partial u_{i}} \\cdot \\frac{-1}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}}\\right)+\\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}} \\cdot \\frac{\\sum_{i=1}^{B}-2\\left(z_{i}-\\mu_{B}\\right)}{B} ∂μB​∂Div​=(i=1∑B​∂ui​∂Div​⋅σB2​+ϵ​−1​)+∂σB2​∂Div​⋅B∑i=1B​−2(zi​−μB​)​ The last term is zero, and because μz=1B∑zi\\mu_z = \\frac{1}{B} \\sum z_iμz​=B1​∑zi​ ∂μB∂zi=1B \\frac{\\partial \\mu_{B}}{\\partial z_{i}}=\\frac{1}{B} ∂zi​∂μB​​=B1​ So the third term = ∂Div∂μB⋅1B\\frac{\\partial D i v}{\\partial \\mu_{B}} \\cdot \\frac{1}{B}∂μB​∂Div​⋅B1​ Overall ∂Div∂zi=∂Div∂ui⋅1σB2+ϵ+∂Div∂σB2⋅2(zi−μB)B+∂Div∂μB⋅1B \\frac{\\partial D i v}{\\partial z_{i}}=\\frac{\\partial D i v}{\\partial u_{i}} \\cdot \\frac{1}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}}+\\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}} \\cdot \\frac{2\\left(z_{i}-\\mu_{B}\\right)}{B}+\\frac{\\partial D i v}{\\partial \\mu_{B}} \\cdot \\frac{1}{B} ∂zi​∂Div​=∂ui​∂Div​⋅σB2​+ϵ​1​+∂σB2​∂Div​⋅B2(zi​−μB​)​+∂μB​∂Div​⋅B1​ ∂Div∂σB2=−12(σB2+ϵ)−3/2∑i=1B∂Div∂ui(zi−μB) \\frac{\\partial D i v}{\\partial \\sigma_{B}^{2}}=\\frac{-1}{2}\\left(\\sigma_{B}^{2}+\\epsilon\\right)^{-3 / 2} \\sum_{i=1}^{B} \\frac{\\partial D i v}{\\partial u_{i}}\\left(z_{i}-\\mu_{B}\\right) ∂σB2​∂Div​=2−1​(σB2​+ϵ)−3/2i=1∑B​∂ui​∂Div​(zi​−μB​) ∂Div∂μB=−1σB2+ϵ∑i=1B∂Div∂ui \\frac{\\partial D i v}{\\partial \\mu_{B}}=\\frac{-1}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}}\\sum_{i=1}^{B} \\frac{\\partial D i v}{\\partial u_{i}} ∂μB​∂Div​=σB2​+ϵ​−1​i=1∑B​∂ui​∂Div​ Inference On test data, BN requires μB\\mu_BμB​ and σB2\\sigma_B^2σB2​ We will use the average over all training minibatches μBN=1Nbatches∑batμB(batch) \\mu_{B N}=\\frac{1}{\\text {Nbatches}} \\sum_{b a t} \\mu_{B}(\\text {batch}) μBN​=Nbatches1​bat∑​μB​(batch) σBN2=B(B−1)Nbatches∑batchσB2(batch) \\sigma_{B N}^{2}=\\frac{B}{(B-1) N b a t c h e s} \\sum_{b a t c h} \\sigma_{B}^{2}(\\text {batch}) σBN2​=(B−1)NbatchesB​batch∑​σB2​(batch) Note: these are neuron-specific μB(batch),σBbatch\\mu_B(batch), \\sigma_B{batch}μB​(batch),σB​batch are obtained from the final converged network The 𝐵/(𝐵 − 1) term gives us an unbiased estimator for the variance What can it do Improves both convergence rate and neural network performance Anecdotal evidence that BN eliminates the need for dropout To get maximum benefit from BN, learning rates must be increased and learning rate decay can be faster Since the data generally remain in the high-gradient regions of the activations e.g. For sigmoid function, move data to the linear part, the gradient is high Also needs better randomization of training data order Smoothness Smoothness through network structure MLPs are universal approximators For a given number of parameters, deeper networks impose more smoothness than shallow&wide ones Each layer restricts the shape of the function Smoothness through weight constrain Regularizer The \"desired” output is generally smooth Capture statistical or average trends Overfitting But an unconstrained model will model individual instances instead Why overfitting?4 Using a sigmoid activation, as ∣w∣|w|∣w∣ increases, the response becomes steeper Constraining the weights to be low will force slower perceptrons and smoother output response Regularized training: minimize the loss while also minimizing the weights L(W1,W2,…,WK)=Loss⁡(W1,W2,…,WK)+12λ∑k∥Wk∥22 L\\left(W_{1}, W_{2}, \\ldots, W_{K}\\right)=\\operatorname{Loss}\\left(W_{1}, W_{2}, \\ldots, W_{K}\\right)+\\frac{1}{2} \\lambda \\sum_{k}\\left\\|W_{k}\\right\\|_{2}^{2} L(W1​,W2​,…,WK​)=Loss(W1​,W2​,…,WK​)+21​λk∑​∥Wk​∥22​ λ\\lambdaλ is the regularization parameter whose value depends on how important it is for us to want to minimize the weights Increasing assigns greater importance to shrinking the weights Make greater error on training data, to obtain a more acceptable network Dropout “Dropout” is a stochastic data/model erasure method that sometimes forces the network to learn more robust models Bagging method Using ensemble classifiers to improve prediction Dropout For each input, at each iteration, “turn off” each neuron with a probability 1−α1-\\alpha1−α Also turn off inputs similarly Backpropagation is effectively performed only over the remaining network The effective network is different for different inputs Effectively learns a network that averages over all possible networks (Bagging) Dropout as a mechanism to increase pattern density Dropout forces the neurons to learn “rich” and redundant patterns E.g. without dropout, a noncompressive layer may just “clone” its input to its output Transferring the task of learning to the rest of the network upstream Implementation The expected output of the neuron is yi(k)=ασ(∑jwji(k)yj(k−1)+bi(k)) y_{i}^{(k)}=\\alpha \\sigma\\left(\\sum_{j} w_{j i}^{(k)} y_{j}^{(k-1)}+b_{i}^{(k)}\\right) yi(k)​=ασ(j∑​wji(k)​yj(k−1)​+bi(k)​) During test, push the a to all outgoing weights zi(k)=∑jwji(k)yj(k−1)+bi(k)=∑jwji(k)ασ(zj(k−1))+bi(k)=∑j(αwji(k))σ(zj(k−1))+bi(k) \\begin{aligned} z_{i}^{(k)} &=\\sum_{j} w_{j i}^{(k)} y_{j}^{(k-1)}+b_{i}^{(k)} \\\\\\\\ &=\\sum_{j} w_{j i}^{(k)} \\alpha \\sigma\\left(z_{j}^{(k-1)}\\right)+b_{i}^{(k)} \\\\\\\\ &=\\sum_{j}\\left(\\alpha w_{j i}^{(k)}\\right) \\sigma\\left(z_{j}^{(k-1)}\\right)+b_{i}^{(k)} \\end{aligned} zi(k)​​=j∑​wji(k)​yj(k−1)​+bi(k)​=j∑​wji(k)​ασ(zj(k−1)​)+bi(k)​=j∑​(αwji(k)​)σ(zj(k−1)​)+bi(k)​​ So Wtest=αWtrainedW_{test} = \\alpha W_{trained}Wtest​=αWtrained​ Instead of multiplying every output by all weights by α\\alphaα, multiply all weight by α\\alphaα Alternate implementation During training, replace the activation of all neurons in the network by α−1σ(.)\\alpha ^{-1} \\sigma(.)α−1σ(.) Use σ(.)\\sigma(.)σ(.) as the activation during testing, and not modify the weights More tricks Obtain training data Use appropriate representation for inputs and outputs Data Augmentation Choose network architecture More neurons need more data Deep is better, but harder to train Choose the appropriate divergence function Choose regularization Choose heuristics batch norm, dropout ... Choose optimization algorithm Adagrad / Adam / SGD Perform a grid search for hyper parameters (learning rate, regularization parameter, …) on held-out data Train Evaluate periodically on validation data, for early stopping if required 1. A good summary of recent optimizers can be seen in here. ↩ 2. Animations for optimization algorithms ↩ 3. A simple and clear demostration of 2 variables in a single network ↩ 4. The perceptrons in the network are individually capable of sharp changes in output ↩ 5. Batch normalization in Neural Networks ↩ "},"L08 Motivation of CNN.html":{"url":"L08 Motivation of CNN.html","title":"8 Motivation Of CNN","keywords":"","body":" 「Scan for patterns」 Movivation Find a word in a signal of find a item in picture The need for shift invariance The location of a pattern is not important So we can scan with a same MLP for the pattern Just one giant network Restriction: All subnets are identical Regular networks vs. scanning networks In a regular MLP every neuron in a layer is connected by a unique weight to every unit in the previous layer In a scanning MLP each neuron is connected to a subset of neurons in the previous layer The weights matrix is sparse The weights matrix is block structured with identical blocks The network is a shared-parameter model Modifications Order changed Intuitivly, scan at one position and get output, then scan next place But we can also first scan all the position at one layer, then the next layer The result is the same Distrubuting the scan Evaluate small pattern in the first layer The higher layer implicitly learns the arrangement of sub patterns that represents the larger pattern Why distribute? More generalizable Distribution forces localized patterns in lower layers Number of parameters Fewer parameters Significant gains from shared computation Terminology The pattern in the input image that each filter sees is its 「Receptive Field」 Stride Effectively increasing the granularity of the scan This will result in a reduction of the size of the resulting maps Non-overlapped strides Partition the output of the layer into blocks, no overlap Within each block only retain the highest value Pooling We would like to account for some jitter in the first-level patterns Max pooling Is just a neuron This entire structure is called a Convolutional Neural Network The 1-D scan version of the convolutional neural network is the time-delay neural network Used primarily for speech recognition Max pooling optional: jitter matters in speech "},"L09 Cascade Correlation.html":{"url":"L09 Cascade Correlation.html","title":"9 Cascade Correlation","keywords":"","body":" A alternate way to think the architecture of network Cascade-Correlation Algorithm Start with direct I/O connections only. No hidden units. Train output-layer weights using BP or Quickprop. If error is now acceptable, quit. Else, Create one new hidden unit offline. Create a pool of candidate units. Each gets all available inputs. Outputs are not yet connected to anything. Train the incoming weights to maximize the match (covariance) between each unit’s output and the residual error: When all are quiescent, tenure the winner and add it to active net. Kill all the other candidates. Re-train output layer weights and repeat the cycle until done. Why Is Backprop So Slow? Moving Targets All hidden units are being trained at once, changing the environment seen by the other units as they train. Herd Effect Each unit must find a distinct job -- some component of the error to correct. All units scramble for the most important jobs. No central authority or communication. Once a job is taken, it disappears and units head for the next-best job, including the unit that took the best job. This is a very inefficient way to assign a distinct useful job to each unit. Advantages of Cascade Correlation No need to guess size and topology of net in advance. Can build deep nets with higher-order features. Much faster than Backprop or Quickprop. Trains just one layer of weights at a time (fast). Works on smaller training sets (in some cases, at least). Old feature detectors are frozen, not cannibalized, so good for incremental “curriculum” training. Good for parallel implementation. "},"L10 CNN architecture.html":{"url":"L10 CNN architecture.html","title":"10 CNN Architecture","keywords":"","body":"Architecture A convolutional neural network comprises “convolutional” and “downsampling ” layers Convolutional layers comprise neurons that scan their input for patterns Downsampling layers perform max operations on groups of outputs from the convolutional layers Perform on individual map For reduce the number of parameters The two may occur in any sequence, but typically they alternate Followed by an MLP with one or more layers A convolutional layer Each activation map has two components An affine map, obtained by convolution over maps in the previous layer Each affine map has, associated with it, a learnable filter An activation that operates on the output of the convolution What is a convolution Scanning an image with a “filter” Equivalent to scanning with an MLP Weights size of the filter ×\\times× no. of maps in previous layer Size Image size: N×NN\\times NN×N Filter: M×MM\\times MM×M Stride: SSS Output size = ⌊(N−M)/S⌋+1\\lfloor(N-M) / S\\rfloor+1⌊(N−M)/S⌋+1 Jargon Filters are often called “Kernels” The outputs of individual filters are called “channels” Notion Each convolution layer maintains the size of the image With appropriate zero padding If performed without zero padding it will decrease the size of the input Each convolution layer may increase the number of maps from the previous layer Depends on the number of filters Each pooling layer with hop DDD decreases the size of the maps by a factor of DDD Filters within a layer must all be the same size, but sizes may vary with layer Similarly for pooling, DDD may vary with layer In general the number of convolutional filters increases with layers Because the patterns gets more complex, hence larger combinations of patterns to capture Training is as in the case of the regular MLP The only difference is in the structure of the network "},"L12 Prop of CNN.html":{"url":"L12 Prop of CNN.html","title":"11 Prop Of CNN","keywords":"","body":"Convolution Each position in zzz consists of convolution result in previous map Way for shrinking the maps Stride greater than 1 Downsampling (not necessary) Typically performed with strides > 1 Pooling Maxpooling Note: keep tracking of location of max (needed while back prop) Mean pooling Learning the CNN Training is as in the case of the regular MLP The only difference is in the structure of the network Define a divergence between the desired output and true output of the network in response to any input Network parameters are trained through variants of gradient descent Gradients are computed through backpropagation Final flat layers Backpropagation continues in the usual manner until the computation of the derivative of the divergence Recall in Backpropagation Step 1: compute ∂Div∂zn\\frac{\\partial Div}{\\partial z^{n}}∂zn∂Div​、∂Div∂yn\\frac{\\partial Div}{\\partial y^{n}}∂yn∂Div​ Step 2: compute ∂Div∂wn\\frac{\\partial Div}{\\partial w^{n}}∂wn∂Div​ according to step 1 Convolutional layer Computing ∇Z(l)Div\\nabla_{Z(l)} D i v∇Z(l)​Div dDivdz(l,m,x,y)=dDivdY(l,m,x,y)f′(z(l,m,x,y)) \\frac{d D i v}{d z(l, m, x, y)}=\\frac{d D i v}{d Y(l, m, x, y)} f^{\\prime}(z(l, m, x, y)) dz(l,m,x,y)dDiv​=dY(l,m,x,y)dDiv​f′(z(l,m,x,y)) Simple compont-wise computation Computing ∇Y(l−1)Div\\nabla_{Y(l-1)} D i v∇Y(l−1)​Div Each Y(l−1,m,x,y)Y(l-1,m,x,y)Y(l−1,m,x,y) affects several z(l,n,x′,y′)z(l,n,x\\prime,y\\prime)z(l,n,x′,y′) terms for every nnn (map) Through wl(m,n,x−x′,y−y′)w_l(m,n,x-x\\prime,y-y\\prime)wl​(m,n,x−x′,y−y′) Affects terms in all lthl^{th}lth layer maps All of them contribute to the derivative of the divergence Y(l−1,m,x,y)Y(l-1,m,x,y)Y(l−1,m,x,y) Derivative w.r.t a specific yyy term dDivdY(l−1,m,x,y)=∑n∑x′,y′dDivdz(l,n,x′,y′)dz(l,n,x′,y′)dY(l−1,m,x,y) \\frac{d D i v}{d Y(l-1, m, x, y)}=\\sum_{n} \\sum_{x^{\\prime}, y^{\\prime}} \\frac{d D i v}{d z\\left(l, n, x^{\\prime}, y^{\\prime}\\right)} \\frac{d z\\left(l, n, x^{\\prime}, y^{\\prime}\\right)}{d Y(l-1, m, x, y)} dY(l−1,m,x,y)dDiv​=n∑​x′,y′∑​dz(l,n,x′,y′)dDiv​dY(l−1,m,x,y)dz(l,n,x′,y′)​ dDivdY(l−1,m,x,y)=∑n∑x′,y′dDivdz(l,n,x′,y′)wl(m,n,x−x′,y−y′) \\frac{d D i v}{d Y(l-1, m, x, y)}=\\sum_{n} \\sum_{x \\prime, y^{\\prime}} \\frac{d D i v}{d z\\left(l, n, x^{\\prime}, y^{\\prime}\\right)} w_{l}\\left(m, n, x-x^{\\prime}, y-y^{\\prime}\\right) dY(l−1,m,x,y)dDiv​=n∑​x′,y′∑​dz(l,n,x′,y′)dDiv​wl​(m,n,x−x′,y−y′) Computing ∇w(l)Div\\nabla_{w(l)} D i v∇w(l)​Div Each weight wl(m,n,x′,y′)w_l(m,n,x\\prime,y\\prime)wl​(m,n,x′,y′) also affects several z(l,n,x,y)z(l,n,x,y)z(l,n,x,y) term for every nnn Affects terms in only one ZZZ map (the nth map) All entries in the map contribute to the derivative of the divergence w.r.t. wl(m,n,x′,y′)w_l(m,n,x\\prime,y\\prime)wl​(m,n,x′,y′) Derivative w.r.t a specific www term dDivdwl(m,n,x,y)=∑x′,y′dDivdz(l,n,x′,y′)dz(l,n,x′,y′)dwl(m,n,x,y) \\frac{d D i v}{d w_{l}(m, n, x, y)}=\\sum_{x^{\\prime}, y^{\\prime}} \\frac{d D i v}{d z\\left(l, n, x^{\\prime}, y^{\\prime}\\right)} \\frac{d z\\left(l, n, x^{\\prime}, y^{\\prime}\\right)}{d w_{l}(m, n, x, y)} dwl​(m,n,x,y)dDiv​=x′,y′∑​dz(l,n,x′,y′)dDiv​dwl​(m,n,x,y)dz(l,n,x′,y′)​ dDivdwl(m,n,x,y)=∑x′,y′dDivdz(l,n,x′,y′)Y(l−1,m,x′+x,y′+y) \\frac{d D i v}{d w_{l}(m, n, x, y)}=\\sum_{x \\prime, y^{\\prime}} \\frac{d D i v}{d z\\left(l, n, x^{\\prime}, y^{\\prime}\\right)} Y\\left(l-1, m, x^{\\prime}+x, y^{\\prime}+y\\right) dwl​(m,n,x,y)dDiv​=x′,y′∑​dz(l,n,x′,y′)dDiv​Y(l−1,m,x′+x,y′+y) Summary In practice dDivdY(l−1,m,x,y)=∑n∑x′,y′dDivdz(l,n,x′,y′)wl(m,n,x−x′,y−y′) \\frac{d D i v}{d Y(l-1, m, x, y)}=\\sum_{n} \\sum_{x \\prime, y^{\\prime}} \\frac{d D i v}{d z\\left(l, n, x^{\\prime}, y^{\\prime}\\right)} w_{l}\\left(m, n, x-x^{\\prime}, y-y^{\\prime}\\right) dY(l−1,m,x,y)dDiv​=n∑​x′,y′∑​dz(l,n,x′,y′)dDiv​wl​(m,n,x−x′,y−y′) This is a convolution, with defferent order Use mirror image to do normal convolution (flip up down / flip left right) In practice, the derivative at each (x,y) location is obtained from all ZZZ maps This is just a convolution of ∂Div∂z(l,n,x,y)\\frac{\\partial Div}{\\partial z(l,n,x,y)}∂z(l,n,x,y)∂Div​ by the inverted filter After zero padding it first with L−1L-1L−1 zeros on every side Note: the x′,y′x\\prime, y\\primex′,y′ refer to the location in filter Shifting down and right by K−1K-1K−1, such that 0,00,00,0 becomes K−1,K−1K-1,K-1K−1,K−1 zshift(l,n,m,x,y)=z(l,n,x−K+1,y−K+1) z_{\\text {shift}}(l, n, m, x, y)=z(l, n, x-K+1, y-K+1) zshift​(l,n,m,x,y)=z(l,n,x−K+1,y−K+1) ∂Div∂y(l−1,m,x,y)=∑n∑x′,y′w^(l,n,m,x′,y′)∂Div∂zshift(l,n,x+x′,y+y′) \\frac{\\partial D i v}{\\partial y(l-1, m, x, y)}=\\sum_{n} \\sum_{x^{\\prime}, y^{\\prime}} \\widehat{w}\\left(l, n, m, x^{\\prime}, y^{\\prime}\\right) \\frac{\\partial D i v}{\\partial z_{s h i f t}\\left(l, n, x+x^{\\prime}, y+y^{\\prime}\\right)} ∂y(l−1,m,x,y)∂Div​=n∑​x′,y′∑​w(l,n,m,x′,y′)∂zshift​(l,n,x+x′,y+y′)∂Div​ Regular convolution running on shifted derivative maps using flipped filter Pooling Pooling is typically performed with strides > 1 Results in shrinking of the map Downsampling Derivative of Max pooling dDivdY(l,m,k,l)={dDivdU(l,m,i,j) if (k,l)=P(l,m,i,j)0 otherwise  \\frac{d D i v}{d Y(l, m, k, l)}=\\left\\{\\begin{array}{c} \\frac{d D i v}{d U(l, m, i, j)} \\text { if }(k, l)=P(l, m, i, j) \\\\ 0 \\text { otherwise } \\end{array}\\right. dY(l,m,k,l)dDiv​={dU(l,m,i,j)dDiv​ if (k,l)=P(l,m,i,j)0 otherwise ​ Max pooling selects the largest from a pool of elements 1 Derivative of Mean pooling The derivative of mean pooling is distributed over the pool dy(l,m,k,n)=1Klpool2du(l,m,k,n) d y(l, m, k, n)=\\frac{1}{K_{l p o o l}^{2}} d u(l, m, k, n) dy(l,m,k,n)=Klpool2​1​du(l,m,k,n) Transposed Convolution We’ve always assumed that subsequent steps shrink the size of the maps Can subsequent maps increase in size 2 Output size is typically an integer multiple of input +1 if filter width is odd Model variations Very deep networks 100 or more layers in MLP Formalism called “Resnet” Depth-wise convolutions Instead of multiple independent filters with independent parameters, use common layer-wise weights and combine the layers differently for each filter Depth-wise convolutions In depth-wise convolution the convolution step is performed only once The simple summation is replaced by a weighted sum across channels Different weights (for summation) produce different output channels Models For CIFAR 10 Le-net 5 3 For ILSVRC(Imagenet Large Scale Visual Recognition Challenge) AlexNet NN contains 60 million parameters and 650,000 neurons 5 convolutional layers, some of which are followed by max-pooling layers 3 fully-connected layers VGGNet Only used 3x3 filters, stride 1, pad 1 Only used 2x2 pooling filters, stride 2 ~140 million parameters in all Googlenet Multiple filter sizes simultaneously For ImageNet Resnet Last layer before addition must have the same number of filters as the input to the module Batch normalization after each convolution Densenet All convolutional Each layer looks at the union of maps from all previous layers Instead of just the set of maps from the immediately previous layer 1. Backprop Through Max-Pooling Layers? ↩ 2. Transposed Convolution Demystified ↩ 3. https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html ↩ "},"L13 RNN.html":{"url":"L13 RNN.html","title":"12 RNN","keywords":"","body":"Modelling Series In many situations one must consider a series of inputs to produce an output Outputs too may be a series Finite response model Can use convolutional neural net applied to series data (slide) Also called a Time-Delay neural network Something that happens today only affects the output of the system for days into the future Yt=f(Xt,Xt−1,…,Xt−N) Y_{t}=f\\left(X_{t}, X_{t-1}, \\ldots, X_{t-N}\\right) Yt​=f(Xt​,Xt−1​,…,Xt−N​) Infinite response systems Systems often have long-term dependencies What happens today can continue to affect the output forever Yt=f(Xt,Xt−1,…,Xt−∞) Y_{t}=f\\left(X_{t}, X_{t-1}, \\ldots, X_{t-\\infty}\\right) Yt​=f(Xt​,Xt−1​,…,Xt−∞​) Infinite response systems A one-tap NARX network 「nonlinear autoregressive network with exogenous inputs」 Yt=f(Xt,Yt−1)Y_t = f(X_t,Y_{t-1})Yt​=f(Xt​,Yt−1​) An input at t=0 affects outputs forever An explicit memory variable whose job it is to remember mt=r(yt−1,ht−1′,mt−1)ht=f(xt,mt)yt=g(ht) \\begin{array}{c} m_{t}=r\\left(y_{t-1}, h_{t-1}^{\\prime}, m_{t-1}\\right) \\\\\\\\ h_{t}=f\\left(x_{t}, m_{t}\\right) \\\\\\\\ y_{t}=g\\left(h_{t}\\right) \\end{array} mt​=r(yt−1​,ht−1′​,mt−1​)ht​=f(xt​,mt​)yt​=g(ht​)​ Jordan Network Memory unit simply retains a running average of past outputs Memory has fixed structure; does not “learn” to remember Elman Networks Separate memory state from output Only the weight from the memory unit to the hidden unit is learned But during training no gradient is backpropagated over the “1” link (Just cloned state) Problem “Simple” (or partially recurrent) because during learning current error does not actually propagate to the past State-space model ht=f(xt,ht−1)yt=g(ht) \\begin{array}{c} h_{t}=f\\left(x_{t}, h_{t-1}\\right) \\\\\\\\ y_{t}=g\\left(h_{t}\\right) \\end{array} ht​=f(xt​,ht−1​)yt​=g(ht​)​ hth_tht​ is the state of the network Model directly embeds the memory in the state State summarizes information about the entire past Recurrent neural network1 Variants All columns are identical The simplest structures are most popular Recurrent neural network Forward pass Backward pass BPTT Back Propagation Through Time Defining a divergence between the actual and desired output sequences Backpropagating gradients over the entire chain of recursion Backpropagation through time Pooling gradients with respect to individual parameters over time Notion The divergence computed is between the sequence of outputs by the network and the desired sequence of outputs DIV is a scalar function of a ..series.. of vectors This is not just the sum of the divergences at individual times Y(t)Y(t)Y(t) is the output at time ttt Yi(t)Y_i(t)Yi​(t) is the ith output Z(2)(t)Z^{(2)}(t)Z(2)(t) is the pre-activation value of the neurons at the output layer at time ttt h(t)h(t)h(t) is the output of the hidden layer at time ttt BPTT Y(t)Y(t)Y(t) is a column vector DIVDIVDIV is a scalar dDivdY(t)\\frac{d Div}{d Y(t)}dY(t)dDiv​ is a row vector Derivative at time TTT Compute dDIVdYi(T)\\frac{d DIV}{d Y_i(T)}dYi​(T)dDIV​ for all iii In general we will be required to compute dDIVdYi(t)\\frac{d DIV}{d Y_i(t)}dYi​(t)dDIV​ for all iii and ttt as we will see This can be a source of significant difficulty in many scenarios Special case, when the overall divergence is a simple sum of local divergences at each time dDIVdYi(t)=dDiv(t)dYi(t) \\frac{d D I V}{d Y_{i}(t)}=\\frac{d D i v(t)}{d Y_{i}(t)} dYi​(t)dDIV​=dYi​(t)dDiv(t)​ Compute ∇Z(2)(T)DIV\\nabla_{Z^{(2)}(T)}{D I V}∇Z(2)(T)​DIV ∇Z(2)(T)DIV=∇Y(T)DIV∇Z(2)(T)Y(T) \\nabla_{Z^{(2)}(T)}{D I V}=\\nabla_{Y(T)} D I V \\nabla_{Z^{(2)}(T)} Y(T) ∇Z(2)(T)​DIV=∇Y(T)​DIV∇Z(2)(T)​Y(T) For scalar output activation dDIVdZi(2)(T)=dDIVdYi(T)dYi(T)dZi(2)(T) \\frac{d D I V}{d Z_{i}^{(2)}(T)}=\\frac{d D I V}{d Y_{i}(T)} \\frac{d Y_{i}(T)}{d Z_{i}^{(2)}(T)} dZi(2)​(T)dDIV​=dYi​(T)dDIV​dZi(2)​(T)dYi​(T)​ For vector output activation dDIVdZi(2)(T)=∑idDIVdYj(T)dYj(T)dZi(2)(T) \\frac{d D I V}{d Z_{i}^{(2)}(T)}=\\sum_{i} \\frac{d D I V}{d Y_{j}(T)} \\frac{d Y_{j}(T)}{d Z_{i}^{(2)}(T)} dZi(2)​(T)dDIV​=i∑​dYj​(T)dDIV​dZi(2)​(T)dYj​(T)​ Compute ∇h(T)DIV\\nabla_{h_(T)}{D I V}∇h(​T)​DIV W(2)h(T)=Z(2)(T) W^{(2)} h(T) = Z^{(2)}(T) W(2)h(T)=Z(2)(T) dDIVdhi(T)=∑jdDIVdZj(2)(T)dZj(2)(T)dhi(T)=∑jwij(2)dDIVdZj(2)(T) \\frac{d D I V}{d h_{i}(T)}=\\sum_{j} \\frac{d D I V}{d Z_{j}^{(2)}(T)} \\frac{d Z_{j}^{(2)}(T)}{d h_{i}(T)}=\\sum_{j} w_{i j}^{(2)} \\frac{d D I V}{d Z_{j}^{(2)}(T)} dhi​(T)dDIV​=j∑​dZj(2)​(T)dDIV​dhi​(T)dZj(2)​(T)​=j∑​wij(2)​dZj(2)​(T)dDIV​ ∇h(T)DIV=∇Z(2)(T)DIVW(2) \\nabla_{h(T)} D I V=\\nabla_{Z^{(2)}(T)} D I V W^{(2)} ∇h(T)​DIV=∇Z(2)(T)​DIVW(2) Compute ∇W(2)DIV\\nabla_{W^{(2)}}{D I V}∇W(2)​DIV dDIVdwij(2)=dDIVdZj(2)(T)hi(T) \\frac{d D I V}{d w_{i j}^{(2)}}=\\frac{d D I V}{d Z_{j}^{(2)}(T)} h_{i}(T) dwij(2)​dDIV​=dZj(2)​(T)dDIV​hi​(T) ∇W(2)DIV=h(T)∇Z(2)(T)DIV \\nabla_{W^{(2)}} D I V=h(T) \\nabla_{Z^{(2)}(T)} D I V ∇W(2)​DIV=h(T)∇Z(2)(T)​DIV Compute ∇Z(1)(T)DIV\\nabla_{Z^{(1)}(T)}{D I V}∇Z(1)(T)​DIV dDIVdZi(1)(T)=dDIVdhi(T)dhi(T)dZi(1)(T) \\frac{d D I V}{d Z_{i}^{(1)}(T)}=\\frac{d D I V}{d h_{i}(T)} \\frac{d h_{i}(T)}{d Z_{i}^{(1)}(T)} dZi(1)​(T)dDIV​=dhi​(T)dDIV​dZi(1)​(T)dhi​(T)​ ∇Z(1)(T)DIV=∇h(T)DIV∇Z(1)(T)h(T) \\nabla_{Z^{(1)}(T)} D I V=\\nabla_{h(T)} D I V \\nabla_{Z^{(1)}(T)} h(T) ∇Z(1)(T)​DIV=∇h(T)​DIV∇Z(1)(T)​h(T) Compute ∇W(1)DIV\\nabla_{W^{(1)}}{D I V}∇W(1)​DIV W(1)X(T)+W(11)h(T−1)=Z(1)(T) W^{(1)} X(T) + W^{(11)} h(T-1)= Z^{(1)}(T) W(1)X(T)+W(11)h(T−1)=Z(1)(T) dDIVdwij(1)=dDIVdZj(1)(T)Xi(T) \\frac{d D I V}{d w_{i j}^{(1)}}=\\frac{d D I V}{d Z_{j}^{(1)}(T)} X_{i}(T) dwij(1)​dDIV​=dZj(1)​(T)dDIV​Xi​(T) ∇W(1)DIV=X(T)∇Z(1)(T)DIV \\nabla_{W^{(1)}} D I V=X(T) \\nabla_{Z^{(1)}(T)} D I V ∇W(1)​DIV=X(T)∇Z(1)(T)​DIV Compute ∇W(11)DIV\\nabla_{W^{(11)}}{D I V}∇W(11)​DIV dDIVdwii(11)=dDIVdZi(1)(T)hi(T−1) \\frac{d D I V}{d w_{i i}^{(11)}}=\\frac{d D I V}{d Z_{i}^{(1)}(T)} h_{i}(T-1) dwii(11)​dDIV​=dZi(1)​(T)dDIV​hi​(T−1) ∇W(11)DIV=h(T−1)∇Z(1)(T)DIV \\nabla_{W}^{(11)} D I V=h(T-1) \\nabla_{Z^{(1)}(T)} D I V ∇W(11)​DIV=h(T−1)∇Z(1)(T)​DIV Derivative at time T−1T-1T−1 Compute ∇Z(2)(T−1)DIV\\nabla_{Z^{(2)}(T-1)}{D I V}∇Z(2)(T−1)​DIV ∇Z(2)(T−1)DIV=∇Y(T−1)DIV∇Z(2)(T−1)Y(T−1) \\nabla_{Z^{(2)}(T-1)}{D I V}=\\nabla_{Y(T-1)} D I V \\nabla_{Z^{(2)}(T-1)} Y(T-1) ∇Z(2)(T−1)​DIV=∇Y(T−1)​DIV∇Z(2)(T−1)​Y(T−1) For scalar output activation dDIVdZi(2)(T−1)=dDIVdYi(T−1)dYi(T−1)dZi(2)(T−1) \\frac{d D I V}{d Z_{i}^{(2)}(T-1)}=\\frac{d D I V}{d Y_{i}(T-1)} \\frac{d Y_{i}(T-1)}{d Z_{i}^{(2)}(T-1)} dZi(2)​(T−1)dDIV​=dYi​(T−1)dDIV​dZi(2)​(T−1)dYi​(T−1)​ For vector output activation dDIVdZi(2)(T−1)=∑jdDIVdYj(T−1)dYj(T−1)dZi(2)(T−1) \\frac{d D I V}{d Z_{i}^{(2)}(T-1)}=\\sum_{j} \\frac{d D I V}{d Y_{j}(T-1)} \\frac{d Y_{j}(T-1)}{d Z_{i}^{(2)}(T-1)} dZi(2)​(T−1)dDIV​=j∑​dYj​(T−1)dDIV​dZi(2)​(T−1)dYj​(T−1)​ Compute ∇h(T−1)DIV\\nabla_{h_(T-1)}{D I V}∇h(​T−1)​DIV dDIVdhi(T−1)=∑jwij(2)dDIVdZj(2)(T−1)+∑jwij(11)dDIVdZj(1)(T) \\frac{d D I V}{d h_{i}(T-1)}=\\sum_{j} w_{i j}^{(2)} \\frac{d D I V}{d Z_{j}^{(2)}(T-1)}+\\sum_{j} w_{i j}^{(11)} \\frac{d D I V}{d Z_{j}^{(1)}(T)} dhi​(T−1)dDIV​=j∑​wij(2)​dZj(2)​(T−1)dDIV​+j∑​wij(11)​dZj(1)​(T)dDIV​ ∇h(T−1)DIV=∇Z(2)(T−1)DIVW(2)+∇Z(1)(T)DIVW(11) \\nabla_{h(T-1)} D I V=\\nabla_{Z^{(2)}(T-1)} D I V W^{(2)}+\\nabla_{Z^{(1)}(T)} D I V W^{(11)} ∇h(T−1)​DIV=∇Z(2)(T−1)​DIVW(2)+∇Z(1)(T)​DIVW(11) Compute ∇W(2)DIV\\nabla_{W^{(2)}}{D I V}∇W(2)​DIV dDIVdwij(2)+=dDIVdZj(2)(T−1)hi(T−1) \\frac{d D I V}{d w_{i j}^{(2)}}+=\\frac{d D I V}{d Z_{j}^{(2)}(T-1)} h_{i}(T-1) dwij(2)​dDIV​+=dZj(2)​(T−1)dDIV​hi​(T−1) ∇W(2)DIV+=h(T−1)∇Z(2)(T−1)DIV \\nabla_{W^{(2)}} D I V+=h(T-1) \\nabla_{Z^{(2)}(T-1)} D I V ∇W(2)​DIV+=h(T−1)∇Z(2)(T−1)​DIV Compute ∇Z(1)(T−1)DIV\\nabla_{Z^{(1)}(T-1)}{D I V}∇Z(1)(T−1)​DIV dDIVdZi(1)(T−1)=dDIVdhi(T−1)dhi(T−1)dZi(1)(T−1) \\frac{d D I V}{d Z_{i}^{(1)}(T-1)}=\\frac{d D I V}{d h_{i}(T-1)} \\frac{d h_{i}(T-1)}{d Z_{i}^{(1)}(T-1)} dZi(1)​(T−1)dDIV​=dhi​(T−1)dDIV​dZi(1)​(T−1)dhi​(T−1)​ ∇Z(1)(T−1)DIV=∇h(T−1)DIV∇Z(1)(T−1)h(T−1) \\nabla_{Z^{(1)}(T-1)} D I V=\\nabla_{h(T-1)} D I V \\nabla_{Z^{(1)}(T-1)} h(T-1) ∇Z(1)(T−1)​DIV=∇h(T−1)​DIV∇Z(1)(T−1)​h(T−1) Compute ∇W(1)DIV\\nabla_{W^{(1)}}{D I V}∇W(1)​DIV dDIVdwij(1)+=dDIVdZj(1)(T−1)Xi(T−1) \\frac{d D I V}{d w_{i j}^{(1)}}+=\\frac{d D I V}{d Z_{j}^{(1)}(T-1)} X_{i}(T-1) dwij(1)​dDIV​+=dZj(1)​(T−1)dDIV​Xi​(T−1) ∇W(1)DIV+=X(T−1)∇Z(1)(T−1)DIV \\nabla_{W^{(1)}} D I V+=X(T-1) \\nabla_{Z^{(1)}(T-1)} D I V ∇W(1)​DIV+=X(T−1)∇Z(1)(T−1)​DIV Compute ∇W(11)DIV\\nabla_{W^{(11)}}{D I V}∇W(11)​DIV dDIVdwij(11)+=dDIVdZj(1)(T−1)hi(T−2) \\frac{d D I V}{d w_{i j}^{(11)}}+=\\frac{d D I V}{d Z_{j}^{(1)}(T-1)} h_{i}(T-2) dwij(11)​dDIV​+=dZj(1)​(T−1)dDIV​hi​(T−2) {% math %} \\nabla{W^{(11)}} D I V+=h(T-2) \\nabla{Z^{(1)}(T-1)} D I V {% endmath %} Back Propagation Through Time dDIVdhi(−1)=∑iwij(11)dDIVdZj(1)(0) \\frac{d D I V}{d h_{i}(-1)}=\\sum_{i} w_{i j}^{(11)} \\frac{d D I V}{d Z_{j}^{(1)}(0)} dhi​(−1)dDIV​=i∑​wij(11)​dZj(1)​(0)dDIV​ dDIVdhi(k)(t)=∑jwi,j(k+1)dDIVdZj(k+1)(t)+∑jwi,j(k,k)dDIVdZj(k)(t+1) \\frac{d D I V}{d h_{i}^{(k)}(t)}=\\sum_{j} w_{i, j}^{(k+1)} \\frac{d D I V}{d Z_{j}^{(k+1)}(t)}+\\sum_{j} w_{i, j}^{(k, k)} \\frac{d D I V}{d Z_{j}^{(k)}(t+1)} dhi(k)​(t)dDIV​=j∑​wi,j(k+1)​dZj(k+1)​(t)dDIV​+j∑​wi,j(k,k)​dZj(k)​(t+1)dDIV​ dDIVdZi(k)(t)=dDIVdhi(k)(t)fk′(Zi(k)(t)) \\frac{d D I V}{d Z_{i}^{(k)}(t)}=\\frac{d D I V}{d h_{i}^{(k)}(t)} f_{k}^{\\prime}\\left(Z_{i}^{(k)}(t)\\right) dZi(k)​(t)dDIV​=dhi(k)​(t)dDIV​fk′​(Zi(k)​(t)) dDIVdwij(1)=∑tdDIVdZj(1)(t)Xi(t) \\frac{d D I V}{d w_{i j}^{(1)}}=\\sum_{t} \\frac{d D I V}{d Z_{j}^{(1)}(t)} X_{i}(t) dwij(1)​dDIV​=t∑​dZj(1)​(t)dDIV​Xi​(t) dDIVdwij(11)=∑tdDIVdZj(1)(t)hi(t−1) \\frac{d D I V}{d w_{i j}^{(11)}}=\\sum_{t} \\frac{d D I V}{d Z_{j}^{(1)}(t)} h_{i}(t-1) dwij(11)​dDIV​=t∑​dZj(1)​(t)dDIV​hi​(t−1) Algorithm Bidirectional RNN Two independent RNN Clearly, this is not an online process and requires the entire input data It is easy to learning two RNN independently Forward pass: Compute both forward and backward networks and final output Backpropagation A basic backprop routine that we will call Two calls to the routine within a higher-level wrapper 1:The Unreasonable Effectiveness of Recurrent Neural Networks "},"L14 Stability analysis and LSTMs.html":{"url":"L14 Stability analysis and LSTMs.html","title":"13 Stability Analysis And LST Ms","keywords":"","body":"Stability Will this necessarily be「Bounded Input Bounded Output」? Guaranteed if output and hidden activations are bounded But will it saturate？ Analyzing Recursion Sufficient to analyze the behavior of the hidden layer since it carries the relevant information Assumed linear systems zk=Whhk−1+Wxxk,hk=zk z_{k}=W_{h} h_{k-1}+W_{x} x_{k}, \\quad h_{k}=z_{k} zk​=Wh​hk−1​+Wx​xk​,hk​=zk​ Sufficient to analyze the response to a single input at t=0t =0t=0 (else is zero input) Simple scalar linear recursion h(t)=wh(t−1)+cx(t)h(t) = wh(t-1) + cx(t)h(t)=wh(t−1)+cx(t) h0(t)=wtcx(0)h_0(t) = w^tcx(0)h0​(t)=wtcx(0) If w>1w > 1w>1 it will blow up Simple Vector linear recursion h(t)=Wh(t−1)+Cx(t)h(t) = Wh(t-1) + Cx(t)h(t)=Wh(t−1)+Cx(t) h0(t)=WtCx(0)h_0(t) = W^tCx(0)h0​(t)=WtCx(0) For any input, for large the length of the hidden vector will expand or contract according to the t−t-t− th power of the largest eigen value of the hidden-layer weight matrix If ∣λmax>1∣|\\lambda_{max} > 1|∣λmax​>1∣ it will blow up, otherwise it will contract and shrink to 0 rapidly Non-linearities Sigmoid: Saturates in a limited number of steps, regardless of www To a value dependent only on www (and bias, if any) Rate of saturation depends on www Tanh: Sensitive to www, but eventually saturates “Prefers” weights close to 1.0 Relu: Sensitive to www, can blow up Lessons Recurrent networks retain information from the infinite past in principle In practice, they tend to blow up or forget If the largest Eigen value of the recurrent weights matrix is greater than 1, the network response may blow up If it’s less than one, the response dies down very quickly The “memory” of the network also depends on the parameters (and activation) of the hidden units Sigmoid activations saturate and the network becomes unable to retain new information RELU activations blow up or vanish rapidly Tanh activations are the most effective at storing memory And still has very short “memory” Still sensitive to Eigenvalues of WWW Vanishing gradient A particular problem with training deep networks is the gradient of the error with respect to weights is unstable For Div⁡(X)=D(fN(WN−1fN−1(WN−2fN−2(…W0X))))\\operatorname{Div}(X)=D\\left(f_{N}\\left(W_{N-1} f_{N-1}\\left(W_{N-2} f_{N-2}\\left(\\ldots W_{0} X\\right)\\right)\\right)\\right)Div(X)=D(fN​(WN−1​fN−1​(WN−2​fN−2​(…W0​X)))) We get ∇fkDiv⁡=∇D.∇fN.WN−1.∇fN−1.WN−2…∇fk+1Wk\\nabla_{f_{k}} \\operatorname{Div}=\\nabla D . \\nabla f_{N} . W_{N-1} . \\nabla f_{N-1} . W_{N-2} \\ldots \\nabla f_{k+1} W_{k}∇fk​​Div=∇D.∇fN​.WN−1​.∇fN−1​.WN−2​…∇fk+1​Wk​ Where ∇fn\\nabla{f_{n}}∇fn​ is jacobian of fN()f_N()fN​() to its current input For activation For RNN ∇ft(zi)=[ft,1′(z1)0⋯00ft,2′(z2)⋯0⋮⋮⋱⋮00⋯ft,N′(zN)]\\nabla f_{t}\\left(z_{i}\\right)=\\left[\\begin{array}{cccc}f_{t, 1}^{\\prime}\\left(z_{1}\\right) & 0 & \\cdots & 0 \\\\\\\\ 0 & f_{t, 2}^{\\prime}\\left(z_{2}\\right) & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & f_{t, N}^{\\prime}\\left(z_{N}\\right)\\end{array}\\right]∇ft​(zi​)=⎣⎢⎢⎢⎢⎢⎢⎢⎢⎡​ft,1′​(z1​)0⋮0​0ft,2′​(z2​)⋮0​⋯⋯⋱⋯​00⋮ft,N′​(zN​)​⎦⎥⎥⎥⎥⎥⎥⎥⎥⎤​ For vector activations: A full matrix For scalar activations: A matrix where the diagonal entries are the derivatives of the activation of the recurrent hidden layer The derivative (or subgradient) of the activation function is always bounded Most common activation functions, such as sigmoid, tanh() and RELU have derivatives that are always less than 1 Multiplication by the Jacobian is always a shrinking operation After a few layers the derivative of the divergence at any time is totally “forgotten” For weights In a single-layer RNN, the weight matrices are identical The conclusion below holds for any deep network, though The chain product for ∇fkDiv\\nabla_{f_k} Div∇fk​​Div will Expand ∇D\\nabla D∇D along directions in which the singular values of the weight matrices are greater than 1 Shrink ∇D\\nabla D∇D in directions where the singular values are less than 1 Repeated multiplication by the weights matrix will result in Exploding or vanishing gradients LSTM Problem Recurrent nets are very deep nets Stuff gets forgotten in the forward pass too Each weights matrix and activation can shrink components of the input Need the long-term dependency The memory retention of the network depends on the behavior of the weights and jacobian Which in turn depends on the parameters WWW rather than what it is trying to remember We need Not be directly dependent on vagaries of network parameters, but rather on input-based determination of whether it must be remembered Retain memories until a switch based on the input flags them as ok to forget 「Curly brace must remember until curly brace is closed」 LSTM Address the problem of input-dependent memory behavior Architecture The σ\\sigmaσ are multiplicative gates that decide if something is important or not Key component Remembered cell state Mutiply is a switch Should I continue remember or not? (scale up / down) Acddition Should I agument the memory? CtC_tCt​ is the linear history carried by the constant-error carousel Carries information through, only affected by a gate And addition of history, which too is gated.. Gates Gates are simple sigmoidal units with outputs in the range (0,1) Controls how much of the information is to be let through Forget gate The first gate determines whether to carry over the history or to forget it More precisely, how much of the history to carry over Also called the “forget” gate Note, we’re actually distinguishing between the cell memory CCC and the state hhh that is coming over time! They’re related though Hidden state is compute from memory (which is stored) Input gate The second input has two parts A perceptron layer that determines if there’s something new and interesting in the input 「See a curly brace」 A gate that decides if its worth remembering 「Curly brace is in comment section, ignore it」 Memory cell update If something new and worth remembering Added to the current memory cell Output and Output gate The output of the cell Simply compress it with tanh to make it lie between 1 and -1 Note that this compression no longer affects our ability to carry memory forward Controlled by an output gate To decide if the memory contents are worth reporting at this time The “Peephole” Connection The raw memory is informative by itself and can also be input Note, we’re using both CCC and hhh Forward Backward1 ∇CtDiv=∇htDiv∘(ot∘tanh⁡′(.)+tanh⁡(.)∘σ′(.)WCo)+∇Ct+1Div∘(ft+1+Ct∘σ′(.)WCf+C~t+1∘σ′(.)WCi∘tanh⁡(.)…) \\begin{array}{l} \\nabla_{C_{t}} D i v=&\\nabla_{h_{t}} D i v \\circ\\left(o_{t} \\circ \\tanh ^{\\prime}(.)+\\tanh (.) \\circ \\sigma^{\\prime}(.) W_{C o}\\right)+ \\\\\\\\ &\\nabla_{C_{t+1}} D i v \\circ\\left(f_{t+1}+C_{t} \\circ \\sigma^{\\prime}(.) W_{C f}+\\tilde{C}_{t+1} \\circ \\sigma^{\\prime}(.) W_{C i} \\circ \\tanh (.) \\ldots\\right) \\end{array} ∇Ct​​Div=​∇ht​​Div∘(ot​∘tanh′(.)+tanh(.)∘σ′(.)WCo​)+∇Ct+1​​Div∘(ft+1​+Ct​∘σ′(.)WCf​+C~t+1​∘σ′(.)WCi​∘tanh(.)…)​ ∇htDiv=∇ztDiv∇htzt+∇Ct+1Div∘(Ct∘σ′(.)Whf+C~t+1∘σ′(.)Whi)+∇Ct+1Div∘ot+1∘tanh⁡′(.)Whi+∇ht+1Div∘tanh⁡(.)∘σ′(.)Who \\begin{aligned} \\nabla_{h_{t}} D i v=& \\nabla_{z_{t}} D i v \\nabla_{h_{t}} z_{t}+\\nabla_{C_{t+1}} D i v \\circ\\left(C_{t} \\circ \\sigma^{\\prime}(.) W_{h f}+\\tilde{C}_{t+1} \\circ \\sigma^{\\prime}(.) W_{h i}\\right)+\\\\\\\\ &\\nabla_{C_{t+1}} D i v \\circ o_{t+1} \\circ \\tanh ^{\\prime}(.) W_{h i}+\\nabla_{h_{t+1}} D i v \\circ \\tanh (.) \\circ \\sigma^{\\prime}(.) W_{h o} \\end{aligned} ∇ht​​Div=​∇zt​​Div∇ht​​zt​+∇Ct+1​​Div∘(Ct​∘σ′(.)Whf​+C~t+1​∘σ′(.)Whi​)+∇Ct+1​​Div∘ot+1​∘tanh′(.)Whi​+∇ht+1​​Div∘tanh(.)∘σ′(.)Who​​ And weights? Gated Recurrent Units Combine forget and input gates In new input is to be remembered, then this means old memory is to be forgotten No need to compute twice Don’t bother to separately maintain compressed and regular memories Redundant representation Summary LSTMs are an alternative formalism where memory is made more directly dependent on the input, rather than network parameters/structure Through a “Constant Error Carousel” memory structure with no weights or activations, but instead direct switching and “increment/decrement” from pattern recognizers Do not suffer from a vanishing gradient problem but do suffer from exploding gradient issue 1. http://arunmallya.github.io/writeups/nn/lstm/index.html#/ ↩ "},"L15 Divergence of RNN.html":{"url":"L15 Divergence of RNN.html","title":"14 Divergence Of RNN","keywords":"","body":"Variants on recurrent nets Architectures How to train recurrent networks of different architectures Synchrony The target output is time-synchronous with the input The target output is order-synchronous, but not time synchronous One to one No recurrence in model Exactly as many outputs as inputs One to one correspondence between desired output and actual output Common assumption ∇Y(t)Div⁡(Ytarget(1…T),Y(1…T))=wt∇Y(t)Div⁡(Ytarget(t),Y(t)) \\nabla_{Y(t)} \\operatorname{Div}\\left(Y_{\\text {target}}(1 \\ldots T), Y(1 \\ldots T)\\right)=w_{t} \\nabla_{Y(t)} \\operatorname{Div}\\left(Y_{\\text {target}}(t), Y(t)\\right) ∇Y(t)​Div(Ytarget​(1…T),Y(1…T))=wt​∇Y(t)​Div(Ytarget​(t),Y(t)) wtw_twt​ is typically set to 1.0 Many to many The divergence computed is between the sequence of outputs by the network and the desired sequence of outputs This is not just the sum of the divergences at individual times Language modelling: Representing words Represent words as one-hot vectors Sparse problem Makes no assumptions about the relative importance of words The Projected word vectors Replace every one-hot vector WiW_iWi​ by PWiPW_iPWi​ PPP is an M×NM\\times NM×N matrix How to learn projections Soft bag of words Predict word based on words in immediate context Without considering specific position Skip-grams Predict adjacent words based on current word Many to one Example Question answering Input : Sequence of words Output: Answer at the end of the question Speech recognition Input : Sequence of feature vectors (e.g. Mel spectra) Output: Phoneme ID at the end of the sequence Outputs are actually produced for every input We only read it at the end of the sequence How to train Define the divergence everywhere DIV(Ytarget,Y)=∑twtXent⁡(Y(t), Phoneme)D I V\\left(Y_{\\text {target}}, Y\\right)=\\sum_{t} w_{t} \\operatorname{Xent}(Y(t), \\text { Phoneme})DIV(Ytarget​,Y)=∑t​wt​Xent(Y(t), Phoneme) Typical weighting scheme for speech All are equally important Problem like question answering Answer only expected after the question ends Sequence-to-sequence How do we know when to output symbols In fact, the network produces outputs at every time Which of these are the real outputs Outputs that represent the definitive occurrence of a symbol Option 1: Simply select the most probable symbol at each time Merge adjacent repeated symbols, and place the actual emission of the symbol in the final instant Cannot distinguish between an extended symbol and repetitions of the symbol Resulting sequence may be meaningless Option 2: Impose external constraints on what sequences are allowed Only allow sequences corresponding to dictionary words Sub-symbol units How to train when no timing information provided Only the sequence of output symbols is provided for the training data But no indication of which one occurs where How do we compute the divergence? And how do we compute its gradient "},"L16 Connectionist Temporal Classification.html":{"url":"L16 Connectionist Temporal Classification.html","title":"15 Connectionist Temporal Classification","keywords":"","body":"Sequence to sequence Sequence goes in, sequence comes out No notion of “time synchrony” between input and output May even nots maintain order of symbols (from one language to another) With order synchrony The input and output sequences happen in the same order Although they may be time asynchronous E.g. Speech recognition The input speech corresponds to the phoneme sequence output Question How do we know when to output symbols In fact, the network produces outputs at every time Which of these are the real outputs? Option 1 Simply select the most probable symbol at each time Merge adjacent repeated symbols, and place the actual emission of the symbol in the final instant Problem Cannot distinguish between an extended symbol and repetitions of the symbol Resulting sequence may be meaningless Option 2 Impose external constraints on what sequences are allowed E.g. only allow sequences corresponding to dictionary words Decoding The process of obtaining an output from the network Time-synchronous & order-synchronous sequence aaabbbbbbccc => abc (probility 0.5) aabbbbbbbccc => abc (probility 0.001) cccddddddeee => cde (probility 0.4) cccddddeeeee => cde (probility 0.4) So abc is the most likely time-synchronous output sequence But cde is the the most likely order-synchronous sequence Option 2 is in fact a suboptimal decode that actually finds the most likely time-synchronous output sequence The “merging” heuristics do not guarantee optimal order-synchronous sequences No timing information provided Only the sequence of output symbols is provided for the training data But no indication of which one occurs where Guess the alignment Initialize Assign an initial alignment Either randomly, based on some heuristic, or any other rationale Iterate Train the network using the current alignment Reestimate the alignment for each training instance Constraining the alignment Try 1 Block out all rows that do not include symbols from the target sequence E.g. Block out rows that are not /B/ /IY/ or /F/ Only decode on reduced grid We are now assured that only the appropriate symbols will be hypothesized But this still doesn’t assure that the decode sequence correctly expands the target symbol sequence Order variance Try 2 Explicitly arrange the constructed table Arrange the constructed table so that from top to bottom it has the exact sequence of symbols required If a symbol occurs multiple times, we repeat the row in the appropriate location Constrain that the first symbol in the decode must be the top left block The last symbol must be the bottom right The rest of the symbols must follow a sequence that monotonically travels down from top left to bottom right This guarantees that the sequence is an expansion of the target sequence Compose a graph such that every path in the graph from source to sink represents a valid alignment The “score” of a path is the product of the probabilities of all nodes along the path Find the most probable path from source to sink using any dynamic programming algorithm (viterbi algorithm) Viterbi algorithm Main idea The best path to any node must be an extension of the best path to one of its parent nodes Dynamically track the best path (and the score of the best path) from the source node to every node in the graph At each node, keep track of The best incoming parent edge (BP) The score of the best path from the source to the node through this best parent edge (Bscr) Process Algorithm Gradients DIV=∑tXent(Yt,symboltbestpath)=−∑tlog⁡Y(t,symboltbestpath) D I V=\\sum_{t} X e n t\\left(Y_{t}, s y m b o l_{t}^{b e s t p a t h}\\right)=-\\sum_{t} \\log Y\\left(t, s y m b o l_{t}^{b e s t p a t h}\\right) DIV=t∑​Xent(Yt​,symboltbestpath​)=−t∑​logY(t,symboltbestpath​) The gradient w.r.t the -th output vector YtY_tYt​ ∇YtDIV=[00⋅⋅⋅−1Y(t,symboltbestpath)0⋅⋅⋅0] \\nabla_{Y_{t}} D I V=[0 \\quad 0 \\cdot \\cdot \\cdot \\frac{-1}{Y(t, s y m b o l_{t}^{b e s t p a t h})} \\quad 0 \\cdot \\cdot \\cdot 0] ∇Yt​​DIV=[00⋅⋅⋅Y(t,symboltbestpath​)−1​0⋅⋅⋅0] Problem Approach heavily dependent on initial alignment Prone to poor local optima Because we commit to the single “best” estimated alignment This can be way off, particularly in early iterations, or if the model is poorly initialized Alternate view There is a probability distribution over alignments of the target Symbol sequence (to the input) Selecting a single alignment is the same as drawing a single sample from it Instead of only selecting the most likely alignment, use the statistical expectation over all possible alignments DIV=E[−∑tlog⁡Y(t,st)] D I V=E\\left[-\\sum_{t} \\log Y\\left(t, s_{t}\\right)\\right] DIV=E[−t∑​logY(t,st​)] Use the entire distribution of alignments This will mitigate the issue of suboptimal selection of alignment Using the linearity of expectation DIV=−∑tE[log⁡Y(t,st)] D I V=-\\sum_{t} E\\left[\\log Y\\left(t, s_{t}\\right)\\right] DIV=−t∑​E[logY(t,st​)] DIV=−∑t∑S∈S1…SKP(st=S∣S,X)log⁡Y(t,st=S) D I V=-\\sum_{t} \\sum_{S \\in S_{1} \\ldots S_{K}} P\\left(s_{t}=S | \\mathbf{S}, \\mathbf{X}\\right) \\log Y\\left(t, s_{t}=S\\right) DIV=−t∑​S∈S1​…SK​∑​P(st​=S∣S,X)logY(t,st​=S) A posteriori probabilities of symbols P(st=S∣S,X)P(s_{t}=S | \\mathbf{S}, \\mathbf{X})P(st​=S∣S,X) is the probability of seeing the specific symbol sss at time ttt, given that the symbol sequence is an expansion of S=s0,...,SK−1S = s_0,...,S_{K-1}S=s0​,...,SK−1​ and given the input sequence X=Xo,...,XN−1X = X_o,...,X_{N-1}X=Xo​,...,XN−1​ P(st=Sr∣S,X)∝P(st=Sr,S∣X)P\\left(s_{t}=S_{r} | \\mathbf{S}, \\mathbf{X}\\right) \\propto P\\left(s_{t}=S_{r}, \\mathbf{S} | \\mathbf{X}\\right)P(st​=Sr​∣S,X)∝P(st​=Sr​,S∣X) P(st=Sr,S∣X)P\\left(s_{t}=S_{r}, \\mathbf{S} | \\mathbf{X}\\right)P(st​=Sr​,S∣X) is the total probability of all valid paths in the graph for target sequence SSS that go through the symbol SrS_rSr​ (the rthr^{th}rth symbol in the sequence S0,...,SK−1S_0,...,S_{K-1}S0​,...,SK−1​ ) at time For a recurrent network without feedback from the output we can make the conditional independence assumption P(st=Sr,S∣X)=P(S0…Sr,st=Sr∣X)P(st+1∈succ⁡(Sr),succ⁡(Sr),…,SK−1∣X) P\\left(s_{t}=S_{r}, \\mathbf{S} | \\mathbf{X}\\right)=P\\left(S_{0} \\ldots S_{r}, s_{t}=S_{r} | \\mathbf{X}\\right) P\\left(s_{t+1} \\in \\operatorname{succ}\\left(S_{r}\\right), \\operatorname{succ}\\left(S_{r}\\right), \\ldots, S_{K-1} | \\mathbf{X}\\right) P(st​=Sr​,S∣X)=P(S0​…Sr​,st​=Sr​∣X)P(st+1​∈succ(Sr​),succ(Sr​),…,SK−1​∣X) We will call the first term the forward probability α(t,r)\\alpha(t,r)α(t,r) We will call the second term the backward probability β(t,r)\\beta(t,r)β(t,r) Forward algorithm In practice the recursion will gererally underflow α(t,l)=(α(t−1,l)+α(t−1,l−1))ytS(l) \\alpha(t, l)=(\\alpha(t-1, l)+\\alpha(t-1, l-1)) y_{t}^{S(l)} α(t,l)=(α(t−1,l)+α(t−1,l−1))ytS(l)​ Instead we can do it in the log domain log⁡α(t,l)=log⁡(elog⁡α(t−1,l)+elog⁡α(t−1,l−1))+log⁡ytS(l) \\log \\alpha(t, l)=\\log \\left(e^{\\log \\alpha(t-1, l)}+e^{\\log \\alpha(t-1, l-1)}\\right)+\\log y_{t}^{S(l)} logα(t,l)=log(elogα(t−1,l)+elogα(t−1,l−1))+logytS(l)​ Backward algorithm β(t,r)=P(st+1∈succ⁡(Sr),succ⁡(Sr),…,SK−1∣X) \\beta(t, r)=P\\left(s_{t+1} \\in \\operatorname{succ}\\left(S_{r}\\right), \\operatorname{succ}\\left(S_{r}\\right), \\ldots, S_{K-1} | \\mathbf{X}\\right) β(t,r)=P(st+1​∈succ(Sr​),succ(Sr​),…,SK−1​∣X) The joint probability P(st=Sr,S∣X)=α(t,r)β(t,r) P\\left(s_{t}=S_{r}, \\mathbf{S} | \\mathbf{X}\\right)=\\alpha(t, r) \\beta(t, r) P(st​=Sr​,S∣X)=α(t,r)β(t,r) Need to be normalized, get posterior probability γ(t,r)=P(st=Sr∣S,X)=P(st=Sr,S∣X)∑Sr′P(st=Sr′,S∣X)=α(t,r)β(t,r)∑r′α(t,r′)β(t,r′) \\gamma(t,r) = P\\left(s_{t}=S_{r} | \\mathbf{S}, \\mathbf{X}\\right)=\\frac{P\\left(s_{t}=S_{r}, \\mathbf{S} | \\mathbf{X}\\right)}{\\sum_{S_{r}^{\\prime}} P\\left(s_{t}=S_{r}^{\\prime}, \\mathbf{S} | \\mathbf{X}\\right)}=\\frac{\\alpha(t, r) \\beta(t, r)}{\\sum_{r^{\\prime}} \\alpha\\left(t, r^{\\prime}\\right) \\beta\\left(t, r^{\\prime}\\right)} γ(t,r)=P(st​=Sr​∣S,X)=∑Sr′​​P(st​=Sr′​,S∣X)P(st​=Sr​,S∣X)​=∑r′​α(t,r′)β(t,r′)α(t,r)β(t,r)​ We can also write this using the modified beta formula as (you will see this in papers) γ(t,r)=1ytS(r)α(t,r)β^(t,r)∑r′1ytS(r)α(t,r)β^(t,r) \\gamma(t, r)=\\frac{\\frac{1}{y_{t}^{S(r)}} \\alpha(t, r) \\hat{\\beta}(t, r)}{\\sum_{r^{\\prime}} \\frac{1}{y_{t}^{S(r)}} \\alpha(t, r) \\hat{\\beta}(t, r)} γ(t,r)=∑r′​ytS(r)​1​α(t,r)β^​(t,r)ytS(r)​1​α(t,r)β^​(t,r)​ The expected divergence DIV=−∑t∑s∈S0…SK−1P(st=s∣S,X)log⁡Y(t,st=s) D I V=-\\sum_{t} \\sum_{s \\in S_{0} \\ldots S_{K-1}} P\\left(s_{t}=s | \\mathbf{S}, \\mathbf{X}\\right) \\log Y\\left(t, s_{t}=s\\right) DIV=−t∑​s∈S0​…SK−1​∑​P(st​=s∣S,X)logY(t,st​=s) DIV=−∑t∑rγ(t,r)log⁡ytS(r) D I V=-\\sum_{t} \\sum_{r} \\gamma(t, r) \\log y_{t}^{S(r)} DIV=−t∑​r∑​γ(t,r)logytS(r)​ The derivative of the divergence w.r.t the output YtY_tYt​ of the net at any time ∇YtDIV=[dDIVdytS0dDIVdytS1⋯dDIVdytSL−1] \\nabla_{Y_{t}} D I V=\\left[\\begin{array}{llll} \\frac{d D I V}{d y_{t}^{S_{0}}} & \\frac{d D I V}{d y_{t}^{S_{1}}} & \\cdots & \\frac{d D I V}{d y_{t}^{S_{L-1}}} \\end{array}\\right] ∇Yt​​DIV=[dytS0​​dDIV​​dytS1​​dDIV​​⋯​dytSL−1​​dDIV​​] Compare to Viterbi algorithm, components will be non-zero only for symbols that occur in the training instance Compute derivative dDIVdytl=−∑r:S(r)=lddytS(r)γ(t,r)log⁡ytS(r) \\frac{d D I V}{d y_{t}^{l}}=-\\sum_{r: S(r)=l} \\frac{d}{d y_{t}^{S(r)}} \\gamma(t, r) \\log y_{t}^{S(r)} dytl​dDIV​=−r:S(r)=l∑​dytS(r)​d​γ(t,r)logytS(r)​ ddytS(r)γ(t,r)log⁡ytS(r)=γ(t,r)ytS(r)+dγ(t,r)dytS(r)log⁡ytS(r) \\frac{d}{d y_{t}^{S(r)}} \\gamma(t, r) \\log y_{t}^{S(r)}=\\frac{\\gamma(t, r)}{y_{t}^{S(r)}}+\\frac{d \\gamma(t, r)}{d y_{t}^{S(r)}} \\log y_{t}^{S(r)} dytS(r)​d​γ(t,r)logytS(r)​=ytS(r)​γ(t,r)​+dytS(r)​dγ(t,r)​logytS(r)​ Normally we ignore the second term, and think of as a maximum-likelihood estimate And the derivatives at both these locations must be summed if occurs repetition dDIVdytl=−∑r:S(r)=lγ(t,r)ytS(r) \\frac{d D I V}{d y_{t}^{l}}=-\\sum_{r: S(r)=l} \\frac{\\gamma(t, r)}{y_{t}^{S(r)}} dytl​dDIV​=−r:S(r)=l∑​ytS(r)​γ(t,r)​ Repetitive decoding problem Cannot distinguish between an extended symbol and repetitions of the symbol We have a decode: R R R O O O O O D Is this the symbol sequence ROD or ROOD? Solution: Introduce an explicit extra symbol which serves to separate discrete versions of a symbol A “blank” (represented by “-”) RRR---OO---DDD = ROD RR-R---OO---D-DD = RRODD Modified forward algorithm A blank is mandatory between repetitions of a symbol but not required between distinct symbols Skips are permitted across a blank, but only if the symbols on either side are different Modified backward algorithm Overall training procedure Setup the network Typically many-layered LSTM Initialize all parameters of the network Include a 「blank」 symbol in vocabulary Foreach training instance Pass the training instance through the network and obtain all symbol probabilities at each time Construct the graph representing the specific symbol sequence in the instance Backword pass Perform the forward backward algorithm to compute α(t,r)\\alpha(t,r)α(t,r) and β(t,r)\\beta(t,r)β(t,r) Compute derivative of divergence ∇YtDIV\\nabla_{Y_{t}} D I V∇Yt​​DIV for each YtY_tYt​ ∇YtDIV=[dDIVdyt0dDIVdyt1⋯dDIVdytL−1]\\nabla_{Y_{t}} D I V=\\left[\\begin{array}{llll}\\frac{d D I V}{d y_{t}^{0}} & \\frac{d D I V}{d y_{t}^{1}} & \\cdots & \\frac{d D I V}{d y_{t}^{L-1}}\\end{array}\\right]∇Yt​​DIV=[dyt0​dDIV​​dyt1​dDIV​​⋯​dytL−1​dDIV​​] dDIVdytl=−∑r:S(r)=lγ(t,r)ytS(r)\\frac{d D I V}{d y_{t}^{l}}=-\\sum_{r: S(r)=l} \\frac{\\gamma(t, r)}{y_{t}^{S(r)}}dytl​dDIV​=−r:S(r)=l∑​ytS(r)​γ(t,r)​ Aggregate derivatives over minibatch and update parameters CTC: Connectionist Temporal Classification The overall framework is referred to as CTC This is in fact a suboptimal decode that actually finds the most likely time-synchronous output sequence Actual decoding objective S^=argmax⁡sαS(SK−1,T−1)\\hat{\\mathbf{S}}=\\underset{\\mathbf{s}}{\\operatorname{argmax}} \\alpha_{\\mathbf{S}}\\left(S_{K-1}, T-1\\right)S^=sargmax​αS​(SK−1​,T−1) Explicit computation of this will require evaluate of an exponential number of symbol sequences Beam search Only used in test Uses breadth-first search to build its search tree Choose top k words for next prediction (prone) Reference "},"L17 Seq2seq and attention model.html":{"url":"L17 Seq2seq and attention model.html","title":"16 Seq 2 Seq And Attention Model","keywords":"","body":"Generating Language Synthesis Input: symbols as one-hot vectors Dimensionality of the vector is the size of the 「vocabulary」 Projected down to lower-dimensional “embeddings” The hidden units are (one or more layers of) LSTM units Output at each time: A probability distribution that ideally assigns peak probability to the next word in the sequence Divergence Div⁡(Ytarget(1…T),Y(1…T))=∑_tXent⁡(Y_target(t),Y(t))=−∑_tlog⁡Y(t,w_t+1) \\operatorname{Div}(\\mathbf{Y}_{\\text {target}}(1 \\ldots T), \\mathbf{Y}(1 \\ldots T))=\\sum\\_{t}\\operatorname{Xent}(\\mathbf{Y}\\_{\\text {target}}(t), \\mathbf{Y}(t))=-\\sum\\_{t} \\log Y(t, w\\_{t+1}) Div(Ytarget​(1…T),Y(1…T))=∑_tXent(Y_target(t),Y(t))=−∑_tlogY(t,w_t+1) Feed the drawn word as the next word in the series And draw the next word from the output probability distribution Beginnings and ends A sequence of words by itself does not indicate if it is a complete sentence or not To make it explicit, we will add two additional symbols (in addition to the words) to the base vocabulary : Indicates start of a sentence : Indicates end of a sentence When do we stop? Continue this process until we draw an Or we decide to terminate generation based on some other criterion Delayed sequence to sequence Pseudocode Problem: Each word that is output depends only on current hidden state, and not on previous outputs The input sequence feeds into a recurrent structure The input sequence is terminated by an explicit symbol The hidden activation at the “stores” all information about the sentence Subsequently a second RNN uses the hidden activation as initial state to produce a sequence of outputs The output at each time becomes the input at the next time Output production continues until an is produced Autoencoder The recurrent structure that extracts the hidden representation from the input sequence is the encoder The recurrent structure that utilizes this representation to produce the output sequence is the decoder Generating output At each time the network produces a probability distribution over words, given the entire input and previous outputs At each time a word is drawn from the output distribution P(O1,…,OL∣W1in,…,WNin)=y1O1y1O2…y1OL P\\left(O_{1}, \\ldots, O_{L} \\mid W_{1}^{i n}, \\ldots, W_{N}^{i n}\\right)=y_{1}^{O_{1}} y_{1}^{O_{2}} \\ldots y_{1}^{O_{L}} P(O1​,…,OL​∣W1in​,…,WNin​)=y1O1​​y1O2​​…y1OL​​ The objective of drawing: Produce the most likely output (that ends in an ) argmax⁡O1,…,OLy1O1y1O2…y1OL \\underset{O_{1}, \\ldots, O_{L}}{\\operatorname{argmax}} y_{1}^{O_{1}} y_{1}^{O_{2}} \\ldots y_{1}^{O_{L}} O1​,…,OL​argmax​y1O1​​y1O2​​…y1OL​​ How to draw words? Greedy answer Select the most probable word at each time Not good, making a poor choice at any time commits us to a poor future Randomly draw a word at each time according to the output probability distribution Not guaranteed to give you the most likely output Beam search Search multiple choices and prune At each time, retain only the top K scoring forks Terminate: When the current most likely path overall ends in Train In practice, if we apply SGD, we may randomly sample words from the output to actually use for the backprop and update Randomly select training instance: (input, output) Forward pass Randomly select a single output y(t)y(t)y(t) and corresponding desired output d(t)d(t)d(t) for backprop Trick The input sequence is fed in reverse order This happens both for training and during actual decode Problem All the information about the input sequence is embedded into a single vector In reality: All hidden values carry information Attention model Compute a weighted combination of all the hidden outputs into a single vector Weights vary by output time Require a time-varying weight that specifies relationship of output time to input time Weights are functions of current output state ei(t)=g(hi,st−1) e_{i}(t)=g\\left(\\boldsymbol{h}_{i}, \\boldsymbol{s}_{t-1}\\right) ei​(t)=g(hi​,st−1​) wi(t)=exp⁡(ei(t))∑jexp⁡(ej(t)) w_{i}(t)=\\frac{\\exp \\left(e_{i}(t)\\right)}{\\sum_{j} \\exp \\left(e_{j}(t)\\right)} wi​(t)=∑j​exp(ej​(t))exp(ei​(t))​ Attention weight Typical option for g()g()g() Inner product g(h_i,s_t−1)=h_iTs_t−1g\\left(\\boldsymbol{h}\\_{i}, \\boldsymbol{s}\\_{t-1}\\right)=\\boldsymbol{h}\\_{i}^{T} \\boldsymbol{s}\\_{t-1}g(h_i,s_t−1)=h_iTs_t−1 Project to the same demension g(hi,s_t−1)=h_iTW_gs_t−1g\\left(\\boldsymbol{h}_{i}, \\boldsymbol{s}\\_{t-1}\\right)=\\boldsymbol{h}\\_{i}^{T} \\boldsymbol{W}\\_{g} \\boldsymbol{s}\\_{t-1}g(hi​,s_t−1)=h_iTW_gs_t−1 Non-linear activation g(h_i,s_t−1)=v_gTtanh(W_g[his_t−1])g\\left(\\boldsymbol{h}\\_{i}, \\boldsymbol{s}\\_{t-1}\\right)=v\\_{g}^{T} \\boldsymbol{t} \\boldsymbol{a} \\boldsymbol{n} \\boldsymbol{h}\\left(\\boldsymbol{W}\\_{g}\\left[\\begin{array}{c}\\boldsymbol{h}_{i} \\\\\\\\ \\boldsymbol{s}\\_{t-1}\\end{array}\\right]\\right)g(h_i,s_t−1)=v_gTtanh⎝⎛​W_g⎣⎡​hi​s_t−1​⎦⎤​⎠⎞​ MLP g(h_i,s_t−1)=MLP⁡([h_i,s_t−1])g\\left(\\boldsymbol{h}\\_{i}, \\boldsymbol{s}\\_{t-1}\\right)=\\operatorname{MLP}\\left(\\left[\\boldsymbol{h}\\_{i}, \\boldsymbol{s}\\_{t-1}\\right]\\right)g(h_i,s_t−1)=MLP([h_i,s_t−1]) Pseudocode Train Back propagation also updates parameters of the “attention” function Trick: Occasionally pass drawn output instead of ground truth, as input Randomly select from output, force network to produce correct word even the prioir word is not correct variants Bidirectional processing of input sequence Neural Machine Translation by Jointly Learning to Align and Translate Local attention vs global attention Effective Approaches to Attention-based Neural Machine Translation Multihead attention Derive 「value」, and multiple 「keys」 from the encoder Vi,Kil,i=1…T,l=1…NheadV_{i}, K_{i}^{l}, i=1 \\ldots T, l=1 \\ldots N_{\\text {head}}Vi​,Kil​,i=1…T,l=1…Nhead​ Derive one or more 「queries」 from decoder Qjl,j=1…M,l=1…NheadQ_{j}^{l}, j=1 \\ldots M, l=1 \\ldots N_{\\text {head}}Qjl​,j=1…M,l=1…Nhead​ Each query-key pair gives you one attention distribution And one context vector aj,il=a_{j, i}^{l}=aj,il​=attention(Qjl,Kil,i=1…T),Cjl=∑iaj,ilVi\\left(Q_{j}^{l}, K_{i}^{l}, i=1 \\ldots T\\right), \\quad C_{j}^{l}=\\sum_{i} a_{j, i}^{l} V_{i}(Qjl​,Kil​,i=1…T),Cjl​=∑i​aj,il​Vi​ Concatenate set of context vectors into one extended context vector Cj=[Cj1Cj2…CjNhead]C_{j}=\\left[C_{j}^{1} C_{j}^{2} \\ldots C_{j}^{N_{\\text {head}}}\\right]Cj​=[Cj1​Cj2​…CjNhead​​] Each 「attender」 focuses on a different aspect of the input that’s important for the decode "},"L18 Representation.html":{"url":"L18 Representation.html","title":"17 Representation","keywords":"","body":"Logistic regression This the perceptron with a sigmoid activation It actually computes the probability that the input belongs to class 1 Decision boundaries may be obtained by comparing the probability to a threshold These boundaries will be lines (hyperplanes in higher dimensions) The sigmoid perceptron is a linear classifier Estimating the model Given: Training data: (X1,y1),(X2,y2),…,(XN,yN)\\left(X_{1}, y_{1}\\right),\\left(X_{2}, y_{2}\\right), \\ldots,\\left(X_{N}, y_{N}\\right)(X1​,y1​),(X2​,y2​),…,(XN​,yN​) XXX are vectors, yyy are binary (0/1) class values Total probability of data P((X1,y1),(X2,y2),…,(XN,yN))=∏iP(Xi,yi)=∏iP(yi∣Xi)P(Xi)=∏i11+e−yi(w0+wTXi)P(Xi) \\begin{array}{l} P\\left(\\left(X_{1}, y_{1}\\right),\\left(X_{2}, y_{2}\\right), \\ldots,\\left(X_{N}, y_{N}\\right)\\right)= \\prod_{i} P\\left(X_{i}, y_{i}\\right) \\\\\\\\ =\\prod_{i} P\\left(y_{i} \\mid X_{i}\\right) P\\left(X_{i}\\right)=\\prod_{i} \\frac{1}{1+e^{-y_{i}\\left(w_{0}+w^{T} X_{i}\\right)}} P\\left(X_{i}\\right) \\end{array} P((X1​,y1​),(X2​,y2​),…,(XN​,yN​))=∏i​P(Xi​,yi​)=∏i​P(yi​∣Xi​)P(Xi​)=∏i​1+e−yi​(w0​+wTXi​)1​P(Xi​)​ Likelihood P(Training data)=∏i11+e−yi(w0+wTXi)P(Xi) P(\\text {Training data})=\\prod_{i} \\frac{1}{1+e^{-y_{i}\\left(w_{0}+w^{T} X_{i}\\right)}} P\\left(X_{i}\\right) P(Training data)=i∏​1+e−yi​(w0​+wTXi​)1​P(Xi​) Log likelihood log⁡P(Training data)=∑ilog⁡P(Xi)−∑ilog⁡(1+e−yi(w0+wTXi)) \\begin{array}{l} \\log P(\\text {Training data})= \\sum_{i} \\log P\\left(X_{i}\\right)-\\sum_{i} \\log \\left(1+e^{-y_{i}\\left(w_{0}+w^{T} X_{i}\\right)}\\right) \\end{array} logP(Training data)=∑i​logP(Xi​)−∑i​log(1+e−yi​(w0​+wTXi​))​ Maximum Likelihood Estimate w0,w1=argmax⁡w0,w1log⁡P(Training data) w_{0}, w_{1}=\\underset{w_{0}, w_{1}}{\\operatorname{argmax}} \\log P(\\text {Training data}) w0​,w1​=w0​,w1​argmax​logP(Training data) Equals (note argmin rather than argmax) w0,w1=argmin⁡w0,w∑ilog⁡(1+e−yi(w0+wTXi)) w_{0}, w_{1}=\\underset{w_{0}, w}{\\operatorname{argmin}} \\sum_{i} \\log \\left(1+e^{-y_{i}\\left(w_{0}+w^{T} X_{i}\\right)}\\right) w0​,w1​=w0​,wargmin​i∑​log(1+e−yi​(w0​+wTXi​)) Identical to minimizing the KL divergence between the desired output and actual output 11+e−(w0+wTXi)\\frac{1}{1+e^{-\\left(w_{0}+w^{T} X_{i}\\right)}}1+e−(w0​+wTXi​)1​ MLP Separable case The rest of the network may be viewed as a transformation that transforms data from non-linear classes to linearly separable features We can now attach any linear classifier above it for perfect classification Need not be a perceptron Could even train an SVM on top of the features! For insufficient structures, the network may attempt to transform the inputs to linearly separable features Will fail to separate exactly, but will try to minimize error The network until the second-to-last layer is a non-linear function f(X)f(X)f(X) that converts the input space XXX of into the feature space where the classes are maximally linearly separable Lower layers Manifold hypothesis: For separable classes, the classes are linearly separable on a non-linear manifold Layers sequentially “straighten” the data manifold The “feature extraction” layer transforms the data such that the posterior probability may now be modelled by a logistic Weight as a template In high dimensional space, all vectors are more or less the same length Which means all xxx are in this surface of sphere The perceptron fires if the input is within a specified angle of the weight Represents a convex region on the surface of the sphere! The network is a Boolean function over these regions Neuron fires if the input vector is close enough to the weight vector If the input pattern matches the weight pattern closely enough The perceptron is a correlation filter! Autoencoder The lowest layers of a network detect significant features in the signal The signal could be (partially) reconstructed using these features Will retain all the significant components of the signal Simplest autoencoder This is just PCA! The autoencoder finds the direction of maximum energy Simply varying the hidden representation will result in an output that lies along the major axis Terminology Encoder The “Analysis” net which computes the hidden representation Decoder The “Synthesis” which recomposes the data from the hidden representation Nonlinearity When the hidden layer has a linear activation the decoder represents the best linear manifold to fit the data Varying the hidden value will move along this linear manifold When the hidden layer has non-linear activation, the net performs nonlinear PCA The decoder represents the best non-linear manifold to fit the data Varying the hidden value will move along this non-linear manifold The model is specific to the training data Varying the hidden layer value only generates data along the learned manifold Any input will result in an output along the learned manifold But may not generalize beyond the manifold Input unseen data may behave beyond intuitive manner, no constrain! The decoder can only generate data on the manifold that the training data lie on This also makes it an excellent “generator” of the distribution of the training data Dictionary-based techniques The decoder represents a source-specific generative dictionary Exciting it will produce typical data from the source! Signal separation Separation: Identify the combination of entries from both dictionaries that compose the mixed signal Given mixed signal and source dictionaries, find excitation that best recreates mixed signal Simple backpropagation Intermediate results are separated signals "},"L19 Hopfield network.html":{"url":"L19 Hopfield network.html","title":"18 Hopfield Network","keywords":"","body":"Hopfield Net So far, neural networks for computation are all feedforward structures Loopy network Each neuron is a perceptron with +1/-1 output Every neuron receives input from every other neuron Every neuron outputs signals to every other neuron At each time each neuron receives a “field” ∑j≠iwjiyj+bi\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}∑j≠i​wji​yj​+bi​ If the sign of the field matches its own sign, it does not respond If the sign of the field opposes its own sign, it “flips” to match the sign of the field If the sign of the field at any neuron opposes its own sign, it “flips” to match the field Which will change the field at other nodes Which may then flip... and so on... Filp behavior Let yi−y^{-}_{i}yi−​ be the output of the iii-th neuron just before it responds to the current field Let yi+y_{i}^{+}yi+​ be the output of the iii-th neuron just after it responds to the current field if yi−=sign⁡(∑j≠iwjiyj+bi)y_{i}^{-}=\\operatorname{sign}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right)yi−​=sign(∑j≠i​wji​yj​+bi​), then yi+=−yi−y_{i}^{+} = -y_{i}^{-}yi+​=−yi−​ If the sign of the field matches its own sign, it does not flip yi+(∑j≠iwjiyj+bi)−yi−(∑j≠iwjiyj+bi)=0 y_{i}^{+}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right)-y_{i}^{-}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right)=0 yi+​⎝⎛​j≠i∑​wji​yj​+bi​⎠⎞​−yi−​⎝⎛​j≠i∑​wji​yj​+bi​⎠⎞​=0 if yi−≠sign⁡(∑j≠iwjiyj+bi)y_{i}^{-}\\neq\\operatorname{sign}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right)yi−​≠sign(∑j≠i​wji​yj​+bi​), then yi+=−yi−y_{i}^{+} = -y_{i}^{-}yi+​=−yi−​ yi+(∑j≠iwjiyj+bi)−yi−(∑j≠iwjiyj+bi)=2yi+(∑j≠iwjiyj+bi) y_{i}^{+}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right)-y_{i}^{-}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right)=2 y_{i}^{+}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right) yi+​⎝⎛​j≠i∑​wji​yj​+bi​⎠⎞​−yi−​⎝⎛​j≠i∑​wji​yj​+bi​⎠⎞​=2yi+​⎝⎛​j≠i∑​wji​yj​+bi​⎠⎞​ This term is always positive! Every flip of a neuron is guaranteed to locally increase yi(∑j≠iwjiyj+bi)y_{i}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right)yi​(∑j≠i​wji​yj​+bi​) Globally Consider the following sum across all nodes D(y1,y2,…,yN)=∑iyi(∑j≠iwjiyj+bi)=∑i,j≠iwijyiyj+∑ibiyi \\begin{array}{c} D\\left(y_{1}, y_{2}, \\ldots, y_{N}\\right)=\\sum_{i} y_{i}\\left(\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}\\right) \\\\\\\\ =\\sum_{i, j \\neq i} w_{i j} y_{i} y_{j}+\\sum_{i} b_{i} y_{i} \\end{array} D(y1​,y2​,…,yN​)=∑i​yi​(∑j≠i​wji​yj​+bi​)=∑i,j≠i​wij​yi​yj​+∑i​bi​yi​​ Assume wii=0w_{ii} = 0wii​=0 For any unit kkk that “flips” because of the local field ΔD(yk)=D(y1,…,yk+,…,yN)−D(y1,…,yk−,…,yN) \\Delta D\\left(y_{k}\\right)=D\\left(y_{1}, \\ldots, y_{k}^{+}, \\ldots, y_{N}\\right)-D\\left(y_{1}, \\ldots, y_{k}^{-}, \\ldots, y_{N}\\right) ΔD(yk​)=D(y1​,…,yk+​,…,yN​)−D(y1​,…,yk−​,…,yN​) ΔD(yk)=(yk+−yk−)(∑j≠kwjkyj+bk) \\Delta D\\left(y_{k}\\right)=\\left(y_{k}^{+}-y_{k}^{-}\\right)\\left(\\sum_{j \\neq k} w_{j k} y_{j}+b_{k}\\right) ΔD(yk​)=(yk+​−yk−​)⎝⎛​j≠k∑​wjk​yj​+bk​⎠⎞​ This is always positive! Every flip of a unit results in an increase in DDD Overall Flipping a unit will result in an increase (non-decrease) of D=∑i,j≠iwijyiyj+∑ibiyi D=\\sum_{i, j \\neq i} w_{i j} y_{i} y_{j}+\\sum_{i} b_{i} y_{i} D=i,j≠i∑​wij​yi​yj​+i∑​bi​yi​ DDD is bounded Dmax⁡=∑i,j≠i∣wij∣+∑i∣bi∣ D_{\\max }=\\sum_{i, j \\neq i}\\left|w_{i j}\\right|+\\sum_{i}\\left|b_{i}\\right| Dmax​=i,j≠i∑​∣wij​∣+i∑​∣bi​∣ The minimum increment of DDD in a flip is ΔDmin⁡=min⁡i,{yi,i=1.…N}2∣∑j≠iwjiyj+bi∣ \\Delta D_{\\min }=\\min _{i,\\{y_{i}, i=1 . \\ldots N\\}} 2|\\sum_{j \\neq i} w_{j i} y_{j}+b_{i}| ΔDmin​=i,{yi​,i=1.…N}min​2∣j≠i∑​wji​yj​+bi​∣ Any sequence of flips must converge in a finite number of steps Think of this as an infinite deep network where every weights at every layers are identical Find the maximum layer! The Energy of a Hopfield Net Define the Energy of the network as E=−∑i,j≠iwijyiyj−∑ibiyi E=-\\sum_{i, j \\neq i} w_{i j} y_{i} y_{j}-\\sum_{i} b_{i} y_{i} E=−i,j≠i∑​wij​yi​yj​−i∑​bi​yi​ Just the negative of DDD The evolution of a Hopfield network constantly decreases its energy This is analogous to the potential energy of a spin glass(Magnetic diploes) The system will evolve until the energy hits a local minimum We remove bias for better understanding The network will evolve until it arrives at a local minimum in the energy contour Content-addressable memory Each of the minima is a “stored” pattern If the network is initialized close to a stored pattern, it will inevitably evolve to the pattern This is a content addressable memory Recall memory content from partial or corrupt values Also called associative memory Evolve and recall pattern by content, not by location Evolution The network will evolve until it arrives at a local minimum in the energy contour We proved that every change in the network will result in decrease in energy So path to energy minimum is monotonic For 2-neuron net Symmetric −12yTWy=−12(−y)TW(−y)-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W} \\mathbf{y}=-\\frac{1}{2}(-\\mathbf{y})^{T} \\mathbf{W}(-\\mathbf{y})−21​yTWy=−21​(−y)TW(−y) If y^\\hat{y}y^​ is a local minimum, so is −y^-\\hat{y}−y^​ Computational algorithm Very simple Updates can be done sequentially, or all at once Convergence when it deos not chage significantly any more E=−∑i∑j>iwjiyjyi E=-\\sum_{i} \\sum_{j>i} w_{j i} y_{j} y_{i} E=−i∑​j>i∑​wji​yj​yi​ Issues Store a specific pattern A network can store multiple patterns Every stable point is a stored pattern So we could design the net to store multiple patterns Remember that every stored pattern PPP is actually two stored patterns, PPP and −P-P−P How could the quadrtic function have multiple minimum? (Convex function) Input has constrain (belong to (−1,1)(-1,1)(−1,1) ) Hebbian learning: wji=yjyiw_{j i}=y_{j} y_{i}wji​=yj​yi​ Design a stationary pattern sign⁡(∑j≠iwjiyj)=yi∀i\\operatorname{sign}\\left(\\sum_{j \\neq i} w_{j i} y_{j}\\right)=y_{i} \\quad \\forall isign(∑j≠i​wji​yj​)=yi​∀i So sign⁡(∑j≠iwjiyj)=sign⁡(∑j≠iyjyiyj)\\operatorname{sign}\\left(\\sum_{j \\neq i} w_{j i} y_{j}\\right)=\\operatorname{sign}\\left(\\sum_{j \\neq i} y_{j} y_{i} y_{j}\\right)sign(∑j≠i​wji​yj​)=sign(∑j≠i​yj​yi​yj​) =sign⁡(∑j≠iyj2yi)=sign⁡(yi)=yi\\quad=\\operatorname{sign}\\left(\\sum_{j \\neq i} y_{j}^{2} y_{i}\\right)=\\operatorname{sign}\\left(y_{i}\\right)=y_{i}=sign(∑j≠i​yj2​yi​)=sign(yi​)=yi​ Energy E=−∑i∑jiwjiyjyi=−∑i∑jiyi2yj2=−∑i∑ji1=−0.5N(N−1)\\begin{aligned} E=&-\\sum_{i} \\sum_{jE=​−i∑​ji∑​wji​yj​yi​=−i∑​ji∑​yi2​yj2​=−i∑​ji∑​1=−0.5N(N−1)​ This is the lowest possible energy value for the network Stored pattern has lowest energy No matter where it begin, it will evolve into yellow pattern(lowest energy) How many patterns can we store? To store more than one pattern wji=∑yp∈{yp}yipyjp w_{j i}=\\sum_{\\mathbf{y}_{p} \\in\\left\\{\\mathbf{y}_{p}\\right\\}} y_{i}^{p} y_{j}^{p} wji​=yp​∈{yp​}∑​yip​yjp​ {yP}\\{y_P\\}{yP​} is the set of patterns to store Super/subscript ppp represents the specific pattern Hopfield: For a network of neurons can store up to ~0.15N0.15N0.15N patterns through Hebbian learning(Provided in PPT) Orthogonal/ Non-orthogonal patterns Orthogonal patterns Patterns are local minima (stationary and stable) No other local minima exist But patterns perfectly confusable for recall Non-orthogonal patterns Patterns are local minima (stationary and stable) No other local minima exist Actual wells for patterns Patterns may be perfectly recalled! (Note K > 0.14 N) Two orthogonal 6-bit patterns Perfectly stationary and stable Several spurious “fake-memory” local minima.. Observations Many “parasitic” patterns Undesired patterns that also become stable or attractors Patterns that are non-orthogonal easier to remember I.e. patterns that are closer are easier to remember than patterns that are farther!! Seems possible to store K > 0.14N patterns i.e. obtain a weight matrix W such that K > 0.14N patterns are stationary Possible to make more than 0.14N patterns at-least 1-bit stable "},"L20 Boltzmann machines 1.html":{"url":"L20 Boltzmann machines 1.html","title":"19 Boltzmann Machines 1","keywords":"","body":"Training hopfield nets Geometric approach Behavior of E(y)=yTWy\\mathbf{E}(\\mathbf{y})=\\mathbf{y}^{T} \\mathbf{W y}E(y)=yTWy with W=YYT−NpI\\mathbf{W}=\\mathbf{Y} \\mathbf{Y}^{T}-N_{p} \\mathbf{I}W=YYT−Np​I is identical to behavior with W=YYTW=YY^TW=YYT Energy landscape only differs by an additive constant Gradients and location of minima remain same (Have the same eigen vectors) Sine : yT(YYT−NpI)y=yTYYTy−NNp\\mathbf{y}^{T}\\left(\\mathbf{Y} \\mathbf{Y}^{T}-N_{p} \\mathbf{I}\\right) \\mathbf{y}=\\mathbf{y}^{T} \\mathbf{Y} \\mathbf{Y}^{T} \\mathbf{y}-N N_{p}yT(YYT−Np​I)y=yTYYTy−NNp​ We use yTYYTy\\mathbf{y}^{T} \\mathbf{Y} \\mathbf{Y}^{T} \\mathbf{y}yTYYTy for analyze A pattern ypy_pyp​ is stored if: sign⁡(Wyp)=y_p\\operatorname{sign}\\left(\\mathbf{W} \\mathbf{y}_{p}\\right)=\\mathbf{y}\\_{p}sign(Wyp​)=y_p for all target patterns Training: Design WWW such that this holds Simple solution: ypy_pyp​ is an Eigenvector of WWW Storing k orthogonal patterns Let Y=[y_1y_2…y_K]\\mathbf{Y}=\\left[\\mathbf{y}\\_{1} \\mathbf{y}\\_{2} \\ldots \\mathbf{y}\\_{K}\\right]Y=[y_1y_2…y_K] W=YΛYT\\mathbf{W}=\\mathbf{Y} \\Lambda \\mathbf{Y}^{T}W=YΛYT λ1,...,λk\\lambda_1,...,\\lambda_kλ1​,...,λk​ are positive for λ1=λ2=λk=1\\lambda_1= \\lambda_2=\\lambda_k= 1λ1​=λ2​=λk​=1 this is exactly the Hebbian rule Any pattern yyy can be written as y=a1y1+a2y2+⋯+aNyN\\mathbf{y}=a_{1} \\mathbf{y}_{1}+a_{2} \\mathbf{y}_{2}+\\cdots+a_{N} \\mathbf{y}_{N}y=a1​y1​+a2​y2​+⋯+aN​yN​ Wy=a1Wy1+a2Wy2+⋯+aNWyN=y\\mathbf{W y}=a_{1} \\mathbf{W y}_{1}+a_{2} \\mathbf{W y}_{2}+\\cdots+a_{N} \\mathbf{W y}_{N} = yWy=a1​Wy1​+a2​Wy2​+⋯+aN​WyN​=y All patterns are stable Remembers everything Completely useless network Even if we store fewer than NNN patterns Let Y=[y_1y_2…y_Kr_K+1r_K+2…r_N]Y=\\left[\\mathbf{y}\\_{1} \\mathbf{y}\\_{2} \\ldots \\mathbf{y}\\_{K} \\mathbf{r}\\_{K+1} \\mathbf{r}\\_{K+2} \\ldots \\mathbf{r}\\_{N}\\right]Y=[y_1y_2…y_Kr_K+1r_K+2…r_N] W=YΛYTW=Y \\Lambda Y^{T}W=YΛYT r_K+1r_K+2…r_N\\mathbf{r}\\_{K+1} \\mathbf{r}\\_{K+2} \\ldots \\mathbf{r}\\_{N}r_K+1r_K+2…r_N are orthogonal to y1y2…yK\\mathbf{y}_1 \\mathbf{y}_2 \\ldots \\mathbf{y}_Ky1​y2​…yK​ λ1=λ2=λk=1\\lambda_1= \\lambda_2=\\lambda_k= 1λ1​=λ2​=λk​=1 Problem arise because eigen values are all 1.0 Ensures stationarity of vectors in the subspace All stored patterns are equally important General (nonorthogonal) vectors wji=∑p∈{p}yipyjpw_{j i}=\\sum_{p \\in\\{p\\}} y_{i}^{p} y_{j}^{p}wji​=∑p∈{p}​yip​yjp​ The maximum number of stationary patterns is actually exponential in NNN (McElice and Posner, 84’) For a specific set of KKK patterns, we can always build a network for which all KKK patterns are stable provided k≤Nk \\le Nk≤N But this may come with many “parasitic” memories Optimization Energy function E=−12yTWy−bTyE=-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W} \\mathbf{y}-\\mathbf{b}^{T} \\mathbf{y}E=−21​yTWy−bTy This must be maximally low for target patterns Must be maximally high for all other patterns So that they are unstable and evolve into one of the target patterns Estimate WWW such that EEE is minimized for y1,...,yPy_1,...,y_Py1​,...,yP​ EEE is maximized for all other yyy Minimize total energy of target patterns E(y)=−12yTWyW^=argmin⁡W∑y∈YPE(y)E(\\mathbf{y})=-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W y} \\quad \\widehat{\\mathbf{W}}=\\underset{\\mathbf{W}}{\\operatorname{argmin}} \\sum_{\\mathbf{y} \\in \\mathbf{Y}_{P}} E(\\mathbf{y})E(y)=−21​yTWyW=Wargmin​∑y∈YP​​E(y) However, might also pull all the neighborhood states down Maximize the total energy of all non-target patterns E(y)=−12yTWyE(\\mathbf{y})=-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W} \\mathbf{y}E(y)=−21​yTWy W^=argmin⁡W∑y∈YPE(y)−∑y∉YPE(y)\\widehat{\\mathbf{W}}=\\underset{\\mathbf{W}}{\\operatorname{argmin}} \\sum_{\\mathbf{y} \\in \\mathbf{Y}_{P}} E(\\mathbf{y})-\\sum_{\\mathbf{y} \\notin \\mathbf{Y}_{P}} E(\\mathbf{y})W=Wargmin​∑y∈YP​​E(y)−∑y∉YP​​E(y) Simple gradient descent W=w+η(∑y∈YPyyT−∑y∉YPyyT)\\mathbf{W}=\\mathbf{w}+\\eta\\left(\\sum_{\\mathbf{y} \\in \\mathbf{Y}_{P}} \\mathbf{y} \\mathbf{y}^{T}-\\sum_{\\mathbf{y} \\notin \\mathbf{Y}_{P}} \\mathbf{y} \\mathbf{y}^{T}\\right)W=w+η(∑y∈YP​​yyT−∑y∉YP​​yyT) minimize the energy at target patterns raise all non-target patterns Do we need to raise everything? Raise negative class Focus on raising the valleys If you raise every valley, eventually they’ll all move up above the target patterns, and many will even vanish How do you identify the valleys for the current WWW? Initialize the network randomly and let it evolve It will settle in a valley Should we randomly sample valleys? Are all valleys equally important? Major requirement: memories must be stable They must be broad valleys Solution: initialize the network at valid memories and let it evolve It will settle in a valley If this is not the target pattern, raise it What if there’s another target pattern downvalley no need to raise the entire surface, or even every valley Raise the neighborhood of each target memory Storing more than N patterns Visible neurons The neurons that store the actual patterns of interest Hidden neurons The neurons that only serve to increase the capacity but whose actual values are not important The maximum number of patterns the net can store is bounded by the width NNN of the patterns.. So lets pad the patterns with KKK “don’t care” bits The new width of the patterns is N+KN+KN+K Now we can store N+KN+KN+K patterns! Taking advantage of don’t care bits Simple random setting of don’t care bits, and using the usual training and recall strategies for Hopfield nets should work However, to exploit it properly, it helps to view the Hopfield net differently: as a probabilistic machine A probabilistic interpretation For binary y the energy of a pattern is the analog of the negative log likelihood of a Boltzmann distribution Minimizing energy maximizes log likelihood E(y)=−12yTWyP(y)=Cexp⁡(−E(y))E(\\mathbf{y})=-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W y} \\quad P(\\mathbf{y})=\\operatorname{Cexp}(-E(\\mathbf{y}))E(y)=−21​yTWyP(y)=Cexp(−E(y)) Boltzmann Distribution E(y)=−12yTWy−bTyE(\\mathbf{y})=-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W} \\mathbf{y}-\\mathbf{b}^{T} \\mathbf{y}E(y)=−21​yTWy−bTy P(y)=Cexp⁡(−E(y)kT)P(\\mathbf{y})=\\operatorname{Cexp}\\left(\\frac{-E(\\mathbf{y})}{k T}\\right)P(y)=Cexp(kT−E(y)​) C=1∑yexp⁡(−E(y)kT)C=\\frac{1}{\\sum_{\\mathrm{y}} \\exp \\left(\\frac{-E(\\mathbf{y})}{k T}\\right)}C=∑y​exp(kT−E(y)​)1​ kkk is the Boltzmann constant, TTT is the temperature of the system Optimizing WWW E(y)=−12yTWyW^=argmin⁡W∑y∈YPE(y)−∑y∉YPE(y)E(\\mathbf{y})=-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W} \\mathbf{y} \\quad \\widehat{\\mathbf{W}}=\\underset{\\mathbf{W}}{\\operatorname{argmin}} \\sum_{\\mathbf{y} \\in \\mathbf{Y}_{P}} E(\\mathbf{y})-\\sum_{\\mathbf{y} \\notin \\mathbf{Y}_{P}} E(\\mathbf{y})E(y)=−21​yTWyW=Wargmin​∑y∈YP​​E(y)−∑y∉YP​​E(y) Simple gradient descent W=W+η(∑y∈YPαyyyT−∑y∉YPβ(E(y))yyT)\\mathbf{W}=\\mathbf{W}+\\eta\\left(\\sum_{\\mathbf{y} \\in \\mathbf{Y}_{P}} \\alpha_{\\mathbf{y}} \\mathbf{y} \\mathbf{y}^{T}-\\sum_{\\mathbf{y} \\notin \\mathbf{Y}_{P}} \\beta(E(\\mathbf{y})) \\mathbf{y} \\mathbf{y}^{T}\\right)W=W+η(∑y∈YP​​αy​yyT−∑y∉YP​​β(E(y))yyT) αy\\alpha_yαy​ more importance to more frequently presented memories β(E(y))\\beta (E(y))β(E(y)) more importance to more attractive spurious memories Looks like an expectation W=W+η(Ey∼YPyyT−Ey∼YyyT)\\mathbf{W}=\\mathbf{W}+\\eta\\left(E_{\\mathbf{y} \\sim \\mathbf{Y}_{P}} \\mathbf{y} \\mathbf{y}^{T}-E_{\\mathbf{y} \\sim Y} \\mathbf{y} \\mathbf{y}^{T}\\right)W=W+η(Ey∼YP​​yyT−Ey∼Y​yyT) The behavior of the Hopfield net is analogous to annealed dynamics of a spin glass characterized by a Boltzmann distribution "},"L21 Boltzmann machines 2.html":{"url":"L21 Boltzmann machines 2.html","title":"20 Boltzmann Machines 2","keywords":"","body":" This lecture redefined a regular Hopfield net as a **stochastic** system: Boltzmann machines. And talked about the training, sampling issues of Boltzmann machines model, introduced *Restricted* Boltzmann Machines, which is a common used model in practice. The Hopfield net as a distribution The Helmholtz Free Energy of a System At any time, the probability of finding the system in state sss at temperature TTT is PT(s)P_T(s)PT​(s) At each state it has a potential energy EsE_sEs​ The internal energy of the system, representing its capacity to do work, is the average UT=∑SPT(s)ES U_{T}=\\sum_{S} P_{T}(s) E_{S} UT​=S∑​PT​(s)ES​ The capacity to do work is counteracted by the internal disorder of the system, i.e. its entropy HT=−∑SPT(s)log⁡PT(s) H_{T}=-\\sum_{S} P_{T}(s) \\log P_{T}(s) HT​=−S∑​PT​(s)logPT​(s) The Helmholtz free energy of the system measures the useful work derivable from it and combines the two terms FT=UT+kTHT F_{T}=U_{T}+k T H_{T} FT​=UT​+kTHT​ =∑SPT(s)ES−kT∑SPT(s)log⁡PT(s) =\\sum_{S} P_{T}(s) E_{S}-k T \\sum_{S} P_{T}(s) \\log P_{T}(s) =S∑​PT​(s)ES​−kTS∑​PT​(s)logPT​(s) The probability distribution of the states at steady state is known as the Boltzmann distribution Minimizing this w.r.t PT(s)P_T(s)PT​(s), we get PT(s)=1Zexp⁡(−ESkT) P_{T}(s)=\\frac{1}{Z} \\exp \\left(\\frac{-E_{S}}{k T}\\right) PT​(s)=Z1​exp(kT−ES​​) ZZZ is a normalizing constant Hopfield net as a distribution E(S)=−∑ijwijsisj−bisiE(S)=-\\sum_{iE(S)=−∑ij​wij​si​sj​−bi​si​ P(S)=exp⁡(−E(S))∑S′exp⁡(−E(S′))P(S)=\\frac{\\exp (-E(S))}{\\sum_{S^{\\prime}} \\exp \\left(-E\\left(S^{\\prime}\\right)\\right)}P(S)=∑S′​exp(−E(S′))exp(−E(S))​ The stochastic Hopfield network models a probability distribution over states It is a generative model: generates states according to P(S)P(S)P(S) The field at a single node Let's take one node as example Let SSS and S′S^\\primeS′ be the states with the +1 and -1 states P(S)=P(si=1∣sj≠i)P(sj≠i)P(S)=P\\left(s_{i}=1 \\mid s_{j \\neq i}\\right) P\\left(s_{j \\neq i}\\right)P(S)=P(si​=1∣sj≠i​)P(sj≠i​) P(S′)=P(si=−1∣sj≠i)P(sj≠i)P\\left(S^{\\prime}\\right)=P\\left(s_{i}=-1 \\mid s_{j \\neq i}\\right) P\\left(s_{j \\neq i}\\right)P(S′)=P(si​=−1∣sj≠i​)P(sj≠i​) log⁡P(S)−log⁡P(S′)=log⁡P(si=1∣sj≠i)−log⁡P(si=−1∣sj≠i)\\log P(S)-\\log P\\left(S^{\\prime}\\right)=\\log P\\left(s_{i}=1 \\mid s_{j \\neq i}\\right)-\\log P\\left(s_{i}=-1 \\mid s_{j \\neq i}\\right)logP(S)−logP(S′)=logP(si​=1∣sj≠i​)−logP(si​=−1∣sj≠i​) log⁡P(S)−log⁡P(S′)=log⁡P(si=1∣sj≠i)1−P(si=1∣sj≠i)\\log P(S)-\\log P\\left(S^{\\prime}\\right)=\\log \\frac{P\\left(s_{i}=1 \\mid s_{j \\neq i}\\right)}{1-P\\left(s_{i}=1 \\mid s_{j \\neq i}\\right)}logP(S)−logP(S′)=log1−P(si​=1∣sj≠i​)P(si​=1∣sj≠i​)​ log⁡P(S)=−E(S)+C\\log P(S)=-E(S)+ClogP(S)=−E(S)+C E(S)=−12(Enot i+∑j≠iwijsj+bi)E(S)=-\\frac{1}{2}\\left(E_{\\text {not } i}+\\sum_{j \\neq i} w_{i j} s_{j}+b_{i}\\right)E(S)=−21​(Enot i​+∑j≠i​wij​sj​+bi​) E(S′)=−12(Enot i−∑j≠iwijsj−bi)E\\left(S^{\\prime}\\right)=-\\frac{1}{2}\\left(E_{\\text {not } i}-\\sum_{j \\neq i} w_{i j} s_{j}-b_{i}\\right)E(S′)=−21​(Enot i​−∑j≠i​wij​sj​−bi​) log⁡P(S)−log⁡P(S′)=E(S′)−E(S)=∑j≠iwijSj+bi\\log P(S)-\\log P\\left(S^{\\prime}\\right)=E\\left(S^{\\prime}\\right)-E(S)=\\sum_{j \\neq i} w_{i j} S_{j}+b_{i}logP(S)−logP(S′)=E(S′)−E(S)=∑j≠i​wij​Sj​+bi​ log⁡(P(si=1∣sj≠i)1−P(si=1∣sj≠i))=∑j≠iwijsj+bi\\log \\left(\\frac{P\\left(s_{i}=1 \\mid s_{j \\neq i}\\right)}{1-P\\left(s_{i}=1 \\mid s_{j \\neq i}\\right)}\\right)=\\sum_{j \\neq i} w_{i j} s_{j}+b_{i}log(1−P(si​=1∣sj≠i​)P(si​=1∣sj≠i​)​)=∑j≠i​wij​sj​+bi​ P(si=1∣sj≠i)=11+e−(∑j≠iwijsj+bi)P\\left(s_{i}=1 \\mid s_{j \\neq i}\\right)=\\frac{1}{1+e^{-\\left(\\sum_{j \\neq i} w_{i j} s_{j}+b_{i}\\right)}}P(si​=1∣sj≠i​)=1+e−(∑j≠i​wij​sj​+bi​)1​ The probability of any node taking value 1 given other node values is a logistic Redefining the network Redefine a regular Hopfield net as a stochastic system Each neuron is now a stochastic unit with a binary state sis_isi​, which can take value 0 or 1 with a probability that depends on the local field zi=∑jwijsj+biz_{i}=\\sum_{j} w_{i j} s_{j}+b_{i}zi​=∑j​wij​sj​+bi​ P(si=1∣sj≠i)=11+e−ziP\\left(s_{i}=1 \\mid s_{j \\neq i}\\right)=\\frac{1}{1+e^{-z_{i}}}P(si​=1∣sj≠i​)=1+e−zi​1​ Note The Hopfield net is a probability distribution over binary sequences (Boltzmann distribution) The conditional distribution of individual bits in the sequence is a logistic The evolution of the Hopfield net can be made stochastic Instead of deterministically responding to the sign of the local field, each neuron responds probabilistically Recall patterns The Boltzmann Machine The entire model can be viewed as a generative model Has a probability of producing any binary vector yyy E(y)=−12yTWyE(\\mathbf{y})=-\\frac{1}{2} \\mathbf{y}^{T} \\mathbf{W} \\mathbf{y}E(y)=−21​yTWy P(y)=Cexp⁡(−E(y)T)P(\\mathbf{y})=\\operatorname{Cexp}\\left(-\\frac{E(\\mathbf{y})}{T}\\right)P(y)=Cexp(−TE(y)​) Training a Hopfield net: Must learn weights to “remember” target states and “dislike” other states Must learn weights to assign a desired probability distribution to states Just maximize likelihood Maximum Likelihood Training log⁡(P(S))=(∑ijwijsisj)−log⁡(∑S′exp⁡(∑ijwijsi′sj′))\\log (P(S))=\\left(\\sum_{ilog(P(S))=(∑ij​wij​si​sj​)−log(∑S′​exp(∑ij​wij​si′​sj′​)) L=1N∑S∈Slog⁡(P(S))=1N∑S(∑ijwijsisj)−log⁡(∑S′exp⁡(∑ijwijsi′sj′))\\mathcal{L}=\\frac{1}{N} \\sum_{S \\in \\mathbf{S}} \\log (P(S)) =\\frac{1}{N} \\sum_{S}\\left(\\sum_{iL=N1​∑S∈S​log(P(S))=N1​∑S​(∑ij​wij​si​sj​)−log(∑S′​exp(∑ij​wij​si′​sj′​)) Second term derivation dlog⁡(∑S′exp⁡(∑ijwijsi′sj′))dwij=∑S′exp⁡(∑ijwijsi′sj′)∑S′exp⁡(∑ijwijsi′′sj′)si′sj′\\frac{d \\log \\left(\\sum_{S^{\\prime}} \\exp \\left(\\sum_{idwij​dlog(∑S′​exp(∑ij​wij​si′​sj′​))​=∑S′​∑S′​exp(∑ij​wij​si′′​sj′​)exp(∑ij​wij​si′​sj′​)​si′​sj′​ dlog⁡(∑S′exp⁡(∑ijwijsi′sj′))dwij=∑S′P(S′)si′sj′\\frac{d \\log \\left(\\sum_{S^{\\prime}} \\exp \\left(\\sum_{idwij​dlog(∑S′​exp(∑ij​wij​si′​sj′​))​=∑S′​​P(S′)si′​sj′​ The second term is simply the expected value of siSjs_iS_jsi​Sj​, over all possible values of the state We cannot compute it exhaustively, but we can compute it by sampling! Overall gradient ascent rule wij=wij+ηd⟨log⁡(P(S))⟩dwijw_{i j}=w_{i j}+\\eta \\frac{d\\langle\\log (P(\\mathbf{S}))\\rangle}{d w_{i j}}wij​=wij​+ηdwij​d⟨log(P(S))⟩​ Overall Training Initialize weights Let the network run to obtain simulated state samples Compute gradient and update weights Iterate Note the similarity to the update rule for the Hopfield network The only difference is how we got the samples Adding Capacity Visible neurons The neurons that store the actual patterns of interest Hidden neurons The neurons that only serve to increase the capacity but whose actual values are not important We could have multiple hidden patterns coupled with any visible pattern These would be multiple stored patterns that all give the same visible output We are interested in the marginal probabilities over visible bits S=(V,H)S=(V,H)S=(V,H) P(S)=exp⁡(−E(S))∑S′exp⁡(−E(S′))P(S)=\\frac{\\exp (-E(S))}{\\sum_{S^{\\prime}} \\exp \\left(-E\\left(S^{\\prime}\\right)\\right)}P(S)=∑S′​exp(−E(S′))exp(−E(S))​ P(S)=P(V,H)P(S) = P(V,H)P(S)=P(V,H) P(V)=∑HP(S)P(V)=\\sum_{H} P(S)P(V)=∑H​P(S) Train to maximize probability of desired patterns of visible bits E(S)=−∑ijwijsisjE(S)=-\\sum_{iE(S)=−∑ij​wij​si​sj​ P(S)=exp⁡(∑ijwijsisj)∑S′exp⁡(∑ijwijsi′sj′)P(S)=\\frac{\\exp \\left(\\sum_{iP(S)=∑S′​exp(∑ij​wij​si′​sj′​)exp(∑ij​wij​si​sj​)​ P(V)=∑Hexp⁡(∑ijwijsisj)∑S′exp⁡(∑ijwijsi′sj′)P(V)=\\sum_{H} \\frac{\\exp \\left(\\sum_{iP(V)=∑H​∑S′​exp(∑ij​wij​si′​sj′​)exp(∑ij​wij​si​sj​)​ Maximum Likelihood Training log⁡(P(V))=log⁡(∑Hexp⁡(∑ijwijsisj))−log⁡(∑S′exp⁡(∑ijwijsi′sj′))\\log (P(V))=\\log \\left(\\sum_{H} \\exp \\left(\\sum_{ilog(P(V))=log(H∑​exp(ij∑​wij​si​sj​))−log(S′​∑​exp(ij∑​wij​si′​sj′​)) L=1N∑V∈Vlog⁡(P(V))\\mathcal{L}=\\frac{1}{N} \\sum_{V \\in \\mathbf{V}} \\log (P(V))L=N1​V∈V∑​log(P(V)) dLdwij=1N∑V∈V∑HP(S∣V)sisj−∑S!P(S′)si′sj′ \\frac{d \\mathcal{L}}{d w_{i j}}=\\frac{1}{N} \\sum_{V \\in \\mathbf{V}} \\sum_{H} P(S \\mid V) s_{i} s_{j}-\\sum_{S !} P\\left(S^{\\prime}\\right) s_{i}^{\\prime} s_{j}^{\\prime} dwij​dL​=N1​V∈V∑​H∑​P(S∣V)si​sj​−S!∑​P(S′)si′​sj′​ ∑HP(S∣V)sisj≈1K∑H∈HsimulsiSj\\sum_{H} P(S \\mid V) s_{i} s_{j} \\approx \\frac{1}{K} \\sum_{H \\in \\mathbf{H}_{s i m u l}} s_{i} S_{j}∑H​P(S∣V)si​sj​≈K1​∑H∈Hsimul​​si​Sj​ Computed as the average sampled hidden state with the visible bits fixed ∑S′P(S′)si′sj′≈1M∑Si∈Ssimulsi′Sj′\\sum_{S^{\\prime}} P\\left(S^{\\prime}\\right) s_{i}^{\\prime} s_{j}^{\\prime} \\approx \\frac{1}{M} \\sum_{S_{i} \\in \\mathbf{S}_{s i m u l}} s_{i}^{\\prime} S_{j}^{\\prime}∑S′​P(S′)si′​sj′​≈M1​∑Si​∈Ssimul​​si′​Sj′​ Computed as the average of sampled states when the network is running “freely” Training Step1 For each training pattern ViV_iVi​ Fix the visible units to ViV_iVi​ Let the hidden neurons evolve from a random initial point to generate HiH_iHi​ Generate Si=[Vi,Hi]S_i = [V_i,H_i]Si​=[Vi​,Hi​] Repeat K times to generate synthetic training S={S1,1,S1,2,…,S1K,S2,1,…,SN,K} \\mathbf{S}=\\{S_{1,1}, S_{1,2}, \\ldots, S_{1 K}, S_{2,1}, \\ldots, S_{N, K}\\} S={S1,1​,S1,2​,…,S1K​,S2,1​,…,SN,K​} Step2 Now unclamp the visible units and let the entire network evolve several times to generate Ssimul=S_simul,1,S_simul,2,…,S_simul,M \\mathbf{S}_{simul}=S\\_{simul, 1}, S\\_{simul, 2}, \\ldots, S\\_{simul, M} Ssimul​=S_simul,1,S_simul,2,…,S_simul,M Gradients d⟨log⁡(P(S))⟩dwij=1NK∑Ssisj−1M∑Si∈Ssimul si′sj′ \\frac{d\\langle\\log (P(\\mathbf{S}))\\rangle}{d w_{i j}}=\\frac{1}{N K} \\sum_{\\boldsymbol{S}} s_{i} s_{j}-\\frac{1}{M} \\sum_{S_{i} \\in \\mathbf{S}_{\\text {simul }}} s_{i}^{\\prime} s_{j}^{\\prime} dwij​d⟨log(P(S))⟩​=NK1​S∑​si​sj​−M1​Si​∈Ssimul ​∑​si′​sj′​ wij=wij−ηd⟨log⁡(P(S))⟩dwij w_{i j}=w_{i j}-\\eta \\frac{d\\langle\\log (P(\\mathbf{S}))\\rangle}{d w_{i j}} wij​=wij​−ηdwij​d⟨log(P(S))⟩​ Gradients are computed as before, except that the first term is now computed over the expanded training data Issues Training takes for ever Doesn’t really work for large problems A small number of training instances over a small number of bits Restricted Boltzmann Machines Partition visible and hidden units Visible units ONLY talk to hidden units Hidden units ONLY talk to visible units Training Step1 For each sample Anchor visible units Sample from hidden units No looping!! Step2 Now unclamp the visible units and let the entire network evolve several times to generate Ssimul=S_simul,1,S_simul,2,…,S_simul,M \\mathbf{S}_{simul}=S\\_{simul, 1}, S\\_{simul, 2}, \\ldots, S\\_{simul, M} Ssimul​=S_simul,1,S_simul,2,…,S_simul,M For each sample Initialize V0V_0V0​ (visible) to training instance value Iteratively generate hidden and visible units Gradient ∂log⁡p(v)∂wij=vihj>0−vihj>∞ \\frac{\\partial \\log p(v)}{\\partial w_{i j}}=^{0}-^{\\infty} ∂wij​∂logp(v)​=vi​hj​>0−vi​hj​>∞ A Shortcut: Contrastive Divergence Recall: Raise the neighborhood of each target memory Sufficient to run one iteration to give a good estimate of the gradient ∂log⁡p(v)∂wij=vihj>0−vihj>1 \\frac{\\partial \\log p(v)}{\\partial w_{i j}}=^{0}-^{1} ∂wij​∂logp(v)​=vi​hj​>0−vi​hj​>1 "},"L22 EM algorithm.html":{"url":"L22 EM algorithm.html","title":"21 EM Algorithm","keywords":"","body":" This lecture introduced EM algorithm: an iterative technique to estimate probaility models for missing data. Meanwhile, mixture Gaussian, PCA and factor analysis are actually *generative* models in a way of EM. Key points EM: An iterative technique to estimate probability models for data with missing components or information By iteratively “completing” the data and reestimating parameters PCA: Is actually a generative model for Gaussian data Data lie close to a linear manifold, with orthogonal noise A lienar autoencoder! Factor Analysis: Also a generative model for Gaussian data Data lie close to a linear manifold Like PCA, but without directional constraints on the noise (not necessarily orthogonal) Generative models Learning a generative model You are given some set of observed data X={x}X=\\{x\\}X={x} You choose a model P(x;θ)P(x ; \\theta)P(x;θ) for the distribution of xxx θ\\thetaθ are the parameters of the model Estimate the theta such that P(x;θ)P(x ; \\theta)P(x;θ) best “fits” the observations X={x}X=\\{x\\}X={x} How to define \"best fits\"? Maximum likelihood! Assumption: The data you have observed are very typical of the process EM algorithm Tackle missing data and information problem in model estimation Let ooo are observed data log⁡P(o)=log⁡∑hP(h,o)=log⁡∑hQ(h)P(h,o)Q(h) \\log P(o)=\\log \\sum_{h} P(h, o)=\\log \\sum_{h} Q(h) \\frac{P(h, o)}{Q(h)} logP(o)=logh∑​P(h,o)=logh∑​Q(h)Q(h)P(h,o)​ The logarithm is a concave function, therefore log⁡∑hQ(h)P(h,o)Q(h)≥∑hQ(h)log⁡P(h,o)Q(h) \\log \\sum_{h} Q(h) \\frac{P(h, o)}{Q(h)} \\geq \\sum_{h} Q(h) \\log \\frac{P(h, o)}{Q(h)} logh∑​Q(h)Q(h)P(h,o)​≥h∑​Q(h)logQ(h)P(h,o)​ Choose a tight lower bound Let Q(h)=P(h∣o;θ′)Q(h)=P(h \\mid o ; \\theta^{\\prime})Q(h)=P(h∣o;θ′) log⁡P(o;θ)≥∑hP(h∣o;θ′)log⁡P(h,o;θ)P(h∣o;θ′) \\begin{aligned} \\log P(o ; \\theta) \\geq \\sum_{h} P\\left(h \\mid o ; \\theta^{\\prime}\\right) \\log \\frac{P(h, o ; \\theta)}{P\\left(h \\mid o ; \\theta^{\\prime}\\right)} \\end{aligned} logP(o;θ)≥h∑​P(h∣o;θ′)logP(h∣o;θ′)P(h,o;θ)​​ Let J(θ,θ′)=∑hP(h∣o;θ′)log⁡P(h,o;θ)P(h∣o;θ′)J\\left(\\theta, \\theta^{\\prime}\\right)=\\sum_{h} P\\left(h \\mid o ; \\theta^{\\prime}\\right) \\log \\frac{P(h, o ; \\theta)}{P\\left(h \\mid o ; \\theta^{\\prime}\\right)}J(θ,θ′)=∑h​P(h∣o;θ′)logP(h∣o;θ′)P(h,o;θ)​ log⁡P(o;θ)≥J(θ,θ′) \\begin{array}{l} \\log P(o ; \\theta) \\geq J\\left(\\theta, \\theta^{\\prime}\\right) \\end{array} logP(o;θ)≥J(θ,θ′)​ The algorithm process EM for missing data “Expand” every incomplete vector out into all possibilities With proportion P(m∣o)P(m|o)P(m∣o) (from previous estimate of the model) Estimate the statistics from the expanded data EM for missing information Problem : We are not given the actual Gaussian for each observation What we want: (o1,k1),(o2,k2),(o3,k3)…\\left(o_{1}, k_{1}\\right),\\left(o_{2}, k_{2}\\right),\\left(o_{3}, k_{3}\\right) \\ldots(o1​,k1​),(o2​,k2​),(o3​,k3​)… What we have: o1,o2,o3…o_{1}, o_{2}, o_{3} \\ldotso1​,o2​,o3​… The algorithm process General EM principle “Complete” the data by considering every possible value for missing data/variables Reestimate parameters from the “completed” data Principal Component Analysis Find the principal subspace such that when all vectors are approximated as lying on that subspace, the approximation error is minimal Closed form Total projection error for all data L=∑xxTx−wTxxTw L=\\sum_{x} x^{T} x-w^{T} x x^{T} w L=x∑​xTx−wTxxTw Minimizing this w.r.t 𝑤 (subject to 𝑤 = unit vector) gives you the Eigenvalue equation (∑xxTx)w=λw \\left(\\sum_{x} x^{T} x\\right) w=\\lambda w (x∑​xTx)w=λw This can be solved to find the principal subspace However, it is not feasible for large matrix (need to find eigenvalue) Iterative solution Objective: Find a vector (subspace) www and a position zzz on www such that zw≈xzw\\approx xzw≈x most closely (in an L2 sense) for the entire (training) data The algorithm process PCA & linear autoencoder We put data XXX into the inital subpace, got ZZZ The fix ZZZ to get a better subpace WWW, etc... This is an autoencoder with linear activations ! Backprop actually works by simultaneously updating (implicitly) and in tiny increments PCA is actually a generative model The observed data are Gaussian Gaussian data lying very close to a principal subspace Comprising “clean” Gaussian data on the subspace plus orthogonal noise "},"L23 Variational Autoencoders.html":{"url":"L23 Variational Autoencoders.html","title":"22 Variational Autoencoders","keywords":"","body":" Non-linear extensions of linear Gaussian models. EM for PCA With complete information If we knew zzz for each xxx, estimating AAA and DDD would be simple x=Az+E x=A z+E x=Az+E P(x∣z)=N(Az,D) P(x \\mid z)=N(A z, D) P(x∣z)=N(Az,D) Given complete information (x1,z1),(x2,z2)\\left(x_{1}, z_{1}\\right),\\left(x_{2}, z_{2}\\right)(x1​,z1​),(x2​,z2​) argmax⁡A,D∑(x,z)log⁡P(x,z)=argmax⁡A,D∑(x,z)log⁡P(x∣z) \\underset{A, D}{\\operatorname{argmax}} \\sum_{(x, z)} \\log P(x, z)=\\underset{A, D}{\\operatorname{argmax}} \\sum_{(x, z)} \\log P(x \\mid z) A,Dargmax​(x,z)∑​logP(x,z)=A,Dargmax​(x,z)∑​logP(x∣z) =argmax⁡A,D∑(x,Z)log⁡1(2π)d∣D∣exp⁡(−0.5(x−Az)TD−1(x−Az)) =\\underset{A, D}{\\operatorname{argmax}} \\sum_{(x, Z)} \\log \\frac{1}{\\sqrt{(2 \\pi)^{d}|D|}} \\exp \\left(-0.5(x-A z)^{T} D^{-1}(x-A z)\\right) =A,Dargmax​(x,Z)∑​log(2π)d∣D∣​1​exp(−0.5(x−Az)TD−1(x−Az)) We can get a close form solution: A=XZ+A = XZ^{+}A=XZ+ But we don't have ZZZ => missing With incomplete information Initialize the plane Complete the data by computing the appropriate zzz for the plane P(z∣X;A)P(z|X;A)P(z∣X;A) is a delta, because EEE is orthogonal to AAA Reestimate the plane using the zzz Iterate Linear Gaussian Model PCA assumes the noise is always orthogonal to the data Not always true The noise added to the output of the encoder can lie in any direction (uncorrelated) We want a generative model: to generate any point Take a Gaussian step on the hyperplane Add full-rank Gaussian uncorrelated noise that is independent of the position on the hyperplane Uncorrelated: diagonal covariance matrix Direction of noise is unconstrained With complete information x=Az+e x=A z+e x=Az+e P(x∣z)=N(Az,D) P(x \\mid z)=N(A z, D) P(x∣z)=N(Az,D) Given complete information X=[x1,x2,…],Z=[z1,z2,…]X=\\left[x_{1}, x_{2}, \\ldots\\right], Z=\\left[z_{1}, z_{2}, \\ldots\\right]X=[x1​,x2​,…],Z=[z1​,z2​,…] argmax⁡A,D∑(x,z)log⁡P(x,z)=argmax⁡A,D∑(x,z)log⁡P(x∣z) \\underset{A, D}{\\operatorname{argmax}} \\sum_{(x, z)} \\log P(x, z)=\\underset{A, D}{\\operatorname{argmax}} \\sum_{(x, z)} \\log P(x \\mid z) A,Dargmax​(x,z)∑​logP(x,z)=A,Dargmax​(x,z)∑​logP(x∣z) =argmax⁡A,D∑(x,z)log⁡1(2π)d∣D∣exp⁡(−0.5(x−Az)TD−1(x−Az)) =\\underset{A, D}{\\operatorname{argmax}} \\sum_{(x, z)} \\log \\frac{1}{\\sqrt{(2 \\pi)^{d}|D|}} \\exp \\left(-0.5(x-A z)^{T} D^{-1}(x-A z)\\right) =A,Dargmax​(x,z)∑​log(2π)d∣D∣​1​exp(−0.5(x−Az)TD−1(x−Az)) =argmax⁡A,D∑(x,z)−12log⁡∣D∣−0.5(x−Az)TD−1(x−Az) =\\underset{A, D}{\\operatorname{argmax}} \\sum_{(x, z)}-\\frac{1}{2} \\log |D|-0.5(x-A z)^{T} D^{-1}(x-A z) =A,Dargmax​(x,z)∑​−21​log∣D∣−0.5(x−Az)TD−1(x−Az) We can also get closed form solution With incomplete information Option 1 In every possible way proportional to P(z∣x)P(z|x)P(z∣x) (Gaussian) Compute the solution from the completed data argmax⁡A,D∑x∫−∞∞p(z∣x)(−12log⁡∣D∣−0.5(x−Az)TD−1(x−Az))dz \\underset{A, D}{\\operatorname{argmax}} \\sum_{x} \\int_{-\\infty}^{\\infty} p(z \\mid x)\\left(-\\frac{1}{2} \\log |D|-0.5(x-A z)^{T} D^{-1}(x-A z)\\right) d z A,Dargmax​x∑​∫−∞∞​p(z∣x)(−21​log∣D∣−0.5(x−Az)TD−1(x−Az))dz The same as before Option 2 By drawing samples from P(z∣x)P(z|x)P(z∣x) Compute the solution from the completed data The intuition behind Linear Gaussian Model z∼N(0,I)z \\sim N(0, I)z∼N(0,I) => AzAzAz The linear transform stretches and rotates the K-dimensional input space onto a Kdimensional hyperplane in the data space X=Az+EX = Az +EX=Az+E Add Gaussian noise to produce points that aren’t necessarily on the plane The posterior probability P(z∣x)P(z|x)P(z∣x) gives you the location of all the points on the plane that could have generated xxx and their probabilities What about data that are not Gaussian distributed close to a plane? Linear Gaussian Models fail How to do that Non-linear Gaussian Model f(z)f(z)f(z) is a non-linear function that produces a curved manifold Like the decoder of a non-linear AE Generating process Draw a sample zzz from a Uniform Gaussian Transform zzz by f(z)f(z)f(z) This places zzz on the curved manifold Add uncorrelated Gaussian noise to get the final observation Key requirement Identifying the dimensionality KKK of the curved manifold Having a function that can transform the (linear) KKK-dimensional input space (space of zzz ) to the desired KKK-dimensional manifold in the data space With complete data x=f(z;θ)+e x=f(z ; \\theta)+e x=f(z;θ)+e P(x∣z)=N(f(z;θ),D) P(x \\mid z)=N(f(z ; \\theta), D) P(x∣z)=N(f(z;θ),D) Given complete information X=[x1,x2,…],Z=[z1,z2,…]X=\\left[x_{1}, x_{2}, \\ldots\\right], \\quad Z=\\left[z_{1}, z_{2}, \\ldots\\right]X=[x1​,x2​,…],Z=[z1​,z2​,…] θ⋆,D⋆=argmax⁡θ,D∑(x,z)log⁡P(x,z)=argmax⁡θ,D∑(x,z)log⁡P(x∣z) \\theta^{\\star}, D^{\\star}=\\underset{\\theta, D}{\\operatorname{argmax}} \\sum_{(x, z)} \\log P(x, z)=\\underset{\\theta, D}{\\operatorname{argmax}} \\sum_{(x, z)} \\log P(x \\mid z) θ⋆,D⋆=θ,Dargmax​(x,z)∑​logP(x,z)=θ,Dargmax​(x,z)∑​logP(x∣z) =argmax⁡θ,D∑(x,Z)log⁡1(2π)d∣D∣exp⁡(−0.5(x−f(z;θ))TD−1(x−f(z;θ))) =\\underset{\\theta, D}{\\operatorname{argmax}} \\sum_{(x, Z)} \\log \\frac{1}{\\sqrt{(2 \\pi)^{d}|D|}} \\exp \\left(-0.5(x-f(z ; \\theta))^{T} D^{-1}(x-f(z ; \\theta))\\right) =θ,Dargmax​(x,Z)∑​log(2π)d∣D∣​1​exp(−0.5(x−f(z;θ))TD−1(x−f(z;θ))) =argmax⁡θ,D∑(x,Z)−12log⁡∣D∣−0.5(x−f(z;θ))TD−1(x−f(z;θ)) =\\underset{\\theta, D}{\\operatorname{argmax}} \\sum_{(x, Z)}-\\frac{1}{2} \\log |D|-0.5(x-f(z ; \\theta))^{T} D^{-1}(x-f(z ; \\theta)) =θ,Dargmax​(x,Z)∑​−21​log∣D∣−0.5(x−f(z;θ))TD−1(x−f(z;θ)) There isn’t a nice closed form solution, but we could learn the parameters using backpropagation Incomplete data The posterior probability is given by P(z∣x)=P(x∣z)P(z)P(x) P(z \\mid x)=\\frac{P(x \\mid z) P(z)}{P(x)} P(z∣x)=P(x)P(x∣z)P(z)​ The denominator P(x)=∫−∞∞N(x;f(z;θ),D)N(z;0,D)dz P(x)=\\int_{-\\infty}^{\\infty} N(x ; f(z ; \\theta), D) N(z ; 0, D) d z P(x)=∫−∞∞​N(x;f(z;θ),D)N(z;0,D)dz Can not have a closed form solution Try to approximate it We approximate P(z∣x)P(z|x)P(z∣x) as P(z∣x)≈Q(z,x)=Gaussian⁡N(z;μ(x),Σ(x)) P(z \\mid x) \\approx Q(z, x)=\\operatorname{Gaussian} N(z ; \\mu(x), \\Sigma(x)) P(z∣x)≈Q(z,x)=GaussianN(z;μ(x),Σ(x)) Sample zzz from N(z;μ(x;ϕ),σ(x;ϕ))N(z;\\mu (x;\\phi),\\sigma (x;\\phi))N(z;μ(x;ϕ),σ(x;ϕ)) for each training instance Draw KKK-dimensional vector ε\\varepsilonε from N(0,I)N(0,I)N(0,I) Compute z=μ(x;φ)+Σ(x;φ)0.5εz=\\mu(x ; \\varphi)+\\Sigma(x ; \\varphi)^{0.5} \\varepsilonz=μ(x;φ)+Σ(x;φ)0.5ε Reestimate θ\\thetaθ from the entire “complete” data Using backpropagation L(θ,D)=∑(x,z)log⁡∣D∣+(x−f(z;θ))TD−1(x−f(z;θ)) L(\\theta, D)=\\sum_{(x, z)} \\log |D|+(x-f(z ; \\theta))^{T} D^{-1}(x-f(z ; \\theta)) L(θ,D)=(x,z)∑​log∣D∣+(x−f(z;θ))TD−1(x−f(z;θ)) θ⋆,D⋆=argmin⁡θ,DL(θ,D) \\theta^{\\star}, D^{\\star}=\\underset{\\theta, D}{\\operatorname{argmin}} L(\\theta, D) θ⋆,D⋆=θ,Dargmin​L(θ,D) Estimate φ\\varphiφ using the entire “complete” data Recall Q(z,x)=N(z;μ(x;φ),Σ(x;φ))Q(z, x)=N(z ; \\mu(x ; \\varphi), \\Sigma(x ; \\varphi))Q(z,x)=N(z;μ(x;φ),Σ(x;φ)) must approximate P(z∣x)P(z|x)P(z∣x) as closely as possible Define a divergence between Q(z,x)Q(z,x)Q(z,x) and P(z∣x)P(z|x)P(z∣x) Variational AutoEncoder Non-linear extensions of linear Gaussian models f(z;θ)f(z;\\theta)f(z;θ) is generally modelled by a neural network μ(x;φ)\\mu(x ; \\varphi)μ(x;φ) and Σ(x;φ)\\Sigma(x ; \\varphi)Σ(x;φ) are generally modelled by a common network with two outputs However, VAE can not be used to compute the likelihoood of data P(x;θ)P(x;\\theta)P(x;θ) is intractable Latent space The latent space zzz often captures underlying structure in the data xxx in a smooth manner Varying zzz continuously in different directions can result in plausible variations in the drawn output "}}