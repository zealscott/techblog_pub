{"./":{"url":"./","title":"Introduction","keywords":"","body":"Data Management Systems This GitBook notes are maintained by zealscott. Material 《统计学习方法》（李航） 手写书中所有代码：GitHub 《机器学习》（周志华） 阅读笔记：GitHub 习题讨论 CS229 Andrew Ng Notes/Videos ProblemSet Syllabus Time Slides CS229 Notes Reading Material 2.24 统计学习概述 The Motivation & Applications of Machine Learning video1 2.27 感知机/KNN An Application of Supervised Learning notes1/video2 3.6 朴素贝叶斯法/Logistic回归 LR Newton’s method Perceptron 线性回归与分类 Ps0notes2/video3 3.13 最大熵模型/SVM Gerentive model, naive Bayes 生成模型 video4/5 3.19 SVM SVM/Dual/SMO SVM理解 video6/7/8 3.27 Adaboost/GBDT GBDT&XGboost小记 4.3 EM算法 Bias/Variance/ERM Learning Theory notes4/video9 4.10 HMM Regularization Regularization&feature selection notes5/video10 4.17 CRF EM 算法Online learningKmeans Online LearningAdvice for applying MLEM 思想 notes6/7 video11/12Advice for applying ML 4.24 CRF(2) EM算法推导 notes8 5.8 NeuralNetwork Factor AnalysisEM算法 Factor Analysis notes9Video13/14 5.15 5.22 PCA PCA PCA算法推导 notes10Video15 5.29 Feature selection 6.6-6.20 ML theory @Last updated at 1/27/2019 "},"Keynotes.html":{"url":"Keynotes.html","title":"Keynotes","keywords":"","body":" Priciple concepts of machine Learning 统计学习概述 分类 监督学习/非监督学习/半监督学习/强化学习 统计学习方法的三要素 模型：模型的假设空间 在监督学习中，就是所要学习的条件概率或决策函数 策略：模型选择的准则 对应风险函数（衡量平均预测的好坏）和损失函数（衡量一次预测的好坏） 经验风险最小化：极大似然估计（容易造成过拟合） 例如：极大似然估计 结构风险最小化（对应于正则化）：加入正则化项， 例如：Bayes最大后验估计 算法：模型学习的算法 问题分类 回归问题：输入变量与输出变量均为连续变量 分类问题：输出变量为有限个离散变量 标注问题：输入与输出变量都是变量序列​ 监督模型类型 生成模型 GDA/Naive Bayes/HMM 可以快速还原出联合概率分布P(X,Y)P(X,Y)P(X,Y)，学习收敛速度更快 判别模型 Logistic/SVM/条件随机场 可以直接对数据进行各种程度的抽象，定义特征，简化学习问题 模型选择 假设空间中有不同复杂度的模型，需要涉及到模型选择问题。模型选择主要考虑到过拟合问题。 主要的模型选择方法： 正则化 min⁡1N∑i=1NL(yi,f(x))+λJ(f)\\min \\frac{1}{N}\\sum\\limits_{i=1}^NL(y_i,f(x)) + \\lambda J(f)minN1​i=1∑N​L(yi​,f(x))+λJ(f) 一般使用L1L_1L1​或者L2L_2L2​范数 交叉验证 分为训练集，验证集和测试集 验证集用于模型的选择，选择对于验证集有最小误差的模型 简单交叉验证（前70%为训练集），S折交叉验证（前S-1个子集的数据进行训练，重复S次 ），留一交叉验证 评价标准 准确率（accuracy）：给定的测试数据集，分类正确的样本数与总样本数的比 精确率（precision）：P=TPTP+FPP = \\frac{TP}{TP+FP}P=TP+FPTP​ 所有预测正类中正确的 召回率（recall）：P=TPTP+FNP = \\frac{TP}{TP+FN}P=TP+FNTP​ 所有预测正确中是正类的 F1值：F1=2TP2TP+FP+FNF_1 = \\frac{2TP}{2TP+FP+FN}F1​=2TP+FP+FN2TP​ SVM 原始问题建模 min⁡12∣∣w∣∣2\\min \\frac{1}{2} ||w||^2min21​∣∣w∣∣2 s.t.yi(w⋅xi+b)−1≥0,i=1,2,...,Ns.t. \\quad y_i(w\\cdot x_i + b ) -1 \\ge 0, i = 1,2,...,Ns.t.yi​(w⋅xi​+b)−1≥0,i=1,2,...,N 当yi(w⋅xi+b)=1y_i(w\\cdot x_i + b) =1yi​(w⋅xi​+b)=1时是support vector 对偶问题 min⁡12∑i∑jαiαjyiyj(xi⋅xj)−∑iαi\\min \\frac{1}{2}\\sum_i\\sum_j \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_i \\alpha_imin21​∑i​∑j​αi​αj​yi​yj​(xi​⋅xj​)−∑i​αi​ s.t.∑αiyi=0αi≥0s.t. \\quad \\sum \\alpha_iy_i = 0\\quad \\alpha_i \\ge 0s.t.∑αi​yi​=0αi​≥0 由KKT条件，w=∑αiyixiw = \\sum \\alpha_iy_ix_iw=∑αi​yi​xi​，同时可求得对应的b 决策函数只依赖于输入样本的内积 求得了α\\alphaα，当αj>0\\alpha_j>0αj​>0时对应的jjj是support vector（真正对www有用的） "},"id3.html":{"url":"id3.html","title":"决策树ID3及C4.5算法","keywords":"","body":"引言 决策树（Decision Tree）是一种基本的分类与回归算法。决策树是一种树形结构，这在计算机中的数据结构中很常见，也很容易理解：我们选择一种属性对其进行划分，通过其属性的取值不同划分节点，直到最后属性完全相同或不能继续划分为止。 因此，其学习的关键在于如何选择最优划分属性。这也是我们接下来要重点介绍的。 然而，决策树也可以认为给定特征分类下条件概率分布的一种表示。该条件分布定义在特征空间的划分上：特征空间被划分成不相交的单元，每个单元定义一个类的概率分布就构成了条件概率分布。 相比朴素贝叶斯分类，决策树的优势在于构造过程不需要任何领域知识或参数设置，因此在实际应用中，对于探测式的知识发现，决策树更加适用。 决策树基本流程 可将决策树算法概括为： 特征选择 决策树生成 预（后）剪枝 其中特征选择是我们最为关注的。常见的特征选择算法有ID3、C4.5、CART算法，下面会依次介绍。 信息论基础 不加说明的，我们找来信息论中的熵来定义决策树的决策选择度量。 信息熵 熵度量了事物的不确定性，越不确定的事物，它的熵就越大。定义在当前样本集合DDD中，取值为iii的样本所占的比例为pip_ipi​，SSS为对应的分类类别数，因此可得： ∑kpk=1\\sum _k p_k = 1k∑​pk​=1 定义DDD的熵为： Ent(D)=−∑kpklogPkEnt(D) = - \\sum_k p_k log P_kEnt(D)=−k∑​pk​logPk​ 可验证，当Pk=0P_k = 0Pk​=0 或 Pk=1P_k = 1Pk​=1时Ent(D)Ent(D)Ent(D)最小，当Pk=1SP_k = \\frac{1}{S}Pk​=S1​时值最大。 信息增益 以上是在样本集合DDD上的熵，我们想要知道如何选择属性能使得划分结果熵越小（也就是纯度越高）。 因此我们对于样本的每一个属性aaa，设此属性有TTT种不同的取值，计算其信息增益（information gain）： Gain(D,a)=Ent(D)−∑t=1T∣Dt∣∣D∣Ent(Dt)Gain(D,a) = Ent(D) - \\sum\\limits_{t = 1}^{T}\\frac{|D^t|}{|D|} Ent(D^t)Gain(D,a)=Ent(D)−t=1∑T​∣D∣∣Dt∣​Ent(Dt) 若∑t=1T∣Dt∣∣D∣Ent(Dt)\\sum\\limits_{t = 1}^{T}\\frac{|D^t|}{|D|} Ent(D^t)t=1∑T​∣D∣∣Dt∣​Ent(Dt)越小，Gain(D,a)Gain(D,a)Gain(D,a)越大，则纯度越大。 有了这个度量方式，我们就可以通过求每个属性的信息增益，选择最大的信息增益作为当前的节点，然后依次递归下去。 概率理解 事实上，我们每次选取的属性也是当前随机变量下的条件概率最大值。用概率的方式理解更直观。 设随机变量X的熵的表达式如下： H(X)=−∑i=1npilogpiH(X) = -\\sum\\limits_{i=1}^n p_ilog p_iH(X)=−i=1∑n​pi​logpi​ 其中nnn代表XXX的不同离散取值。 熟悉了一个变量X的熵，很容易推广到多个个变量的联合熵，这里给出两个变量X和Y的联合熵表达式： H(X,Y)=−∑i=1np(xi,yi)logp(xi,yi)H(X,Y) = -\\sum\\limits_{i=1}^n p(x_i,y_i)log p(x_i,y_i)H(X,Y)=−i=1∑n​p(xi​,yi​)logp(xi​,yi​) 因此，我们可以写出条件熵，它度量了我们在知道YYY之后XXX的不确定性： H(X∣Y)=−∑i=1np(xi,yi)logp(xi∣yi)=∑j=1np(yj)H(X∣yj)H(X|Y) = -\\sum\\limits_{i=1}^n p(x_i,y_i)log p(x_i|y_i) = \\sum\\limits_{j = 1} ^{n}p(y_j)H(X|y_j)H(X∣Y)=−i=1∑n​p(xi​,yi​)logp(xi​∣yi​)=j=1∑n​p(yj​)H(X∣yj​) 因此，I(D,Y)=H(X)−H(X∣Y)I(D,Y) = H(X)-H(X|Y)I(D,Y)=H(X)−H(X∣Y)可以定义为在知道YYY之后XXX的不确定减少程度，这正是我们所需要的。仔细观察，这个值与我们之前定义的信息增益是同一个东西，信息增益越大，说明不确定性减少得越多，也就更适合选择作为划分属性。 ID3算法 ID3是实现决策树最原始的算法，其思想在上文已经阐述，就是用信息增益大小来判断当前节点应该用什么特征来构建决策树，用计算出的信息增益最大的特征来建立决策树的当前节点。具体例子大家可以参考下面的参考资料。 缺点 虽然ID3算法通过信息熵给了我们分类的新思路，但依然有很多不足的地方。 ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个属性aaa有2个值，各为1/2，另一个属性bbb为3个值，各为1/3，我们分别计算其熵： Ent(Da)=−(12lg12+12lg12)=lg2Ent(D^a) = -(\\frac{1}{2} lg\\frac{1}{2} + \\frac{1}{2} lg\\frac{1}{2} ) = lg2Ent(Da)=−(21​lg21​+21​lg21​)=lg2 Ent(Da)=−(13lg13+13lg13+13lg13)=lg3Ent(D^a) = -(\\frac{1}{3} lg\\frac{1}{3} + \\frac{1}{3} lg\\frac{1}{3} + \\frac{1}{3} lg\\frac{1}{3}) = lg3Ent(Da)=−(31​lg31​+31​lg31​+31​lg31​)=lg3 其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？ ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 C4.5算法 基于以上的不足，ID3的算法设计者对其进行了改进，这就是C4.5算法。下面我们就分别来看对于每个不足的改进在什么地方。 不能处理连续特征。 C4.5的思路是将连续的特征离散化。比如mmm个样本的连续特征AAA有mmm个，从小到大排列为a1,a2,...,ama1,a2,...,a_ma1,a2,...,am​,则C4.5取相邻两样本值的平均数，一共取得m−1m-1m−1个划分点，其中第iii个划分点TiT_iTi​表示为：Ti=ai+ai+12T_i=\\frac{a_i+a_{i+1}}{2}Ti​=2ai​+ai+1​​。对于这m−1m-1m−1个点，分别计算以该点作为二元分类点时的信息增益。 选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为ata_tat​,则小于ata_tat​的值为类别1，大于ata_tat​的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 信息增益作为标准容易偏向于取值较多的特征的问题。 我们引入一个信息增益比的变量IR(X,Y)I_R(X,Y)IR​(X,Y)，它是信息增益和特征熵的比值。表达式如下： IR(X,A)=I(D,A)HA(D)=Gain(D,A)HA(D)I_R(X,A) = \\frac{I(D,A)}{H_A(D)}= \\frac {Gain(D,A)}{H_A(D)}IR​(X,A)=HA​(D)I(D,A)​=HA​(D)Gain(D,A)​ HA(D)H_A(D)HA​(D)为特征熵： HA(D)=−∑i=1n∣Di∣∣D∣log∣Di∣∣D∣H_A(D) = -\\sum\\limits_{i=1}^n\\frac{|D_i|}{|D|}log\\frac{|D_i|}{|D|}HA​(D)=−i=1∑n​∣D∣∣Di​∣​log∣D∣∣Di​∣​ 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 在样本某些特征缺失的情况下选择划分的属性 对于某一个有缺失特征值的特征AAA，C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值AAA的数据D1D_1D1​，另一部分是没有特征AAA的数据D2D_2D2​. 然后对于没有缺失特征AAA的数据集D1D_1D1​计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征AAA缺失的样本加权后所占加权总样本的比例。 选定了划分属性，对于在该属性上缺失特征的样本的处理 可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。 比如缺失特征A的样本aaa之前权重为1，特征A有3个特征值A1,A2,A3A1,A2,A3A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则aaa同时划分入A1,A2,A3A1,A2,A3A1,A2,A3。对应权重调节为2/9,3/9, 4/9。 没有考虑过拟合的问题 这留到CART算法中的剪枝问题一起解决。 不足 C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间。 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。 这些问题在CART树中得到了改进。我们下一篇再介绍。 参考资料 决策树从原理到算法实现 决策树算法原理 "},"id3 python.html":{"url":"id3 python.html","title":"决策树python实现","keywords":"","body":"本文的大部分代码基于 Machine Learning in Action 一书，原书代码解释较少，自己增加了很多注释，可以在这里下载。 ID3算法实现 构造数据 def createDataSet(): \"\"\" the dataset for test \"\"\" dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] # name of each feature labels = ['no surfacing', 'flippers'] return dataSet, labels 这是一个简单的数据集，只有两个feature，对应两种输出。 信息增益 计算数据集的香农熵 我们首先计算数据集的香农熵（信息熵），根据计算公式： Ent(D)=−∑kpklogPkEnt(D) = - \\sum_k p_k log P_kEnt(D)=−k∑​pk​logPk​ 注意，我们这里的数据以list为数据结构，最后一列为分类结果，前面的都为属性。 def calcShannonEnt(dataSet): ''' the function is used to calculate Shannon Entropy, a method to measure the disorder of data . the dataSet is like [[1,1,'yes'],[1,0,'no'],...] the last index of each vector stands for which class it belong. the feature needs to be discrete and numeric. return shannon Entropy ''' numEntries = len(dataSet) # use lambda to define a function zero when key is not presented labelCounts = defaultdict(lambda: 0) for feaVec in dataSet: currentLabel = feaVec[-1] labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key]) / numEntries shannonEnt -= prob * log(prob, 2) # -\\sum [p*log(p)] return shannonEnt 按照特征划分数据集 根据算法，我们需要计算每个特征的信息增益，因此首先需要对数据集进行划分： def splitDataSet(dataSet, axis, value): \"\"\"split Dataset to get information gain for this axis axis means the fature to calculate using value return the data splited \"\"\" retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducefeatVec = featVec[:] # copy reducefeatVec.pop(axis) # delete axis feature retDataSet.append(reducefeatVec) return retDataSet 选择最好的划分方式 最后，我们针对不同的划分方式计算香农熵，选出最好的划分方式 def chooseBestFeatureToSplit(dataSet): \"\"\" choose best feature according to information gain the data must have the same dimension return the feature chosen \"\"\" numFeatures = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInforGain = 0.0 bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] uniqueVals = set(featList) newEntropy = 0.0 for value in uniqueVals: subDataSet = splitDataSet(dataSet, i, value) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob*calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy if (infoGain > bestInforGain): bestInforGain = infoGain bestFeature = i return bestFeature 递归构建决策树 停止递归条件 当所有类别标签全部相同，则直接返回该标签 当使用完了所有特征，依然不能将数据集进行分类，这里使用投票的方法选出数量最多的标签作为该类。 import operator def majorityCnt(classList): \"\"\" this function is used when the algorithm cannot continue to classfy because all attributes are used majority of class will be chose. return the class chosen \"\"\" classCount = defaultdict(lambda: 0) for vote in classList: classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 构造决策树 注意，我们使用了一个特征之后，该特征就会被删除不再使用。 def createTree(dataSet, labels): \"\"\" recursively calculate the tree delete the label used return tree dictionary \"\"\" classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): # if all data is the same class, return return classList[0] if len(dataSet[0]) == 1: # if cannot continue, return return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] # init label myTree = {bestFeatLabel: {}} # have used the feature del(labels[bestFeat]) # split the data according to feature value featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] # copy myTree[bestFeatLabel][value] = createTree(splitDataSet (dataSet, bestFeat, value), subLabels) return myTree 结果 在这里我们的决策树就已经构造完毕，最后的输出是一个嵌套的字典，可以使用如下代码输出查看： myDat, labels = createDataSet() myTree = createTree(myDat,labels) print(myTree) 可视化 决策树是一个很直观的算法，那么我们如何进行可视化呢？这里需要用到Matplotlib中的很多技巧，详细的解释可以参考这里。 除了绘图技巧，这里也使用了Python中利用函数实现全局变量的技巧。 绘制树节点 # draw myTree import matplotlib.pyplot as plt decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\") leafNode = dict(boxstyle=\"round4\", fc=\"0.8\") arrow_args = dict(arrowstyle=\" 递归计算树高和节点数 绘图之前，首先要计算树高和节点，这样有利于精确绘制。 def getNumleafs(myTree): \"\"\" get the numbers of leafs recursively \"\"\" numLeafs = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': numLeafs += getNumleafs(secondDict[key]) else: numLeafs += 1 return numLeafs def getTreeDepth(myTree): \"\"\" get the depth of tree \"\"\" maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth > maxDepth: maxDepth = thisDepth return maxDepth 绘制决策树 def retrieveTree(i): \"\"\" simple test of data \"\"\" listOfTrees = [{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}, 3: 'maybe'}}, {'no surfacing': {0: 'no', 1: {'flippers': { 0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}, ] return listOfTrees[i] def plotMidText(cntrPt, parentPt, txtString): \"\"\"plot the text in the middle of two nodes \"\"\" xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1] createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30) def plotTree(myTree, parentPt, nodeTxt): \"\"\" recursively plot the tree \"\"\" numLeafs = getNumleafs(myTree) depth = getTreeDepth(myTree) firstStr = list(myTree.keys())[0] cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0/plotTree.totalW, plotTree.yOff) plotMidText(cntrPt, parentPt, nodeTxt) # plot the text plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD # reduce offset of y for key in list(secondDict.keys()): # test to see if the nodes are dictonaires, if not they are leaf nodes if type(secondDict[key]).__name__ == 'dict': plotTree(secondDict[key], cntrPt, str(key)) # recursion else: plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD def createPlot(inTree): \"\"\" main entry of plot\\n calcalate the size of graph and style \"\"\" fig = plt.figure(1, facecolor='white') fig.clf() axprops = dict(xticks=[], yticks=[]) createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) plotTree.totalW = float(getNumleafs(inTree)) # store the width of tree plotTree.totalD = float(getTreeDepth(inTree)) # store the depth of tree plotTree.xOff = -0.5/plotTree.totalW plotTree.yOff = 1.0 plotTree(inTree, (0.5, 1.0), '') plt.show() 结果如图所示： "},"cart.html":{"url":"cart.html","title":"决策树CART算法及剪枝","keywords":"","body":"在上一节我们将了ID3算法，和ID3算法的改进版C4.5算法。对于C4.5算法，我们也提到了它的不足，特别是不能处理连续数据等。而目前最常见的CART既可以做回归，也可以做分类，在skleran包中的决策树也采用此种方法。 特征选择方法 前面无论是ID3或者C4.5算法，我们都是使用的“熵”这一度量单位来选取特征。但计算熵需要大量的对数运算，有没有其他的特征选取方法呢？答案是肯定的，这里使用了统计学中的基尼系数，其基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。 假设在分类问题中有KKK个类别，第kkk个类别的概率为pkp_kpk​，则基尼系数表示为： Gini(p)=∑k=1K∑j≠kKpkpj=∑k=1Kpk(1−pk)=1−∑k=1Kpk2Gini(p) =\\sum\\limits_{k=1}^K\\sum\\limits_{j\\ne k}^Kp_kp_j= \\sum\\limits_{k=1}^Kp_k(1-p_k) = 1 - \\sum\\limits_{k=1}^Kp_k^2Gini(p)=k=1∑K​j≠k∑K​pk​pj​=k=1∑K​pk​(1−pk​)=1−k=1∑K​pk2​ 从直观上，我们可以认为基尼系数是某种属性分类错误的概率度量。 对于给定的样本DDD,假设有KKK个类别, 第kkk个类别的数量为CkC_kCk​,则样本DDD的基尼系数表达式为： Gini(D)=1−∑k=1K(∣Ck∣∣D∣)2Gini(D) = 1 - \\sum\\limits _{k =1 }^{K}(\\frac{|C_k|}{|D|})^2Gini(D)=1−k=1∑K​(∣D∣∣Ck​∣​)2 我们后面会构造二叉树，因此当只有两个节点时，其公式可以表示为： Gini(D)=∣C1∣∣D∣Gini(D1)+∣C2∣∣D∣Gini(D2)Gini(D) = \\frac{|C_1|}{|D|}Gini(D_1)+\\frac{|C_2|}{|D|}Gini(D_2)Gini(D)=∣D∣∣C1​∣​Gini(D1​)+∣D∣∣C2​∣​Gini(D2​) 二叉树 在CART算法中，生成的决策树始终是二叉树。 离散值处理 对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。 若某个特征AAA被选择建立决策树点，有 A1、A2、A3 三种分类，CART分类树会考虑把 A 分成 {A1} 和 {A2,A3} {A1}，{A2} 和 {A1,A3}， {A3} 和 {A1,A2} 三种情况，找到基尼系数最小的组合，比如 {A2} 和 {A1,A3} {A2},然后建立二叉树节点，一个节点是 A2 对应的样本，另一个节点是 {A1,A3} 对应的节点。 同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征 AAA 来划分 A1A1A1 和 A3A3A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。 连续值处理 CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为： min⁡A,s[min⁡c1∑xi∈D1(A,s)(yi−c1)2+min⁡c2∑xi∈D2(A,s)(yi−c2)2]\\min\\limits_{A,s} [\\min\\limits_{c_1} \\sum\\limits_{x_i\\in D_1(A,s) }(y_i-c_1)^2 + \\min\\limits_{c_2} \\sum\\limits_{x_i\\in D_2(A,s) }(y_i-c_2)^2 ]A,smin​[c1​min​xi​∈D1​(A,s)∑​(yi​−c1​)2+c2​min​xi​∈D2​(A,s)∑​(yi​−c2​)2] 对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。 剪枝 由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的返回能力。但是，有很多的剪枝方法，我们应该这么选择呢？ CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。 因此，CART树的剪枝算法可以概括为两步： 第一步是从原始决策树生成各种剪枝效果的决策树， 第二步是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。 损失函数 首先对于是否对一个节点进行剪枝，我们需要一个度量标准，也就是剪枝前后的损失谁更大。 在剪枝的过程中，对于任意的一刻子树TtT_tTt​,其损失函数为： Cα(Tt)=C(Tt)+α∣Tt∣C_{\\alpha}(T_t) = C(T_t) + \\alpha|T_t|Cα​(Tt​)=C(Tt​)+α∣Tt​∣ 其中，α\\alphaα为正则化参数，这和线性回归的正则化一样。 C(Tt)C(T_t)C(Tt​)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。 ∣Tt∣|T_t|∣Tt​∣是子树TtT_tTt​的叶子节点的数量。 当α=0\\alpha=0α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞\\alpha=\\inftyα=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α\\alphaα 越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α\\alphaα，一定存在使损失函数Cα(T)C_{\\alpha}(T)Cα​(T)最小的唯一子树。 若将节点全部剪掉，只保留根节点，则根节点损失是： Cα(T)=C(T)+αC_{\\alpha}(T) = C(T) + \\alphaCα​(T)=C(T)+α 当α=0\\alpha = 0 α=0或者α\\alphaα很小时，很容易得到： Cα(Tt)≤Cα(T)C_{\\alpha}(T_t) \\le C_{\\alpha}(T)Cα​(Tt​)≤Cα​(T) 这表示根节点损失大于叶节点损失。 而我们需要得到根节点损失小于等于叶节点损失，这样我们才能合理的将其剪枝，即： Cα(Tt)=Cα(T)⇒C(T)+α=C(Tt)+α∣Tt∣C_{\\alpha}(T_t) = C_{\\alpha}(T)\\Rightarrow C(T) + \\alpha = C(T_t) + \\alpha|T_t|Cα​(Tt​)=Cα​(T)⇒C(T)+α=C(Tt​)+α∣Tt​∣ α=C(T)−C(Tt)∣Tt∣−1\\alpha = \\frac{C(T) - C(T_t)}{|T_t | -1}α=∣Tt​∣−1C(T)−C(Tt​)​ 此时，TtT_tTt​和TTT有相同的损失函数，但是TTT节点更少，因此可以对子树进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点TTT。 交叉验证 我们可以计算出每个子树是否剪枝的阈值ααα，然后分别针对不同的ααα所对应的剪枝后的最优子树做交叉验证。 这样就可以选择一个最好的ααα，有了这个ααα，我们就可以用对应的最优子树作为最终结果。 算法小结 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类，回归 二叉树 基尼系数，均方差 支持 支持 支持 优点 简单直观，生成的决策树很直观。 基本不需要预处理，不需要提前归一化，处理缺失值。 使用决策树预测的代价是 O(log⁡2m)O(log⁡2m)\\mathcal{O}(\\log{2m})\\mathcal{O}(\\log{2m})O(log2m)O(log2m)。 m为样本数。 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。 可以处理多维度输出的分类问题。 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释 可以交叉验证的剪枝来选择模型，从而提高泛化能力。 对于异常点的容错能力好，健壮性高。 缺点 决策树算法非常容易过拟合，导致泛化能力不强。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。 参考资料 决策树算法原理 "},"svm.html":{"url":"svm.html","title":"SVM入门","keywords":"","body":"分类问题 SVM（support vector machine）是一种著名的分类算法。我们学过Logistic回归，但它只能处理简单的线性分类。在现实生活中，很多问题的属性不能简单的用线性分类完成，或者说线性分类的效果不好，这时候我们就要想其他办法。 超平面 我们可以想象这样一个方程： wTx+b=0w^Tx + b = 0wTx+b=0 若这里的xxx是二维向量，那么就是我们熟悉的平面方程。若大于二维，则是一个超平面，在SVM中，这个超平面也被称为决策面。 我们的目标就是想找到这样一个决策面，使得样本点能够较好的分布在超平面两侧，这就达到了我们分类的目的。 Part1 首先用一种简单的方法得到优化问题。 分类间隔 很显然，对于样本点来说，这样的决策面肯定不止一个。那么，如何来度量我们分类好坏的标准呢？ 在SVM中，我们使用分类间隔来度量，所谓分类间隔，是指保证决策面方向不变且不会错分样本的情况下移动决策面，会在原来的决策面两侧找到两个极限位置（越过则会产生错分现象）。因此，这两条平行线（面）之间的垂直距离就是这个决策面对应的分类间隔。 不同方向的最优决策面通常是不同的，那个具有最大间隔的决策面就是SVM要寻找的最优解。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为支持向量。 根据我们学习过的平面距离可以得到： d=∣wT+b∣∣∣w∣∣d = \\frac {|w^T + b|}{||w||}d=∣∣w∣∣∣wT+b∣​ 我们首先考虑一个决策面是否能够将所有样本都正确分类的约束。我们可以为每个样本点xix_ixi​加上一个类别标签yi={−1,1}y_i = \\{-1,1\\}yi​={−1,1}，假如我们的决策面方程能够完全正确的对所有样本点进行分类，则可以得到： f(x)={wTxi+b>0 for yi=1wTxi+b0 for yi=−1 f(x)=\\left\\{\\begin{aligned}w^Tx_i + b >0 \\space for \\space y_i = 1 \\\\w^Tx_i + b f(x)={wTxi​+b>0 for yi​=1wTxi​+b0 for yi​=−1​ 如果我们要求再高一点，假设决策面正好处于间隔区域的中轴线上，并且相应的支持向量对应的样本点到决策面的距离为d，那么公式可以进一步写成： f(x)={(wTxi+b)/∣∣w∣∣≥d ∀yi=1(wTxi+b)/∣∣w∣∣≤−d ∀yi=−1 f(x)=\\left\\{\\begin{aligned}(w^Tx_i + b )/||w||\\ge d \\space \\forall y_i = 1 \\\\ (w^Tx_i + b )/||w||\\le -d \\space \\forall y_i = -1\\end{aligned}\\right.f(x)={(wTxi​+b)/∣∣w∣∣≥d ∀yi​=1(wTxi​+b)/∣∣w∣∣≤−d ∀yi​=−1​ 对公式重写（两边同时除以d）： f(x)={wdTxi+bd≥1 for yi=1wdTxi+bd≤1 for yi=−1wd=w∣∣w∣∣d,bd=b∣∣w∣∣d f(x)=\\left\\{\\begin{aligned}w_d^Tx_i + b_d \\ge1 \\space for \\space y_i = 1 \\\\w_d^Tx_i + b_d \\le1 \\space for \\space y_i = -1\\end{aligned}\\right. \\quad w_d = \\frac{w}{||w||d},b_d = \\frac {b}{||w||d}f(x)={wdT​xi​+bd​≥1 for yi​=1wdT​xi​+bd​≤1 for yi​=−1​wd​=∣∣w∣∣dw​,bd​=∣∣w∣∣db​ 由于wdw_dwd​与www并没有本质差别，因此不再做区分，我们的目标是想要在正确分类的情况下使得分类间隔最大化，即max{d}max \\{d\\}max{d}，也就是min⁡∣wdTxi+bd∣∣∣wd∣∣≥1∣∣wd∣∣\\min {\\frac{|w_d^Tx_i +b_d|}{||w_d||}} \\ge \\frac{1}{||w_d||}min∣∣wd​∣∣∣wdT​xi​+bd​∣​≥∣∣wd​∣∣1​，也等价于min{12∣∣wd∣∣2}min \\{ \\frac {1}{2} ||w_d||^2\\}min{21​∣∣wd​∣∣2}。 因此，我们得到我们问题的总描述： min⁡12∣∣w∣∣2\\min \\frac {1}{2} ||w||^2min21​∣∣w∣∣2 s.t.yi(wTxi+b)≥1,i=1,...,ns.t. \\quad y_i(w^Tx_i +b)\\ge1,i=1,...,ns.t.yi​(wTxi​+b)≥1,i=1,...,n Part2 使用margin得到优化问题。 margins Functional margins γ(i)^=y(i)(wTx+b)\\hat{\\gamma^{(i)}} = y^{(i)}(w^Tx + b)γ(i)^​=y(i)(wTx+b) 这个函数可以用来衡量confident和correct 如果分类正确，那么该函数始终是正数，且离决策边界越远，值越大，也就越confident 如果分类错误，那么该函数是负数 因此，我们的目标是找到最小的margin，也就是 γ^=min⁡γ(i)^\\hat{\\gamma} = \\min \\hat{\\gamma^{(i)}}γ^​=minγ(i)^​ Geometric margins Functional margins有一个很大的问题在于，如果我等比例的scale w,bw,bw,b，那么该值就一定会增大。但此时对于margin来说并没有提升，因此无法直接用来衡量。 我们新定义一个Geometric margins，可以认为是一个相对的大小： γ(i)=y(i)(w∣∣w∣∣Tx+b∣∣w∣∣)\\gamma^{(i)} = y^{(i)}(\\frac{w}{||w||}^Tx + \\frac{b}{||w||})γ(i)=y(i)(∣∣w∣∣w​Tx+∣∣w∣∣b​) 实际上，γ=γ^∣∣w∣∣\\gamma = \\frac{\\hat{\\gamma}}{||w||}γ=∣∣w∣∣γ^​​ 我们的目标不变: γ=min⁡γ(i)\\gamma = \\min \\gamma^{(i)}γ=minγ(i) 问题表述为： max⁡γ\\max \\gammamaxγ s.t.y(i)(wTx(i)+b)≥γs.t.\\quad y^{(i)}(w^Tx^{(i)} + b )\\ge \\gammas.t.y(i)(wTx(i)+b)≥γ ∣∣w∣∣=1||w|| = 1∣∣w∣∣=1 很容易证明，这时候无论w,bw,bw,b 如何 scale，都不会影响margins了。（类似于normalization） 但由于有∣∣w∣∣=1||w|| = 1∣∣w∣∣=1的条件，类似于在球面上进行优化，不是一个凸优化问题，很难求解。 Optimal margin classifier 我们的目标是最大化geometry margins，因此可以将原问题写为： max⁡γ∣∣w∣∣\\max \\frac{\\gamma}{||w||}max∣∣w∣∣γ​ s.t.y(i)(wTx(i)+b)≥γs.t. y^{(i)}(w^Tx^{(i)} + b) \\ge \\gammas.t.y(i)(wTx(i)+b)≥γ 但这依然不容易求解，联想到，我们已经使得无论w,bw,bw,b 如何 scale，都不会影响最终的值，因此，总是可以使w,bw,bw,b 满足γ=1\\gamma = 1γ=1，因此，我们的目标函数可以写为max⁡1∣∣w∣∣\\max \\frac{1}{||w||}max∣∣w∣∣1​。注意到，最大化1∣∣w∣∣\\frac{1}{||w||}∣∣w∣∣1​和最小化∣∣w∣∣2||w||^2∣∣w∣∣2是一回事情（更容易求导），因此，我们将原问题转为了凸优化问题（没有局部最优值）： min⁡12∣∣w∣∣2\\min \\frac {1}{2} ||w||^2min21​∣∣w∣∣2 s.t.yi(wTxi+b)≥1,i=1,...,ns.t. \\quad y_i(w^Tx_i +b)\\ge1,i=1,...,ns.t.yi​(wTxi​+b)≥1,i=1,...,n 线性可分情况 拉格朗日函数 这是一个有约束条件的极值问题，因此可使用拉格朗日函数表达： L(w,b,a)=12∣∣w∣∣2−∑i=1nαi(yi(wTxi+b)−1)L(w,b,a) =\\frac {1}{2} ||w||^2 - \\sum\\limits _{i=1}^n \\alpha_i(y_i(w^Tx_i +b)-1) L(w,b,a)=21​∣∣w∣∣2−i=1∑n​αi​(yi​(wTxi​+b)−1) 我们令αi≥0,θ(w)=maxai≥0L(w,b,a)\\alpha_i \\ge 0,\\theta(w) = {max}_{a_i \\ge 0} L(w,b,a) αi​≥0,θ(w)=maxai​≥0​L(w,b,a)。容易验证：当某个约束条件不满足时，例如yi(wTxi+b)1y_i(w^Tx_i +b)yi​(wTxi​+b)1，则有θ(w)=∞\\theta(w) = \\infty θ(w)=∞（只要令αi=∞\\alpha _i = \\inftyαi​=∞而当所有约束都满足时，则有θ(w)=12∣∣w∣∣2\\theta(w) = \\frac {1}{2} ||w||^2θ(w)=21​∣∣w∣∣2，即为最初要最小化的量。 这样，我们就使用拉格朗日函数将所有约束条件集中到一个函数中，目标函数变成了： min⁡w,bθ(w)=min⁡w,bmax⁡αi≥0L(w,b,a)=p∗\\min\\limits_{w,b}\\theta(w) = \\min\\limits_{w,b} \\max\\limits_{\\alpha_i\\ge 0}L(w,b,a) = p^*w,bmin​θ(w)=w,bmin​αi​≥0max​L(w,b,a)=p∗ 这里用p∗p^*p∗表示这个问题的最优解，且与最初的问题是等价的，但如果直接面对这个函数，有w,bw,bw,b两个参数，并且α\\alphaα还是不等式约束，不好求解。那么我们可以转化为对偶问题： max⁡αi≥0min⁡w,bL(w,b,a)=d∗\\max\\limits_{\\alpha_i\\ge 0}\\min\\limits_{w,b} L(w,b,a) = d^*αi​≥0max​w,bmin​L(w,b,a)=d∗ 这个新问题的最优解表示为d⋆d^\\stard⋆，且有d⋆≤p⋆d^{\\star} \\le p^{\\star}d⋆≤p⋆（总是成立）,在某些情况下这两者相等，因此可以求解对偶问题来间接求解原始问题。 KKT条件 由于对偶问题和原始问题有 d⋆≤p⋆d^{\\star} \\le p^{\\star}d⋆≤p⋆ 的关系，但我们更希望取等号，这样我们就可以利用对偶问题来求得原问题的最优解。 而满足这种条件的约束称为KKT条件。 首先重新定义一下凸优化问题： min⁡f(w)\\min f(w)minf(w) s.t.gi(w)≤0s.t. \\quad g_i(w) \\le 0s.t.gi​(w)≤0 hi(w)=0h_i(w) = 0hi​(w)=0 拉格朗日函数可以表示为：L(w,α,β)=f(w)+∑αigi(w)+∑βih(w)L(w,\\alpha,\\beta) = f(w) + \\sum \\alpha_i g_i(w) + \\sum \\beta_i h(w)L(w,α,β)=f(w)+∑αi​gi​(w)+∑βi​h(w) KKT条件可以表示为： ∂∂wiL(w∗,α∗,β∗)=0\\frac{\\partial}{\\partial w_i}L(w^*,\\alpha^*,\\beta^*) = 0∂wi​∂​L(w∗,α∗,β∗)=0 ∂∂βiL(w∗,α∗,β∗)=0\\frac{\\partial}{\\partial \\beta_i}L(w^*,\\alpha^*,\\beta^*) = 0∂βi​∂​L(w∗,α∗,β∗)=0 αi∗gi∗(w∗)=0\\alpha_i^* g_i^*(w^*) = 0αi∗​gi∗​(w∗)=0 gi(w∗)≤0g_i(w^*) \\le 0gi​(w∗)≤0 α∗≥0\\alpha^* \\ge 0α∗≥0 其中第三个条件被称为dual complementarity condition，也就是说，只有在gi⋆(w⋆)=0g_i^{\\star}(w^{\\star}) = 0gi⋆​(w⋆)=0时α≠0\\alpha \\ne 0α≠0，也就是真正作为support vector，在后面的SMO中会有帮助。 对偶问题求解 我们需要求解的方程为： max⁡αi≥0min⁡w,bL(w,b,a)=d∗\\max\\limits_{\\alpha_i\\ge 0}\\min\\limits_{w,b} L(w,b,a) = d^*αi​≥0max​w,bmin​L(w,b,a)=d∗ 首先固定α\\alphaα，对w,bw,bw,b求导数： ∂L∂w=0⇒w=∑i=1naiyixi\\frac{\\partial L }{\\partial w} = 0 \\Rightarrow w =\\sum\\limits_{i=1}^{n}a_iy_ix_i∂w∂L​=0⇒w=i=1∑n​ai​yi​xi​ ∂L∂b=0⇒∑i=1naiyi=0\\frac{\\partial L }{\\partial b} = 0 \\Rightarrow\\sum\\limits_{i=1}^{n}a_iy_i = 0∂b∂L​=0⇒i=1∑n​ai​yi​=0 将上面的结果带到L(w,b,a)L(w,b,a)L(w,b,a)中可得： L(w,b,a)=12∥w∥2−∑i=1nαi(yi(wTxi+b)−1)=12∥w∥2−∑i=1naiyiwTxi−∑i=1naiyib+∑i=1nai=−12∑i,j=1naiajyiyjxT−ixj+∑i=1nai \\begin{aligned} L(w, b, a) &=\\frac{1}{2}\\|w\\|^{2}-\\sum_i=1^{n} \\alpha_ i\\left(y_ i\\left(w^{T} x_ i+b\\right)-1\\right) \\\\ &=\\frac{1}{2}\\|w\\|^{2}-\\sum_ i=1^{n} a_ i y_ i w^{T} x_ i-\\sum_ i=1^{n} a_ i y_ i b+\\sum_ i=1^{n} a_ i \\\\ &=-\\frac{1}{2} \\sum_ i, j=1^{n} a_i a_ j y_ i y_ j x^{T}-i x_ j+\\sum_ i=1^{n} a_ i \\end{aligned} L(w,b,a)​=21​∥w∥2−i∑​=1nαi​(yi​(wTxi​+b)−1)=21​∥w∥2−i∑​=1nai​yi​wTxi​−i∑​=1nai​yi​b+i∑​=1nai​=−21​i∑​,j=1nai​aj​yi​yj​xT−ixj​+i∑​=1nai​​ 这样，我们的目标函数就变为： max⁡α∑i=1nai−12∑i,j=1naiajyiyjxiTxj\\max\\limits_{\\alpha} \\sum\\limits _{i= 1}^n a_i-\\frac {1}{2} \\sum\\limits_{i,j = 1}^n a_ia_jy_iy_jx^T_ix_j αmax​i=1∑n​ai​−21​i,j=1∑n​ai​aj​yi​yj​xiT​xj​ s.t.ai≥0,i=1,...,and∑i=1naiyi=0s.t. \\quad a_i\\ge0,i=1,..., and \\sum\\limits_{i=1}^{n}a_iy_i = 0s.t.ai​≥0,i=1,...,andi=1∑n​ai​yi​=0 这样，我们的目标就变成了求α\\alphaα，从而可以求出： w=∑i=1naiyixiw =\\sum\\limits_{i=1}^{n}a_iy_ix_iw=i=1∑n​ai​yi​xi​ b=−max⁡i:yi=−1wTx+min⁡i:yi=1wTxi2b = - \\frac {\\max _{i:y_i = -1} w^Tx + \\min_{i:y_i = 1}w^Tx _i}{2}b=−2maxi:yi​=−1​wTx+mini:yi​=1​wTxi​​ 求α\\alpha α比直接求w,bw,bw,b简单多了，其中SMO算法是目前最常用的，我们之后再说。 我们目前的分类函数为f(x)=wTx+bf(x) = w^Tx+bf(x)=wTx+b，带入： f(x)=(∑αiyixi)Tx+b=∑αiyi(xi,xj)+b\\begin{aligned} f(x) &=\\left(\\sum \\alpha_i y_i x_ i\\right)^{T} x+b \\\\ &=\\sum \\alpha_i y_i\\left(x_i, x_j\\right)+b \\end{aligned}f(x)​=(∑αi​yi​xi​)Tx+b=∑αi​yi​(xi​,xj​)+b​ 注意，这里的(xi,xj)(x_i,x_j)(xi​,xj​)表示向量乘积，因此，对于新点xxx，只需要计算它与训练数据点的内积即可。这一点在之后的kernel函数中也会使用。 同时，如果我们的αi≠0\\alpha_i \\ne 0αi​≠0，说明该变量对www有贡献，因此为support vector（可以直接从KKT条件中的function margin=1得到）。 线性不可分情况 核函数 将attributes -> feature 的过程定义为feature mapping，例如 ϕ(x)=[xx2x3]\\phi(x) = \\begin{bmatrix}x\\\\x^2 \\\\ x^3\\end{bmatrix}ϕ(x)=⎣⎡​xx2x3​⎦⎤​ 因此，我们想从feature中进行学习，而不是原始的attributes。而注意到，我们对样本的预测只与内积有关，因此可以定义Kernel：K(x,z)=ϕ(x)Tϕ(z)K(x,z) = \\phi(x)^T\\phi(z)K(x,z)=ϕ(x)Tϕ(z) 这样，在原始算法中的所有内积都用Kernel代替，这样就实现了从feature中学习 这里最值得注意的是，为什么我们不直接学习feature的表示，而要学习kernel呢？ 因为kernel的计算代价可能远远小于提取feature 例如，如果K(x,z)=(xTz+c)dK(x,z) = (x^Tz +c)^dK(x,z)=(xTz+c)d，则对应于Cn+dnC_{n+d}^nCn+dn​个feature space，而对于计算kernel来说，复杂度只有O(n)O(n)O(n) 这种kernel的思想并不仅仅适用于SVM，只要有内积的形式，都可以使用，可以大大减少feature空间的维度 直觉来说，如果ϕ(x)\\phi(x)ϕ(x)和ϕ(z)\\phi(z)ϕ(z)越相近，则我们希望得到的K(x,z)K(x,z)K(x,z)越大，反之越小 例如Gaussian kernel：K(x,z)=exp⁡(−∣∣x−z∣∣2wσ2)K(x,z) = \\exp(-\\frac{||x-z||^2}{w\\sigma^2})K(x,z)=exp(−wσ2∣∣x−z∣∣2​) correspond to an infinite dimensional feature mapping 正则化&不可分 当我们用ϕ\\phiϕ将数据映射到高维特征空间，并不能提高线性可分的likelihood。同时，如果样本中存在outlier，会大大影响我们分类的效果和margin的大小。 因此，我们希望模型能够对outlier不敏感，加上正则项(l1l_1l1​正则化）： min⁡γ,w,b12∣∣w∣∣2+C∑i=1mεi\\min_{\\gamma,w,b} \\frac{1}{2}||w||^2 + C\\sum\\limits_{i=1}^m\\varepsilon_iγ,w,bmin​21​∣∣w∣∣2+Ci=1∑m​εi​ s.ty(i)(wTx(i)+b)≥1−εis.t\\quad y^{(i)}(w^Tx^{(i)} + b) \\ge 1- \\varepsilon_i s.ty(i)(wTx(i)+b)≥1−εi​ εi≥0\\varepsilon_i \\ge 0εi​≥0 CCC越大，对误分类对惩罚越大，模型越复杂，泛化能力差。 这样，我们的拉格朗日问题就变为： L(w,b,ε,α,r)=12wTw+C∑i=1mεi−∑i=1mαi[y(i)(xTw+b−1+εi)]−∑i=1mriεiL(w,b,\\varepsilon,\\alpha,r) = \\frac{1}{2} w^Tw + C\\sum\\limits_{i=1}^m\\varepsilon _i - \\sum\\limits_{i=1}^m\\alpha_i[y^{(i)}(x^Tw + b - 1 +\\varepsilon_i)] - \\sum\\limits_{i=1}^m r_i \\varepsilon_iL(w,b,ε,α,r)=21​wTw+Ci=1∑m​εi​−i=1∑m​αi​[y(i)(xTw+b−1+εi​)]−i=1∑m​ri​εi​ 通过同样的方法，可以得到拉格朗日对偶问题为： max⁡α∑i=1nai−12∑i,j=1naiajyiyj(xi(i),x(j))\\max\\limits_{\\alpha} \\sum\\limits _{i= 1}^n a_i-\\frac {1}{2} \\sum\\limits_{i,j = 1}^n a_ia_jy_iy_j(x^{(i)}_i,x^{(j)})αmax​i=1∑n​ai​−21​i,j=1∑n​ai​aj​yi​yj​(xi(i)​,x(j)) s.t.0≤α≤Cs.t. \\quad 0\\le \\alpha \\le Cs.t.0≤α≤C ∑i=1mαiy(i)=0\\sum\\limits_{i=1}^m \\alpha_i y^{(i)} = 0i=1∑m​αi​y(i)=0 根据KKT条件，我们可以得到 αi=0⇒y(i)(wTx(i)+b)≥1\\alpha_i = 0 \\quad \\Rightarrow\\quad y^{(i)}(w^Tx^{(i)} + b) \\ge 1αi​=0⇒y(i)(wTx(i)+b)≥1 αi=C⇒y(i)(wTx(i)+b)≤1\\alpha_i = C \\quad \\Rightarrow\\quad y^{(i)}(w^Tx^{(i)} + b) \\le 1αi​=C⇒y(i)(wTx(i)+b)≤1 0αiC⇒y(i)(wTx(i)+b)=100αi​C⇒y(i)(wTx(i)+b)=1 SMO算法 我们已经将SVM的基本问题从attributes空间通过kernel转到feature空间，同时定义了有正则项的对偶函数，最后剩下的就是如何求解了。 Coordinate ascent 我们之前已经熟悉了gradient ascent和Newton's method两种优化算法，现在介绍一种新的优化方法。 假设我们的优化目标是max⁡αW(α1,α2,...,αm)\\max\\limits_{\\alpha} W(\\alpha_1,\\alpha_2,...,\\alpha_m)αmax​W(α1​,α2​,...,αm​) 那么，我们按照一定的order对某些变量依次进行更新（从启发式算法角度考虑，我们的更新order是从希望更新的参数变化最大的开始）： αi:=arg⁡max⁡αi^W(α1,α2,..αi.,αm)\\alpha_i := \\arg\\max_{\\hat{\\alpha_i}} W(\\alpha_1,\\alpha_2,..\\alpha_i.,\\alpha_m)αi​:=argαi​^​max​W(α1​,α2​,..αi​.,αm​) 这种优化算法非常有效，收敛得很快。 SMO max⁡α∑i=1nai−12∑i,j=1naiajyiyj(x(i),x(j))\\max\\limits_{\\alpha} \\sum\\limits _{i= 1}^n a_i-\\frac {1}{2} \\sum\\limits_{i,j = 1}^n a_ia_jy_iy_j(x^{(i)},x^{(j)})αmax​i=1∑n​ai​−21​i,j=1∑n​ai​aj​yi​yj​(x(i),x(j)) s.t.0≤α≤Cs.t. \\quad 0\\le \\alpha \\le Cs.t.0≤α≤C ∑i=1mαiy(i)=0\\sum\\limits_{i=1}^m \\alpha_i y^{(i)} = 0i=1∑m​αi​y(i)=0 我们如果直接对满足约束条件的优化问题使用coordinate ascent，则会发现，如果我们需要更新的α1\\alpha_1α1​，在约束条件下，没有办法得到更新后的值。这是因为： α1y(1)=−∑i=2mαiy(i)\\alpha_1y^{(1)} = - \\sum\\limits_{i=2}^m \\alpha_i y^{(i)}α1​y(1)=−i=2∑m​αi​y(i) 因此，解决该问题，至少需要我们同时更新两个值。 首先，如果我们同时更新α1,α2\\alpha_1,\\alpha_2α1​,α2​，则约束条件为： α1y(1)+α2y(2)=−∑i=3mαiy(i)=ε\\alpha_1 y^{(1)} + \\alpha_2y^{(2)} = -\\sum\\limits_{i=3}^m \\alpha_i y^{(i)} = \\varepsilonα1​y(1)+α2​y(2)=−i=3∑m​αi​y(i)=ε 实际上，由于0≤α≤C0\\le \\alpha \\le C0≤α≤C，因此可以更进一步得到其范围： 带入目标函数为： W(α1,α2,...,αm)=W((ε−α2y(2)),α2,...,αm)W(\\alpha_1,\\alpha_2,...,\\alpha_m) = W((\\varepsilon - \\alpha_2y^{(2)}),\\alpha_2,...,\\alpha_m)W(α1​,α2​,...,αm​)=W((ε−α2​y(2)),α2​,...,αm​) 实际上，根据我们之前写的WWW的具体形式，这里就是一个关于α2\\alpha_2α2​的二次型函数：aα22+bα2+ca \\alpha_2^2 + b\\alpha_2 + caα22​+bα2​+c，同时满足某些约束L≤α2≤HL\\le \\alpha_2 \\le HL≤α2​≤H，这样我们很容易就可以求得更新后的α2\\alpha_2α2​的值。 这样，我们就可以按照coordinate ascent的方式依次更新所有的参数，直到收敛。 Reference 从另一个方面看SVM的损失函数和正则项 Regularization perspectives on support-vector machines "},"lr.html":{"url":"lr.html","title":"线性回归与分类","keywords":"","body":"LMS 先构造线性函数进行拟合：h(x)=θ0+θ1x1+θ2x2h(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2h(x)=θ0​+θ1​x1​+θ2​x2​ 定义cost function：J(θ)=12∑(h(xi)−yi)2J(\\theta) = \\frac{1}{2} \\sum (h(x^{i}) - y^i)^2J(θ)=21​∑(h(xi)−yi)2 因此，可使用梯度下降进行求解 gradient descent algorithm：θi:=θj−α∂∂θjJ(θ)\\theta_i := \\theta_j - \\alpha\\frac{\\partial} {\\partial \\theta_j} J(\\theta)θi​:=θj​−α∂θj​∂​J(θ) LMS update rule(Widrow-Hoff learning rule)： θj:=θj+α(yi−h(xi))xji\\theta_j := \\theta_j +\\alpha (y^i - h(x^i))x_j^iθj​:=θj​+α(yi−h(xi))xji​ Bath gradient descent This method looks at every example in the entire training set on every step stochastic gradient descent (also incremental gradient descent) Repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only Matrix derivatives θ=(XTX)−1XTy\\theta = (X^TX)^{-1} X^T yθ=(XTX)−1XTy Probabilistic interpretation assume ϵ∼N(0,σ2),yi=θTxi+ϵi\\epsilon \\sim N(0,\\sigma^2), y^i = \\theta^T x^i +\\epsilon^iϵ∼N(0,σ2),yi=θTxi+ϵi 可通过likelihood的方式得到最优解其实就是最小化least square cost min⁡12∑(yi−θTxi)2\\min \\frac{1}{2}\\sum (y^i - \\theta^T x^i)^2min21​∑(yi−θTxi)2 注意，这里对θ\\thetaθ的假设中，与正态分布中的方差大小无关。 Locally weighted linear regression 这是一种非参数模型 在普通的线性拟合中，我们的参数是固定的 而在locally weighted线性模型中，参数是随着训练集合进行增长的（Loess），可以不让我们担心如何来确定feature（在局部进行线性回归） min⁡∑wi(yi−θTxi)2\\min \\sum w^i(y^i - \\theta^T x^i)^2min∑wi(yi−θTxi)2 wi=exp⁡(−(xi−x)22τ2)w^i = \\exp(-\\frac{(x^i - x)^2}{2\\tau ^2})wi=exp(−2τ2(xi−x)2​) 离该样本越近，则权重越大（趋近1），可以看成在局部进行线性回归（局部权重基本不变） 与KNN的关系？ logistic regression sigmoid function: g(z)=11+e−zg(z) = \\frac{1}{1+e^{-z}}g(z)=1+e−z1​ g′(z)=g(z)(1−g(z))g'(z) = g(z)(1-g(z))g′(z)=g(z)(1−g(z)) 同样可以用likelihood得到 l(θ)=∑yilog⁡h(xi)+(1−yi)log⁡(1−h(xi))l(\\theta) = \\sum y^i \\log h(x^i) + (1-y^i)\\log (1-h(x^i))l(θ)=∑yilogh(xi)+(1−yi)log(1−h(xi)) using gradient ascent θj:=θj+α(yi−h(xi))xji\\theta _j := \\theta_j + \\alpha(y^i - h(x^i)) x_j^iθj​:=θj​+α(yi−h(xi))xji​ 同时，我们还可以用Newton法来找最小值 我们想要找极大值点，也就是一阶导数为0，因此： θ:θ−l′(θ)l′′(θ)\\theta: \\theta - \\frac{l^{'}(\\theta)}{l^{''}(\\theta)}θ:θ−l′′(θ)l′(θ)​ 写成矩阵的形式：θ:=θ−H−1▽l(θ)\\theta := \\theta - H^{-1}\\bigtriangledown l(\\theta)θ:=θ−H−1▽l(θ)，其中Hessian矩阵为 Hij=∂l2(θ)∂θi∂θjH_{ij} = \\frac{\\partial l^2(\\theta)}{\\partial \\theta_i \\partial \\theta_j}Hij​=∂θi​∂θj​∂l2(θ)​ 在数据量较小时比gradient ascent收敛快，但计算Hessian困难 Generalized Linear Model 首先介绍exponential family： p(y,η)=b(y)exp⁡(ηTT(y)−a(η))p(y,\\eta) = b(y) \\exp (\\eta^TT(y) - a(\\eta))p(y,η)=b(y)exp(ηTT(y)−a(η)) 很容易可以证明，无论是分类问题（multinomial）还是回归问题（正态分布），都可以转换为指数族的形式 通过指数族的形式，我们可以发现，在线性假设下，我们之前的logistic回归的sigmoid方程其实就是给定x下y的Bernoulli分布。 因此，为什么我们之前要选择sigmoid函数呢？ 因为其广义线性模型的指数族形式的充分统计量的canonical形式就是sigmoid函数。 softmax function 可通过multinomial的指数族形式可以得到：ϕi=eiη∑ejη\\phi_i = \\frac{e^\\eta_i}{\\sum e^\\eta_j}ϕi​=∑ejη​eiη​​ 可以认为是logistic regression的推广 "},"bayes.html":{"url":"bayes.html","title":"生成模型贝叶斯","keywords":"","body":"生成模型和判别模型 discriminative learning algorithms，也就是根据特征值来求结果的概率。 可以表示为P(y∣x;θ)P(y|x;\\theta)P(y∣x;θ)，在参数确定的条件下，直接求得在当前样本feature下的y的概率 实际上是求条件概率 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。 generative learning algorithms，根据yyy不同来学习不同的模型，然后取概率较大的那个 可以表示为P(x∣y)P(x|y)P(x∣y)，很容易看出，使用贝叶斯公式可将两个模型进行统一： P(y∣x)=p(x∣y)p(y)p(x)P(y|x) = \\frac{p(x|y)p(y)}{p(x)}P(y∣x)=p(x)p(x∣y)p(y)​ 由p(x∣y)∗p(y)=p(x,y)p(x|y) *p(y) = p(x,y)p(x∣y)∗p(y)=p(x,y)可得，实际上生成模型是在求联合概率 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine 等。 这篇博客较为详细地介绍了两个模型。 Gaussian discriminant analysis 首先定义了multivariate normal distribution，实际上就是在单变量正态分布上进行了一点点直观的变换 p(x;μ,∑)=1(2π)n/2∣∑∣1/2exp⁡(−12(x−μ)T∑−1(x−μ))p(x;\\mu,\\sum) = \\frac{1}{(2\\pi)^{n/2}|\\sum|^{1/2}}\\exp(-\\frac{1}{2}(x-\\mu)^T\\sum^{-1}(x-\\mu))p(x;μ,∑)=(2π)n/2∣∑∣1/21​exp(−21​(x−μ)T∑−1(x−μ)) 很容易得到，E(X)=μCov(X)=∑E(X) = \\mu \\quad Cov(X) = \\sumE(X)=μCov(X)=∑ 现在假设输入特征xxx是连续性随机变量，因此假设： y∼Bernoulli(ϕ)y \\sim Bernoulli(\\phi)y∼Bernoulli(ϕ) x∣y=0∼N(μ0,∑)x|y= 0 \\sim N(\\mu_0,\\sum)x∣y=0∼N(μ0​,∑) x∣y=1∼N(μ1,∑)x|y= 1 \\sim N(\\mu_1,\\sum)x∣y=1∼N(μ1​,∑) 需要注意的是，在yyy不同的条件下，其均值不相同，但方差是相同的。 因此，我们可以用likelihood得到参数μ0,μ1,ϕ\\mu_0, \\mu_1,\\phiμ0​,μ1​,ϕ的估计值 Difference between GDA&logistic 如果我们根据贝叶斯公式对GDA求后验概率，可以得到就是sigmoid函数的形式 同时，我们可以发现，只要假设是指数族的形式，后验概率都可是写成sigmoid函数的形式 但是，如果是sigmoid函数形式的后验分布，并不能得到GDA 因此，GDA有着更强的假设 在模型符合该假设的前提下，效果比logistic更好 logistic模型的robust更好 可以发现，如果x的条件概率不满足正态分布，而是posisson分布，也能得到sigmoid函数 Naive Bayes 朴素贝叶斯是一种生成式模型，根据现有的数据，使用likelihood计算条件概率的估计，从而得到模型参数。当需要进行预测时，就使用贝叶斯rule进行预测，取最大的那个yyy 朴素贝叶斯方法有个致命的缺点就是对数据稀疏问题过于敏感 当样本中的数据量不够多，不能包含预测的信息时，这时候所有的概率都为0，无法进行预测 因此需要使用平滑的方法，常用的就是laplace平滑 ϕj=∑1{zi=j}+1m+k\\phi_j = \\frac{\\sum 1\\{z^{i} =j \\}+1}{m+k}ϕj​=m+k∑1{zi=j}+1​ notes 中还介绍了在文本分类中，multi-variate Bernoulli event model 和multinomial event model的区别，如果从词典入手，那么该词在不在这个text中是两点分布；而从text入手，这个位置是某个词是多项分布。 multinomial event model 考虑到了词在邮件中的数量，效果更好。 由于在Naive Bayes中，我们是基于Generative model，同时，用bernoulli或者multinomial分布进行估计（都是指数族分布），因此其充分统计量改写后依然为sigmoid函数。 "},"gbdt.html":{"url":"gbdt.html","title":"GBDT/XGboost 小记","keywords":"","body":"集成学习 个体学习器存在强依赖关系，必须串行生成的序列化方法： Boosting 个体学习器不存在强依赖，可同时生成的并行化方法： Bagging & 随机森林 AdaBoost AdaBoost，是英文\"Adaptive Boosting\"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 具体说来，整个Adaboost 迭代算法就3步： 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 Bagging与随机森林 根据Hoeffding定律，如果基学习器的误差相互独立，则集成学习的错误率以指数级别下降，因此，基本想法是希望做到个体学习器尽可能相互独立。 Bagging 基于Bootstrap方法，每次采样后将样本放入数据集（有可能再次被选中）（选中的样本使用自然对数逼近，为63%左右），然后对每个采样集合训练一个基学习器。 预测时，对分类问题采用简单投票法，对回归问题使用平均法。 同时，由于使用Boosttrap方法，可以用剩下的样本估计其泛化能力。 随机森林 传统决策树在选择划分属性时是在当前属性集合中选择一个最优属性，在RF中，随机选择k个属性集合，再选择最优属性进行分类。 与Bagging的区别：Bagging只对对样本进行了随机扰动，而随机森林对属性也进行了随机扰动。 GBDT Gradient boosting(GB) Gradient boosting的思想是迭代生多个（m个）弱的模型，然后将每个弱模型的预测结果相加，每一次都希望损失函数减小得尽可能快，也就是用负梯度方向 计算负梯度，用负梯度近似残差 GBDT 在每一轮的迭代时对每个样本都会有一个预测值，此时的损失函数为均方差损失函数： l(yi,yi)=12(yi−yi)2l(y_i,y^i) = \\frac{1}{2}(y_i - y^i)^2l(yi​,yi)=21​(yi​−yi)2 负梯度为：−∂l(yi,yi)∂yi=(yi−yi)- \\frac{\\partial l(y_i,y^i)}{\\partial y^i} = (y_i - y^i)−∂yi∂l(yi​,yi)​=(yi​−yi) 因此，当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 - 当前模型预测的值），即残差。此时的变量是yiy^iyi，即“当前预测模型的值”，也就是对它求负梯度。 如何选择特征？ 遍历每个特征，对特征的每一个切分点进行选择，选择使得均方差最小的作为分裂点。 如何面对二分类问题？ 首先将类别变为01，每次训练一棵树，得到的结果用logistic得到概率，并通过loss function拟合残差作为下一棵树的输入 如何面对多分类问题？ 首先有K类，就训练K树。每个类的结果用一个向量表示（one-hot），例如[0,1,0]表示第二类。 每轮训练都同时训练K颗树，每一轮的每棵树只预测一个类，输入为[x,1/0]每棵树都给出一个预测值，然后用softmax产生每一类的概率。 根据loss function得到计算每个样本在每一类上的残差，用负梯度方向表示当前输入[x,y(x)]，再次迭代，每次同时训练出K棵树。 预测的时候得到K个预测值，用softmax得到属于每个类的概率进行预测。 XGBoost 模型函数 对于多个树的回归结果，直接相加作为最终的预测值。 回归树输出的是实数，可用于回归、分类、排序等任务中。对于分类问题，需要映射成概率，比如采用Logistic函数 目标函数 损失函数+正则项：obj=∑i=1nL(yi,y^it)+∑i=1tΩ(fi)obj = \\sum\\limits_{i=1}^{n} L(y_i,\\hat y_i^t) + \\sum \\limits_{i=1}^{t}\\Omega(f_i)obj=i=1∑n​L(yi​,y^​it​)+i=1∑t​Ω(fi​) 其中，nnn是样本的数量，ttt是树的数量 正则项 定义为：Ω(f)=γT+12λ∑j=1Twj2\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum\\limits_{j=1}^T w_j^2Ω(f)=γT+21​λj=1∑T​wj2​ 对每棵树的复杂度进行了惩罚：叶子节点个数（相当于在训练过程中做了剪枝）、叶节点分数 γ,λ\\gamma, \\lambdaγ,λ 越大，表示越希望获得结构简单的树，因为此时对较多叶子节点的树的惩罚越大。 损失函数 第ttt次迭代后，模型的预测等于前t−1t-1t−1次的模型预测加上第ttt棵树的预测，目标函数可以写做 obj=∑i=1nL(yi,yi^(t−1)+ft(xi))+Ω(ft)obj = \\sum\\limits_{i=1}^n L(y_i,\\hat{y_i}^{(t-1)} + f_t(x_i)) + \\Omega(f_t)obj=i=1∑n​L(yi​,yi​^​(t−1)+ft​(xi​))+Ω(ft​) 通过Taylor公式进行二次逼近，去除常数项，可以得到目标函数为 obj=∑i=1n[gift(xi)+12hift2(xi)]+Ω(ft)obj = \\sum\\limits_{i=1}^{n}[g_i f_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)] + \\Omega(f_t)obj=i=1∑n​[gi​ft​(xi​)+21​hi​ft2​(xi​)]+Ω(ft​) 其中gi=∂y^(t−1)L(yi,y^t−1)g_i = \\partial _{\\hat{y}(t-1)}L(y_i,\\hat{y}^{t-1})gi​=∂y^​(t−1)​L(yi​,y^​t−1)表示对损失函数的一阶导，hih_ihi​表示二阶导 定义ft(x)=wq(xi)f_t(x) = w_{q(x_i)}ft​(x)=wq(xi​)​为这棵树对样本的预测值 因此，这样我们就得到了新生成的树的优化目标，写成树的形式：wjw_jwj​表示第jjj个叶子节点的值，wq(xi)w_{q(x_i)}wq(xi​)​是将第xix_ixi​个样本映射到某个叶子节点，则可以表示为 obj=∑i=1n[giwq(xi)+12hiwq(xi)2]+γT+12λ∑j=1Twj2obj =\\sum\\limits_{i=1}^{n}[g_i w_{q(x_i)} + \\frac{1}{2}h_iw_{q(x_i)}^2] + \\gamma T + \\frac{1}{2} \\lambda \\sum\\limits_{j=1}^T w_j^2obj=i=1∑n​[gi​wq(xi​)​+21​hi​wq(xi​)2​]+γT+21​λj=1∑T​wj2​ 再用示性函数将样本累加和叶子节点累加统计起来： obj=∑j=1T[(∑i∈Ijgi)wj+12(∑i∈Ijhi+λ)wj2]+γTobj = \\sum\\limits_{j=1}^T[(\\sum\\limits_{i\\in I_j}g_i) w_j + \\frac{1}{2}(\\sum\\limits_{i\\in I_j}h_i+\\lambda) w_j^2] + \\gamma Tobj=j=1∑T​[(i∈Ij​∑​gi​)wj​+21​(i∈Ij​∑​hi​+λ)wj2​]+γT 求导可以得到wj=−GjHj+λw_j = -\\frac{G_j}{H_j + \\lambda}wj​=−Hj​+λGj​​，其中∑i∈Ijgi=Gj\\sum\\limits_{i\\in I_j}g_i = G_ji∈Ij​∑​gi​=Gj​，对每个样本点而言，确定了损失函数后gi,hig_i,h_igi​,hi​可以并行计算，得到最小损失为： obj=−12∑j=1TGj2Hj+λ+γTobj = -\\frac{1}{2} \\sum\\limits _{j =1}^T \\frac {G_j^2}{H_j+\\lambda} + \\gamma Tobj=−21​j=1∑T​Hj​+λGj2​​+γT 因此目标为最小化该目标函数，值越小，树的结构越好 学习策略 如何确定树的结构？ 暴力枚举所有可能的树结构，选择损失值最小的 - NP难问题 贪心法，类似于决策树，每次求得增益最大的分裂点 近似方法，只求分位数点 贪心法 贪心法，每次尝试分裂一个叶节点，计算分裂前后的增益，选择增益最大的（决策树都是这样做的） 对一个叶子节点进行分裂，分裂前后的增益定义为: Gain的值越大，分裂后损失函数减小越多。所以当对一个叶节点分割时，计算所有候选(feature,value)对应的gain，选取gain最大的进行分割 近似算法 对于每个特征，只考察分位数点，减少计算复杂度 这里的分位数使用了以二阶导数hih_ihi​作为权重得到的 这是因为obj=∑12hi(ft(xi)+gihi)2+Ω(ft)+constantobj = \\sum \\frac{1}{2}h_i (f_t(x_i) + \\frac{g_i}{h_i})^2 + \\Omega(f_t) + constantobj=∑21​hi​(ft​(xi​)+hi​gi​​)2+Ω(ft​)+constant 这个形式的loss说明了, xix_ixi​的loss也可以看做是以−gihi-\\frac{g_i}{h_i}−hi​gi​​作为label的均方误差，乘以大小为hih_ihi​的权重，换句话说，xix_ixi​对loss的贡献权重为hih_ihi​。 作者提出了两种近似算法。 一种是全局算法，即在初始化tree的时候划分好候选节点，并且在树的每一层都使用这些候选节点； 另一种是局部算法，即每一次划分的时候都重新计算候选节点。 这两者各有利弊，全局算法不需要多次计算候选节点，当需要一次获取较多的候选节点供后续树生长使用，而局部算法一次获取的候选节点较少，可以在分支过程中不断改善，即适用于生长更深的树，两者在effect和accuracy做trade off。 特点 行抽样（row sample）：bootstrap方法 列抽样（column sample）：借鉴随机森林 Shrinkage（缩减），即学习速率，防止过拟合 将学习速率调小，迭代次数增多，有正则化作用 支持自定义损失函数（需二阶可导） 总结 首先，XGBoost是加法模型。对每棵树的回归结果，直接相加作为最终的预测值 目标函数 对目标函数增加了正则项防止过拟合，抽象来说就是对每棵树的叶子结点个数、叶节点分数进行惩罚。 在第t次迭代时，可以写出其目标函数并使用泰勒公式进行二次逼近，去除常数项（前面已经生成的树）后得到新生成树的优化目标函数，目标为最小化该目标函数（包含每个样本关于损失函数的一阶导、二阶导、叶子结点数量等） 学习策略 可以直接暴力枚举可能的树结构，然后选择损失值最小的。这里采用了与决策树一样的贪心法，对一个叶子结点进行分类，计算分类前后的增益（也就是分裂后损失函数减少多少），选择最大的增益进行分割。 对于计算增益，可以便利所有特征的分割点，在这里使用了近似算法：对于每个特征，只考察以分位数点为分割点的增益，同时，这里的分位数使用了以二阶导为权重得到的 思考 gbdt的残差为什么用负梯度代替? 为了可以扩展到更复杂的损失函数中。只有在平方损失函数中求负梯度后是残差。 boosting的目的是为了是损失函数最小化而不是更好的拟合训练数据。 XGBoost和GBDT有什么不同？ 一般可以认为XGBoost是GBDT的改进版本 XGBoost除了CART作为基分类器，还支持线性分类器 损失函数 GBDT每次关注的是负梯度方向，在平方损失函数中就是残差方向。 可以自定义损失函数，且只需要二阶可导即可。 GBDT使用一阶梯度作为残差，而XGBoost使用二阶梯度，而XGBoost仍然比GBDT快，为什么？ 二阶梯度收敛更快，能够快的进行优化 正则化考虑 gboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。 列抽样（column subsampling）。借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 xgboost工具支持并行。 在训练之前，预先对数据进行了排序，保存为block结构，重复使用（利于并行计算）。 每次并行计算特征的增益。 节点分裂 XGBoost在计算时，用贪心算法效率很低。因此，使用了一种可并行的近似直方图算法：先对数据进行排序，然后按照二阶导权重计算分位数点，只对分位数点计算增益。 增益的计算方式也不同（GBDT是GINI系数，XGBoost是object function推导而来） 工程实现，用block利于并行化，提高cache命中率等 XGBoost和随机森林有什么不同？ 都是集成学习，都是树模型 随机森林是bagging模型，基模型之间是独立的，容易并行化，主要关注降低方差 XGBoost是加法模型，基模型之间相互关联，主要关注于减小偏差 XGBoost计算海瑟矩阵相对于一阶梯度是否增大了计算量？ 并没有，实际上利用二阶梯度能更快收敛 Reference XGBoost: A Scalable Tree Boosting System Introduction to Boosted Trees PPT "},"learning theory.html":{"url":"learning theory.html","title":"Learning Theory","keywords":"","body":" Bias/variance tradeoﬀ / hypothesis class concept Generalization error Measure of how accurately an algorithm is able to predict outcome values for previously unseen data. Bias High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). Model is too 'simple' Variance an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise) in the training data, rather than the intended outputs (overfitting). Model is too 'complex' 如何度量bias和variance最终决定了我们的模型选择 Union bound P(A1∪...∪Ak)≤P(A1)+...+P(Ak)P(A_1 \\cup ... \\cup A_k) \\le P(A_1) + ... + P(A_k)P(A1​∪...∪Ak​)≤P(A1​)+...+P(Ak​) Hoeffding inequality(Chernoff bound) 设观测变量ZkZ_kZk​独立同分布，且服从两点分布，参数为ϕ\\phiϕ P(∣ϕ−ϕ^∣>γ)≤2exp⁡(−2γ2m)P(|\\phi - \\hat{\\phi}| > \\gamma) \\le 2\\exp(-2\\gamma^2 m)P(∣ϕ−ϕ^​∣>γ)≤2exp(−2γ2m) 也就是说，如果我们的样本数mmm越来越大，则估计值会越来越接近真实值 Expirical risk/error ϵ^(h)=1m∑i=1m1(h(xi)≠yi)\\hat{\\epsilon }(h) = \\frac{1}{m} \\sum\\limits_{i=1}^m 1(h(x_i)\\ne y_i)ϵ^(h)=m1​i=1∑m​1(h(xi​)≠yi​) Empirical risk minimization (ERM) 由于我们只有样本，因此最直观的方式就是经验风险最小化： θ^=arg⁡min⁡ϵ^(hθ)\\hat{\\theta} = \\arg\\min \\hat{\\epsilon }(h_\\theta)θ^=argminϵ^(hθ​) Hypothesis class Set of all classiﬁers over X (the domain of the inputs) 其实就是一种类型model的集合，例如线性模型，神经网络等等 因此，经验风险最小化实际上就是在Hypothesis class中，选择一个最好的： h^=arg⁡min⁡ϵ(h)^\\hat{h} = \\arg \\min \\hat{\\epsilon (h)}h^=argminϵ(h)^​ finite hypothesis class 我们使用经验风险最小化来选择我们的模型，因此，最重要的两个问题是： 这个估计可靠吗？ 这个估计对于generalization error的upper-bound是多少？ 根据Chernoff bound我们可以得到： 也就是说，对于hypothesis class中的任何模型，使得其error小于γ\\gammaγ的概率大于1−2kexp⁡(−2γ2m)1-2k\\exp(-2\\gamma^2 m)1−2kexp(−2γ2m) 那么，我们可以分别固定两个变量，求第三个变量需要满足的条件 m≥12γ2log⁡2kδm \\ge\\frac{1}{2\\gamma^2}\\log \\frac{2k}{\\delta} m≥2γ21​logδ2k​ 可以发现，kkk的数量越大，需要的样本量也越大，以log进行增长（增长的很慢） r≤12mlog⁡2kδr \\le \\sqrt{\\frac{1}{2m}\\log\\frac{2k}{\\delta}}r≤2m1​logδ2k​​ 样本量越大，error越小，同时，还可以证明，我们用经验风险最小化得到的model只比最优的model相差两个γ\\gammaγ（需要在同样的hypothesis class中） ϵ(h^)≤ϵ(h⋆)+2γ\\epsilon(\\hat{h}) \\le \\epsilon(h^\\star)+ 2\\gammaϵ(h^)≤ϵ(h⋆)+2γ 现在我们可以形式化的定义bias和generalization error（对于分类问题而言）： ϵ(h^)≤(min⁡ϵ(h))+212mlog⁡2kδ\\epsilon(\\hat{h}) \\le (\\min \\epsilon(h)) + 2\\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}ϵ(h^)≤(minϵ(h))+22m1​logδ2k​​ 其中，第一项可以认为是我们的bias：当我们的模型越来越复杂时（hypothesis class越来越大），则第一项会减小，而第二项为增加（也就是variance） infinite hypothesis class 如果我们的假设空间中的参数有d个实数组成，那么在电脑中用double类型表示，需要64d个bits。因此，实际上计算机将连续的实树离散为264d2^{64d}264d个假设。根据我们之前的不到呢个好死，可以得到我们需要的样本数量为： m≥O(1γ2log⁡kδ)=O(dγ2log⁡1δ)m \\ge O (\\frac{1}{\\gamma^2}\\log \\frac{k}{\\delta} ) = O(\\frac{d}{\\gamma^2}\\log\\frac{1}{\\delta})m≥O(γ21​logδk​)=O(γ2d​logδ1​) 也就是说，我们需要的样本数量与假设空间中的参数个数基本上是线性关系。 we conclude that (for an algorithm that tries to minimize training error) the number of training examples needed is usually roughly linear in the number of parameters of HHH. ERM 对于经验风险最小化而言，如果是对于二分类问题，其损失函数是阶梯函数，不是凸函数，并不容易优化。 回想logistic regression，实际上其优化目标是最大化极大似然，也就是最小化负的log函数，这实际上可以看作是经验风险最小化的凸函数形式的近似。 同样，SVM的损失函数为hinge loss，也可也认为是经验风险最小化的近似。 Reference Bias–variance tradeoff "},"regularization.html":{"url":"regularization.html","title":"Regularization Feature Selection","keywords":"","body":"在之前我们讨论了最小化风险函数，但很多时候这样做的效果并不好，这是由于bias and variance的权衡。因此，我们需要进行模型选择，来自动的选择最合适的模型。 Cross validation 假设我们有一些有限的模型，如何来选择哪个模型能够使得其泛化能力最好？ 常用的方法是交叉验证： hold-out cross validation 使用一部分数据（70%）作为train data进行训练，使用剩下的数据作为validation data进行验证，选择在验证集上误差最小的模型 k-fold cross validation 在hold-out cross validation中，我们的30%的数据实际上都被浪费了。而如果数据量本来就很小，那么这样做并不合理。因此，更常用的是每次使用30%的数据作为验证集，训练多次得到其平均误差。 k=10是常用的做法，但计算量会很大 leave-one-out cross validation. 可以令k=m，也就是划分的数量和数据集大小一样，每次只用一个样本作为验证。 只适合在数据量非常少的情况下使用。 Feature selection 如果我们有很多feature，但很可能大部分feature对于我们的分类任务都没有太大帮助。因此，我们要选择最为合适的feature子集，但其选择空间有2n2^n2n，是在太大了。 因此，有几种启发式的feature selection方法： Wrapper model feature selection forward search 很简单的想法，我们每次尝试加入一个feature，如果加入这个feature能在验证集上表现得更好，那么我们就要他；如果表现得更差，那么就抛弃他 backward search 同样的想法，每次删去一个feature，看训练集上的表现是否提高 Filter feature selection 在之前的算法中，计算量非常大。因此，一个直观的想法是定义一个score function，来衡量某个feature能表示的信息量大小，即S(i)S(i)S(i)，然后，选择kkk个能使得score 最大的feature。 一般来说，S(i)S(i)S(i)是根据xix_ixi​和yyy之间的相关性来度量的 mutual information MI⁡(xi,y)=∑xi∈{0,1}∑y∈{0,1}p(xi,y)log⁡p(xi,y)p(xi)p(y)\\operatorname{MI}\\left(x_{i}, y\\right)=\\sum_{x_{i} \\in\\{0,1\\}} \\sum_{y \\in\\{0,1\\}} p\\left(x_{i}, y\\right) \\log \\frac{p\\left(x_{i}, y\\right)}{p\\left(x_{i}\\right) p(y)}MI(xi​,y)=∑xi​∈{0,1}​∑y∈{0,1}​p(xi​,y)logp(xi​)p(y)p(xi​,y)​ 可以发现，mutual information可以表示为KL散度的形式： MI⁡(xi,y)=KL(p(xi,y)∥p(xi)p(y))\\operatorname{MI}\\left(x_{i}, y\\right)=\\mathrm{KL}\\left(p\\left(x_{i}, y\\right) \\| p\\left(x_{i}\\right) p(y)\\right)MI(xi​,y)=KL(p(xi​,y)∥p(xi​)p(y)) 因此，mutual information衡量了分布p(xi,y)p(x_{i}, y)p(xi​,y) 和 p(xi)p(y)p(x_{i}) p(y)p(xi​)p(y) 的差异。 Bayesian statistics and regularization 频率学派 认为参数是已知的，我们需要做的只是从数据中学习参数，一般使用极大似然估计等方式 贝叶斯学派 认为参数也是随机变量，因此我们有一个先验分布，我们需要求解的是在已知数据的情况下参数的分布（后验分布）： p(θ∣S)=p(S∣θ)p(θ)p(S)=(∏i=1mp(y(i)∣x(i),θ))p(θ)∫θ(∏i=1mp(y(i)∣x(i),θ)p(θ))dθ\\begin{aligned} p(\\theta | S) &=\\frac{p(S | \\theta) p(\\theta)}{p(S)} \\\\ &=\\frac{\\left(\\prod_{i=1}^{m} p\\left(y^{(i)} | x^{(i)}, \\theta\\right)\\right) p(\\theta)}{\\int_{\\theta}\\left(\\prod_{i=1}^{m} p\\left(y^{(i)} | x^{(i)}, \\theta\\right) p(\\theta)\\right) d \\theta} \\end{aligned}p(θ∣S)​=p(S)p(S∣θ)p(θ)​=∫θ​(∏i=1m​p(y(i)∣x(i),θ)p(θ))dθ(∏i=1m​p(y(i)∣x(i),θ))p(θ)​​ 当我们想要得到一个新的xxx的输出时，对后验分布进行积分： p(y∣x,S)=∫θp(y∣x,θ)p(θ∣S)dθp(y | x, S)=\\int_{\\theta} p(y | x, \\theta) p(\\theta | S) d \\thetap(y∣x,S)=∫θ​p(y∣x,θ)p(θ∣S)dθ 分母其实就是 P(S)， 而我们就是要让P(S)在各种参数的影响下能够最大（这里只有参数θ）。因此我们只需求出随机变量θ中最可能的取值，这样求出θ后，可将θ视为固定值，那么预测时就不用积分了，而是直接像最大似然估计中求出θ后一样进行预测，这样就变成了点估计。这种方法称为最大 后验概率估计（Maximum a posteriori）方法： θMAP=arg⁡max⁡θ∏i=1mp(y(i)∣x(i),θ)p(θ)\\theta_{\\mathrm{MAP}}=\\arg \\max _{\\theta} \\prod_{i=1}^{m} p\\left(y^{(i)} | x^{(i)}, \\theta\\right) p(\\theta)θMAP​=argmaxθ​∏i=1m​p(y(i)∣x(i),θ)p(θ) 很神奇的是，我们这个形式与最大似然的形式非常相似，除了最后的p(θ)p(\\theta)p(θ) 一般来说，我们可以将先验假设成：θ∼N(0,τ2I)\\theta \\sim \\mathcal{N}\\left(0, \\tau^{2} I\\right)θ∼N(0,τ2I)，也就是很多的feature的参数值都为零（消除这个feature）。使用这个假设，那么MAP比最大似然有更小的norm，也就是更不容易过拟合。 实际上，对于线性回归而言，最大似然实际上就是使得其误差平方和最小；在MAP中，就是在误差平方和后再加入一个正则项λ∣∣θ∣∣2\\lambda||\\theta||^2λ∣∣θ∣∣2 一个很好的例子是Bayesian logistic regression对于文本分类是一种很有效的算法，即使当feature远远大于样本数量时。 "},"online learning.html":{"url":"online learning.html","title":"Online Learning","keywords":"","body":"之前我们都是讨论的batch learning，就是给了一堆样例后，在样例上学习出假设函数 h。而在线学习就是要根据新来的样例，边学习，边给出结果。 假设样例按照到来的先后顺序依次定义为 (x1,y1),(x2,y2),...,(xm,ym)(x_1, y_1), (x_2, y_2), ...,(x_m, y_m)(x1​,y1​),(x2​,y2​),...,(xm​,ym​)。xxx 为样本特征，yyy 为类别标签。我们的任务是到来一个样例 xxx，给出其类别结果 yyy 的预测值，之后我们会看到 yyy 的真实值，然后根据真实值来重新调整模型参数，整个过程是重复迭代的过程，直到所有的样例完成。这么看来，我们也可以将原来用于批量学习的样例拿来作为在线学习的样例。 在在线学习中我们主要关注在整个预测过程中预测错误的样例数。 perception algorithm 首先给出感知器学习最简单的形式，也就是学习函数： hθ(x)=g(θTx)h_{\\theta}(x)=g\\left(\\theta^{T} x\\right)hθ​(x)=g(θTx) 其中 x 是 n 维特征向量，θ是 n+1 维参数权重。函数ggg用来将θTx\\theta^T xθTx计算结果映射到-1和 1上，其中： g(z)={1 if z≥0−1 if z0g(z)=\\left\\{\\begin{aligned}1 & \\text { if } z \\geq 0 \\\\-1 & \\text { if } zg(z)={1−1​ if z≥0 if z0​ 我们只对分类错误的点进行更新，损失函数可以表示为： min⁡θL(θ)=−∑xi∈Myi(θTx)\\min _{\\theta} L(\\theta)=-\\sum_{x_{i} \\in M} y_{i}(\\theta^T x)θmin​L(θ)=−xi​∈M∑​yi​(θTx) 可以得到更新公式为： θ:=θ+yx\\theta :=\\theta+y xθ:=θ+yx 也就是说，如果对于预测错误的样例，θ\\thetaθ进行调整时只需加上(实际上为正例)或者减去(实际负例)样本特征xxx值即可。θ\\thetaθ初始值为向量 0。这里我们关心的是θTx\\theta^TxθTx的符号，而不是它的具体值。调整方法非常简单。然而这个简单的调整方法还是很有效的，它的错误率不仅是有上界的，而且这个上界不依赖于样例数和特征维度。 Block and Novikoff 该定理给出了感知机学习预测的错误样例数的上界： 给定按照顺序到来的(x(1),y(1)),(x(2),y(2)),…(x(m),y(m))\\left(x^{(1)}, y^{(1)}\\right),\\left(x^{(2)}, y^{(2)}\\right), \\ldots\\left(x^{(m)}, y^{(m)}\\right)(x(1),y(1)),(x(2),y(2)),…(x(m),y(m))样例。假设对于所有的样例∥x(i)∥≤D\\left\\|x^{(i)}\\right\\| \\leq D∥∥​x(i)∥∥​≤D，也就是说特征向量长度有界为DDD。更进一步，假设存在一个单位长度向量u(∣∣u∣∣2=1)u( || u||_ 2=1)u(∣∣u∣∣2​=1)且使得y(i)⋅(uTx(i))≥γy^{(i)} \\cdot\\left(u^{T} x^{(i)}\\right) \\geq \\gammay(i)⋅(uTx(i))≥γ。uuu能够有γ\\gammaγ的间隔将正例和反例分开。 那么感知算法的预测的错误样例数不超过(D/γ)2(D/\\gamma)^2(D/γ)2。 具体证明可见CS229，不再阐述。 可以发现，该定理与SVM非常像，如果我们把DDD认为是几何间隔，定理描述的就是在线性可分的情况下最大化几何间隔的分类错误数。 "},"advice.html":{"url":"advice.html","title":"Advice For Applying ML","keywords":"","body":"Key ideas: Diagnostics for debugging learning algorithms. Error analyses and ablative analysis. How to get started on a machine learning problem. Premature (statistical) optimization. Debugging Learning Algorithms Bias vs Variance 一般来说，在讨论我们模型的可用性时，考虑bias和variance两个方面。 我们可以通过Test data和Train data的特征进行推测 Typical learning curve for high variance（Overfitting）: 主要的特征表示为： 当样本量增大时Test error也依然减小，表明更多的数据有帮助 Large gap between training and test error Typical learning curve for high bias: 主要表现的特征是： 在训练集的error也依然太高了 Small gap between training and test error. Optimization algorithm 在不改变模型的情况下，优化目标就变得非常重要了。有两个主要的问题： 算法是否已经收敛？ 目标函数是否合适？ 如果当目标函数已经够小，但表现出来的结果却不好，那么很有可能是目标函数的问题 如果当目标函数不够小，且表现结果不够好，那么有可能是算法还没收敛 Error Analysis 作者这里以人脸识别为例子，认为分析每一个部分的error非常重要。对于一般的任务来说，我们很多时候希望使用更少的feature达到更好的效果。因此，我们可以去掉某一个feature，看Accuracy是否减少，如果减少的不多，那么该feature并不是特别重要。 Getting started on a learning problem 作者给出了两种设计思路 Careful design. 从feature selection，数据收集，清理，算法设计等各个方面进行考虑，然后再进行训练 这样得到的架构是非常清晰和完整的 Build-and-fix. 首先实现了再说，通过之前的分析找不满意的地方，然后再fix Premature statistical optimization 作者提出的一个建议为非常的认同：我们很多时候过早的进行统计优化了。 例如，拿到一个新问题，不应该直接尝试复杂的模型，或者设计复杂的算法，而是应该将数据plot出来，找到数据中存在的规律和错误。 "},"em.html":{"url":"em.html","title":"EM思想","keywords":"","body":"Kmeans kmeans是一种相当简单和直观的聚类算法，主要分类两步： 对于每个点，选择离他最近的聚类中心作为他的类别： c(i):=arg⁡min⁡j∥x(i)−μj∥2c^{(i)} :=\\arg \\min _{j}\\left\\|x^{(i)}-\\mu_{j}\\right\\|^{2}c(i):=argjmin​∥∥∥​x(i)−μj​∥∥∥​2 对于每个类别，求解聚类这个类的聚类中心： μj:=∑i=1m1c(i)=jx(i)∑i=1m1c(i)=j \\mu_{j}:=\\frac{\\sum_{i=1}^{m} 1{c^{(i)}=j} x^{(i)}}{\\sum_{i=1}^{m} 1{c^{(i)}=j}} μj​:=∑i=1m​1c(i)=j∑i=1m​1c(i)=jx(i)​ 虽然算法很简单，但是我们还是需要回答一个很基本的问题，这个算法会收敛吗？ 我们定义一个distortion function：J(c,μ)=∑i=1m∥x(i)−μc(i)∥2J(c, \\mu)=\\sum_{i=1}^{m}\\left\\|x^{(i)}-\\mu_{c^{(i)}}\\right\\|^{2}J(c,μ)=∑i=1m​∥∥​x(i)−μc(i)​∥∥​2 这个函数衡量了点到对应的聚类中心的距离平方和，实际上，我们的kmeans算法能使得distortion function不断减小，具体来说： 第一步是在μ\\muμ固定的情况下，我们通过ccc不断减小JJJ 第二步是在ccc固定的情况下，我们通过μ\\muμ不断减小JJJ 因此，JJJ一定是单调递减的，因此也保证了算法的收敛性。 但在实际应用中，kmeans算法并不能保证全局最优解，同时可能存在着震荡，这是因为我们的优化目标JJJ不是一个凸函数。而kmeans算法的每一步都是在寻找局部最优解（local optima），因此，最好的办法是多次重复该算法，并选择最小的JJJ。 GMM Model 假设我们有一系列训练集{x(1),…,x(m)}\\{x^{(1)}, \\ldots, x^{(m)}\\}{x(1),…,x(m)}，我们需要使用非监督学习的方法进行训练。 我们将这些数据建模成联合分布的形式：p(x(i),z(i))=p(x(i)∣z(i))p(z(i))p\\left(x^{(i)}, z^{(i)}\\right)= p\\left(x^{(i)} | z^{(i)}\\right) p\\left(z^{(i)}\\right) p(x(i),z(i))=p(x(i)∣z(i))p(z(i))。 在这里， z(i)∼z^{(i)} \\simz(i)∼ Multinomial (ϕ)(\\phi)(ϕ) (where ϕj≥0,∑j=1kϕj=1\\phi_{j} \\geq 0, \\sum_{j=1}^{k} \\phi_{j}=1ϕj​≥0,∑j=1k​ϕj​=1），也就是我们的隐变量 在给定zzz的条件下，假设x(i)∣z(i)=j∼N(μj,Σj)x^{(i)} | z^{(i)}=j \\sim \\mathcal{N}\\left(\\mu_{j}, \\Sigma_{j}\\right)x(i)∣z(i)=j∼N(μj​,Σj​) 因此，我们首先需要通过随机变量zzz产生一个z(i)z^{(i)}z(i)，然后再从对应的高斯分布中产生xxx，这种模型被称为高斯混合模型。 不难得到，对于这个模型来说，我们的参数为ϕ,μ\\phi, \\muϕ,μ and Σ\\SigmaΣ。写成似然函数的形式： ℓ(ϕ,μ,Σ)=∑i=1mlog⁡p(x(i);ϕ,μ,Σ)=∑i=1mlog⁡∑z(i)=1kp(x(i)∣z(i);μ,Σ)p(z(i);ϕ) \\begin{aligned} \\ell(\\phi, \\mu, \\Sigma) &=\\sum_{i=1}^{m} \\log p\\left(x^{(i)} ; \\phi, \\mu, \\Sigma\\right) \\\\ &=\\sum_{i=1}^{m} \\log \\sum_{z^{(i)}=1}^{k} p\\left(x^{(i)} | z^{(i)} ; \\mu, \\Sigma\\right) p\\left(z^{(i)} ; \\phi\\right) \\end{aligned} ℓ(ϕ,μ,Σ)​=i=1∑m​logp(x(i);ϕ,μ,Σ)=i=1∑m​logz(i)=1∑k​p(x(i)∣z(i);μ,Σ)p(z(i);ϕ)​ 但很遗憾的是，如果我们直接对这个似然函数求导，无法得到一个cloed form。 If zzz is observed 但如果我们的隐变量zzz是已知的呢，我们是不是就很容易求解了呢？（这里与GDA算法相同，使用MLE求解） 我们重写似然函数为： ℓ(ϕ,μ,Σ)=∑i=1mlog⁡p(x(i)∣z(i);μ,Σ)+log⁡p(z(i);ϕ) \\ell(\\phi, \\mu, \\Sigma)=\\sum_{i=1}^{m} \\log p\\left(x^{(i)} | z^{(i)} ; \\mu, \\Sigma\\right)+\\log p\\left(z^{(i)} ; \\phi\\right) ℓ(ϕ,μ,Σ)=i=1∑m​logp(x(i)∣z(i);μ,Σ)+logp(z(i);ϕ) 带入假设的分布，不难求得： ϕj=1m∑i=1m1{z(i)=j} \\phi_{j}=\\frac{1}{m} \\sum_{i=1}^{m} 1\\left\\{z^{(i)}=j\\right\\} ϕj​=m1​i=1∑m​1{z(i)=j} μj=∑i=1m1{z(i)=j}x(i)∑i=1m1{z(i)=j} \\mu_{j}=\\frac{\\sum_{i=1}^{m} 1\\left\\{z^{(i)}=j\\right\\} x^{(i)}}{\\sum_{i=1}^{m} 1\\left\\{z^{(i)}=j\\right\\}} μj​=∑i=1m​1{z(i)=j}∑i=1m​1{z(i)=j}x(i)​ Σj=∑i=1m1{z(i)=j}(x(i)−μj)(x(i)−μj)T∑i=1m1{z(i)=j} \\Sigma_{j}=\\frac{\\sum_{i=1}^{m} 1\\left\\{z^{(i)}=j\\right\\}\\left(x^{(i)}-\\mu_{j}\\right)\\left(x^{(i)}-\\mu_{j}\\right)^{T}}{\\sum_{i=1}^{m} 1\\left\\{z^{(i)}=j\\right\\}} Σj​=∑i=1m​1{z(i)=j}∑i=1m​1{z(i)=j}(x(i)−μj​)(x(i)−μj​)T​ 因此，如果我们已知zzz，那么MLE后的结果和之前的高斯判别模型完全一致了。 但实际上，zzz是未知的，那么怎么办呢？ EM algorithm 我们使用EM思想来处理。EM是一种迭代的算法，主要有两个步骤： E步：通过期望去gusszzz的最可能的值 wj(i):=p(z(i)=j∣x(i);ϕ,μ,Σ)w_{j}^{(i)} :=p\\left(z^{(i)}=j | x^{(i)} ; \\phi, \\mu, \\Sigma\\right)wj(i)​:=p(z(i)=j∣x(i);ϕ,μ,Σ) 实际上我们是通过后验概率来进行估计： p(z(i)=j∣x(i);ϕ,μ,Σ)=p(x(i)∣z(i)=j;μ,Σ)p(z(i)=j;ϕ)∑l=1kp(x(i)∣z(i)=l;μ,Σ)p(z(i)=l;ϕ) p\\left(z^{(i)}=j | x^{(i)} ; \\phi, \\mu, \\Sigma\\right)=\\frac{p\\left(x^{(i)} | z^{(i)}=j ; \\mu, \\Sigma\\right) p\\left(z^{(i)}=j ; \\phi\\right)}{\\sum_{l=1}^{k} p\\left(x^{(i)} | z^{(i)}=l ; \\mu, \\Sigma\\right) p\\left(z^{(i)}=l ; \\phi\\right)} p(z(i)=j∣x(i);ϕ,μ,Σ)=∑l=1k​p(x(i)∣z(i)=l;μ,Σ)p(z(i)=l;ϕ)p(x(i)∣z(i)=j;μ,Σ)p(z(i)=j;ϕ)​ 在这里，我们分子上的概率都可以直接得到，因此可以得到x(i)=jx^{(i)} = jx(i)=j的概率，也就是soft assignments wj(i)w^{(i)}_jwj(i)​ M步：通过已知的zzz来对模型参数进行估计（与上面一样） ϕj:=1m∑i=1mwj(i) \\phi_{j} :=\\frac{1}{m} \\sum_{i=1}^{m} w_{j}^{(i)} ϕj​:=m1​i=1∑m​wj(i)​ μj:=∑i=1mwj(i)x(i)∑i=1mwj(i) \\mu_{j} :=\\frac{\\sum_{i=1}^{m} w_{j}^{(i)} x^{(i)}}{\\sum_{i=1}^{m} w_{j}^{(i)}} μj​:=∑i=1m​wj(i)​∑i=1m​wj(i)​x(i)​ Σj:=∑i=1mwj(i)(x(i)−μj)(x(i)−μj)T∑i=1mwj(i) \\Sigma_{j} :=\\frac{\\sum_{i=1}^{m} w_{j}^{(i)}\\left(x^{(i)}-\\mu_{j}\\right)\\left(x^{(i)}-\\mu_{j}\\right)^{T}}{\\sum_{i=1}^{m} w_{j}^{(i)}} Σj​:=∑i=1m​wj(i)​∑i=1m​wj(i)​(x(i)−μj​)(x(i)−μj​)T​ 我们会发现，EM算法和kmeans有着很微妙的关系，除了在E步时，kmeans使用了hard cluster assignments而不是soft assignments，也就是对每个点分配了一个类别而不是概率，其他的都完全一样。 因此，EM也是一种local optima的算法，因此随机初始化参数可能会得到不同的结果。 但对于EM而言，还有两个问题没有解决： E步时，如何来估计隐变量是一种较好的选择？ 如何保证算法的收敛性？ 这些内容在下一篇博客会详细介绍。 "},"em algorithm.html":{"url":"em algorithm.html","title":"EM算法推导","keywords":"","body":"Jensen’s inequality 定理：若fff是凸函数，XXX是随机变量，我们有：E[f(X)]≥f(EX)\\mathrm{E}[f(X)] \\geq f(\\mathrm{E} X)E[f(X)]≥f(EX) 若fff是严格凸函数，也就是f′′>0f^{''} > 0f′′>0恒成立，同时X=E[X]X=E[X]X=E[X]（也就是XXX是常数的概率为1），则等号成立。 若fff是凹函数，则该定理也成立，只不过将大于等于换成小于等于。 忽略证明，该定理并不直观，可以用一个简单的例子帮助记忆： 收敛性证明 我们想用模型拟合数据，也就是求似然函数： ℓ(θ)=∑i=1mlog⁡p(x;θ)=∑i=1mlog⁡∑zp(x,z;θ) \\begin{aligned} \\ell(\\theta) &=\\sum_{i=1}^{m} \\log p(x ; \\theta) \\\\ &=\\sum_{i=1}^{m} \\log \\sum_{z} p(x, z ; \\theta) \\end{aligned} ℓ(θ)​=i=1∑m​logp(x;θ)=i=1∑m​logz∑​p(x,z;θ)​ 其中，zzz是隐变量。如果zzz已知，那么直接用MLE求解即可，如果未知，则需要用EM算法迭代求解。 EM算法分为两步： E step：每次得到似然函数ℓ\\ellℓ的一个下界。 M step：对该下界进行优化。 我们首先可以假设QQQ是zzz的分布，也就是满足：∑zQi(z)=1,Qi(z)≥1\\sum_{z} Q_{i}(z)=1, Q_{i}(z) \\geq 1∑z​Qi​(z)=1,Qi​(z)≥1 我们的目标是最大化似然函数， 因此可以得到： ∑ilog⁡p(x(i);θ)=∑ilog⁡∑z(i)p(x(i),z(i);θ)=∑ilog⁡∑z(i)Qi(z(i))p(x(i),z(i);θ)Qi(z(i))≥∑i∑z(i)Qi(z(i))log⁡p(x(i),z(i);θ)Qi(z(i)) \\begin{aligned} \\sum_{i} \\log p\\left(x^{(i)} ; \\theta\\right) &=\\sum_{i} \\log \\sum_{z^{(i)}} p\\left(x^{(i)}, z^{(i)} ; \\theta\\right) \\\\ &=\\sum_{i} \\log \\sum_{z^{(i)}} Q_{i}\\left(z^{(i)}\\right) \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{Q_{i}\\left(z^{(i)}\\right)} \\\\ & \\geq \\sum_{i} \\sum_{z^{(i)}} Q_{i}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{Q_{i}\\left(z^{(i)}\\right)} \\end{aligned} i∑​logp(x(i);θ)​=i∑​logz(i)∑​p(x(i),z(i);θ)=i∑​logz(i)∑​Qi​(z(i))Qi​(z(i))p(x(i),z(i);θ)​≥i∑​z(i)∑​Qi​(z(i))logQi​(z(i))p(x(i),z(i);θ)​​ 这里用到了..期望就是概率..的思想。我们将QQQ函数看成是在随机变量p(x(i),z(i);θ)Qi(z(i))\\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{Q_{i}\\left(z^{(i)}\\right)}Qi​(z(i))p(x(i),z(i);θ)​上的概率分布，将函数fff看成是log function。因此，第二个等式可以看作是f(EX)f(EX)f(EX)。而由于fff函数是凹函数，因此根据Jensen’s inequality，可以得到不等式三。 这样，对于任意的分布QQQ，我们给出了似然函数的下界。因此，我们如何选择一个合适的QQQ呢（最好能取到等号）？ 我们如果对当前的θ\\thetaθ有一个估计值，那么很自然的思想就是用这个估计值来得到不等式的下界。根据之前Jensen’s inequality不等式的分析，如果我们的随机变量是一个常量，那么等式一定成立，即： p(x(i),z(i);θ)Qi(z(i))=c \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{Q_{i}\\left(z^{(i)}\\right)}=c Qi​(z(i))p(x(i),z(i);θ)​=c 因此，我们只需要Qi(z(i))∝p(x(i),z(i);θ)Q_{i}\\left(z^{(i)}\\right) \\propto p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)Qi​(z(i))∝p(x(i),z(i);θ)即可。同时，由于∑zQi(z(i))=1\\sum_{z} Q_{i}\\left(z^{(i)}\\right)=1∑z​Qi​(z(i))=1的条件需要满足，因此构造一个QQQ函数为： Qi(z(i))=p(x(i),z(i);θ)∑zp(x(i),z;θ)=p(x(i),z(i);θ)p(x(i);θ)=p(z(i)∣x(i);θ) \\begin{aligned} Q_{i}\\left(z^{(i)}\\right) &=\\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{\\sum_{z} p\\left(x^{(i)}, z ; \\theta\\right)} \\\\ &=\\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{p\\left(x^{(i)} ; \\theta\\right)} \\\\ &=p\\left(z^{(i)} | x^{(i)} ; \\theta\\right) \\end{aligned} Qi​(z(i))​=∑z​p(x(i),z;θ)p(x(i),z(i);θ)​=p(x(i);θ)p(x(i),z(i);θ)​=p(z(i)∣x(i);θ)​ 实际上，这个QQQ函数就是我们熟悉的在给定θ\\thetaθ下的后验分布。 如何证明收敛性呢？也就是需要证明ℓ(θ(t))≤ℓ(θ(t+1))\\ell\\left(\\theta^{(t)}\\right) \\leq \\ell\\left(\\theta^{(t+1)}\\right)ℓ(θ(t))≤ℓ(θ(t+1))始终成立。 由于我们选择的QQQ函数能使得等式成立，因此在第ttt次迭代时，有： ℓ(θ(t))=∑i∑z(i)Qi(t)(z(i))log⁡p(x(i),z(i);θ(t))Qi(t)(z(i)) \\ell\\left(\\theta^{(t)}\\right)=\\sum_{i} \\sum_{z^{(i)}} Q_{i}^{(t)}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta^{(t)}\\right)}{Q_{i}^{(t)}\\left(z^{(i)}\\right)} ℓ(θ(t))=i∑​z(i)∑​Qi(t)​(z(i))logQi(t)​(z(i))p(x(i),z(i);θ(t))​ 在第t+1t+1t+1次时，我们的θ(t+1)\\theta^{(t+1)}θ(t+1)是最大化右边的式子的来的，因此： ℓ(θ(t+1))≥∑i∑z(i)Qi(t)(z(i))log⁡p(x(i),z(i);θ(t+1))Qi(t)(z(i))≥∑i∑z(i)Qi(t)(z(i))log⁡p(x(i),z(i);θ(t))Qi(t)(z(i))=ℓ(θ(t)) \\begin{aligned} \\ell\\left(\\theta^{(t+1)}\\right) & \\geq \\sum_{i} \\sum_{z^{(i)}} Q_{i}^{(t)}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta^{(t+1)}\\right)}{Q_{i}^{(t)}\\left(z^{(i)}\\right)} \\\\ & \\geq \\sum_{i} \\sum_{z^{(i)}} Q_{i}^{(t)}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta^{(t)}\\right)}{Q_{i}^{(t)}\\left(z^{(i)}\\right)} \\\\ &=\\ell\\left(\\theta^{(t)}\\right) \\end{aligned} ℓ(θ(t+1))​≥i∑​z(i)∑​Qi(t)​(z(i))logQi(t)​(z(i))p(x(i),z(i);θ(t+1))​≥i∑​z(i)∑​Qi(t)​(z(i))logQi(t)​(z(i))p(x(i),z(i);θ(t))​=ℓ(θ(t))​ 其中，第一个不等式是根据Jensen’s inequality，第二个不等式是根据最大化θ\\thetaθ的性质来的。 如果我们定义： J(Q,θ)=∑i∑z(i)Qi(z(i))log⁡p(x(i),z(i);θ)Qi(z(i)) J(Q, \\theta)=\\sum_{i} \\sum_{z^{(i)}} Q_{i}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{Q_{i}\\left(z^{(i)}\\right)} J(Q,θ)=i∑​z(i)∑​Qi​(z(i))logQi​(z(i))p(x(i),z(i);θ)​ 那么，EM算法也可以看作是在JJJ上进行coordinate ascent： E step 时，固定θ\\thetaθ，根据QQQ最大化JJJ 实际上是通过Jensen’s inequality的性质，定义QQQ函数为后验概率满足等式） M step 时，固定QQQ，根据θ\\thetaθ最大化JJJ 实际上是通过MLE进行最大化 GMM revisited GMM的思想不再阐述，这里主要进行推导closed form。 E step E step相对容易一些，我们对于当前步估计的所有参数值，计算zzz的后验分布（这样能保证在当前参数下等式成立，也就是tight bound）： wj(i)=Qi(z(i)=j)=P(z(i)=j∣x(i);ϕ,μ,Σ) w_{j}^{(i)}=Q_{i}\\left(z^{(i)}=j\\right)=P\\left(z^{(i)}=j | x^{(i)} ; \\phi, \\mu, \\Sigma\\right) wj(i)​=Qi​(z(i)=j)=P(z(i)=j∣x(i);ϕ,μ,Σ) M step 根据上一步得到的zzz的分布，我们最大化ℓ\\ellℓ的下界： ∑i=1m∑z(i)Qi(z(i))log⁡p(x(i),z(i);ϕ,μ,Σ)Qi(z(i))=∑i=1m∑j=1kQi(z(i)=j)log⁡p(x(i)∣z(i)=j;μ,Σ)p(z(i)=j;ϕ)Qi(z(i)=j)=∑i=1m∑j=1kwj(i)log⁡1(2π)n/2∣Σj∣1/2exp⁡(−12(x(i)−μj)TΣj−1(x(i)−μj))⋅ϕjwj(i) \\begin{aligned} \\sum_{i=1}^{m} & \\sum_{z^{(i)}} Q_{i}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\phi, \\mu, \\Sigma\\right)}{Q_{i}\\left(z^{(i)}\\right)} \\\\ &=\\sum_{i=1}^{m} \\sum_{j=1}^{k} Q_{i}\\left(z^{(i)}=j\\right) \\log \\frac{p\\left(x^{(i)} | z^{(i)}=j ; \\mu, \\Sigma\\right) p\\left(z^{(i)}=j ; \\phi\\right)}{Q_{i}\\left(z^{(i)}=j\\right)} \\\\ &=\\sum_{i=1}^{m} \\sum_{j=1}^{k} w_{j}^{(i)} \\log \\frac{\\frac{1}{(2 \\pi)^{n / 2}\\left|\\Sigma_{j}\\right|^{1 / 2}} \\exp \\left(-\\frac{1}{2}\\left(x^{(i)}-\\mu_{j}\\right)^{T} \\Sigma_{j}^{-1}\\left(x^{(i)}-\\mu_{j}\\right)\\right) \\cdot \\phi_{j}}{w_{j}^{(i)}}\\end{aligned} i=1∑m​​z(i)∑​Qi​(z(i))logQi​(z(i))p(x(i),z(i);ϕ,μ,Σ)​=i=1∑m​j=1∑k​Qi​(z(i)=j)logQi​(z(i)=j)p(x(i)∣z(i)=j;μ,Σ)p(z(i)=j;ϕ)​=i=1∑m​j=1∑k​wj(i)​logwj(i)​(2π)n/2∣Σj​∣1/21​exp(−21​(x(i)−μj​)TΣj−1​(x(i)−μj​))⋅ϕj​​​ 我们只需要分别对三个参数进行求导，即可得到： μl:=∑i=1mwl(i)x(i)∑i=1mwl(i) \\mu_{l}:=\\frac{\\sum_{i=1}^{m} w_{l}^{(i)} x^{(i)}}{\\sum_{i=1}^{m} w_{l}^{(i)}} μl​:=∑i=1m​wl(i)​∑i=1m​wl(i)​x(i)​ ϕj:=1m∑i=1mwj(i) \\phi_{j}:=\\frac{1}{m} \\sum_{i=1}^{m} w_{j}^{(i)} ϕj​:=m1​i=1∑m​wj(i)​ Σj:=∑i=1mwj(i)(x(i)−μj)(x(i)−μj)T∑i=1mwj(i) \\Sigma_{j}:=\\frac{\\sum_{i=1}^{m} w_{j}^{(i)}\\left(x^{(i)}-\\mu_{j}\\right)\\left(x^{(i)}-\\mu_{j}\\right)^{T}}{\\sum_{i=1}^{m} w_{j}^{(i)}} Σj​:=∑i=1m​wj(i)​∑i=1m​wj(i)​(x(i)−μj​)(x(i)−μj​)T​ 这也就是我们上一个博客给出的EM算法的迭代过程。 "},"fa.html":{"url":"fa.html","title":"因子分析","keywords":"","body":"这应该是学ML以来推导过的最痛苦的算法了，所以我想先用直观的语言描述什么是Factor analysis。 因子分析(factor analysis)是一种数据简化的技术。它通过研究众多变量之间的内部依赖关系，探求观测数据中的基本结构，并用少数几个假想变量来表示其基本的数据结构。这几个假想变量能够反映原来众多变量的主要信息。原始的变量是可观测的显在变量，而假想变量是不可观测的潜在变量，称为因子。 由于存在隐变量，同时不能由MLE得到close form，因此很自然的想到了之前提到的EM算法。本文主要用EM算法推到因子分析的参数估计过程。 问题 之前我们考虑的训练数据中样例 xix_ixi​ 的个数 mmm 都远远大于其特征个数 nnn，这样不管是行回归、聚类等都没有太大的问题。然而当训练样例个数 mmm 太小，甚至 mnmmn 的时候，使 用梯度下降法进行回归时，如果初值不同，得到的参数结果会有很大偏差（因为方程数小于 参数个数）。另外，如果使用多元高斯分布(Multivariate Gaussian distribution)对数据进行拟合时，也会有问题。 例如，多元高斯分布的参数估计如下： μ=1m∑i=1mx(i)Σ=1m∑i=1m(x(i)−μ)(x(i)−μ)T \\begin{array}{c}{\\mu=\\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}} \\\\ {\\Sigma=\\frac{1}{m} \\sum_{i=1}^{m}\\left(x^{(i)}-\\mu\\right)\\left(x^{(i)}-\\mu\\right)^{T}}\\end{array} μ=m1​∑i=1m​x(i)Σ=m1​∑i=1m​(x(i)−μ)(x(i)−μ)T​ 分别是求 mean 和协方差的公式，x 是 n 维向量，Σ\\SigmaΣ是 n*n 协方差矩阵。 当 mΣ\\SigmaΣ是奇异阵（ |Σ\\SigmaΣ| = 0），也就是说Σ−1\\Sigma^{-1}Σ−1 不存在，没办法拟合出多元高斯分布了，确切的说是我们估计不出来Σ\\SigmaΣ。 因此，我们可以对Σ\\SigmaΣ进行限制，从而使得其可逆。最简单的想法就是使得Σ\\SigmaΣ变为对角矩阵，但这样有很大的坏处: ..这样的假设意味着特征间相互独立，表示在图上就是contour的各个维度与坐标轴平行..。 Preliminary 首先不加证明的给出几个结论 设x=[x1x2]x=\\left[ \\begin{array}{l}{x_{1}} \\\\ {x_{2}}\\end{array}\\right]x=[x1​x2​​]，x∼N(μ,Σ)x \\sim \\mathcal{N}(\\mu, \\Sigma)x∼N(μ,Σ)，其中 μ=[μ1μ2],Σ=[Σ11Σ12Σ21Σ22]\\mu=\\left[ \\begin{array}{c}{\\mu_{1}} \\\\ {\\mu_{2}}\\end{array}\\right], \\quad \\Sigma=\\left[ \\begin{array}{cc}{\\Sigma_{11}} & {\\Sigma_{12}} \\\\ {\\Sigma_{21}} & {\\Sigma_{22}}\\end{array}\\right]μ=[μ1​μ2​​],Σ=[Σ11​Σ21​​Σ12​Σ22​​] 求条件概率x1∣x2∼N(μ1∣2,Σ1∣2)x_{1} | x_{2} \\sim \\mathcal{N}\\left(\\mu_{1|2}, \\Sigma_{1 | 2}\\right)x1​∣x2​∼N(μ1∣2​,Σ1∣2​) μ1∣2=μ1+Σ12Σ22−1(x2−μ2)Σ1∣2=Σ11−Σ12Σ22−1Σ21 \\begin{aligned} \\mu_{1|2} &=\\mu_{1}+\\Sigma_{12} \\Sigma_{22}^{-1}\\left(x_{2}-\\mu_{2}\\right) \\\\ \\Sigma_{1 | 2} &=\\Sigma_{11}-\\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\end{aligned} μ1∣2​Σ1∣2​​=μ1​+Σ12​Σ22−1​(x2​−μ2​)=Σ11​−Σ12​Σ22−1​Σ21​​ Factor analysis model 思想 因子分析的实质是认为 m 个 n 维特征的训练样例X(i)(x1(i),x2(i),…,xn(i))\\mathrm{X}^{(i)}\\left(x_{1}^{(i)}, x_{2}^{(i)}, \\ldots, x_{n}^{(i)}\\right)X(i)(x1(i)​,x2(i)​,…,xn(i)​)的产生过程： 首先在一个k维空间中按照多元高斯分布生成m个ziz^{i}zi的k维向量，即： z(i)∼N(0,I)z^{(i)} \\sim N(0, I)z(i)∼N(0,I) 然后定义一个变换矩阵Λ∈Rn×k\\Lambda \\in \\mathbb{R}^{\\mathrm{n} \\times \\mathrm{k}}Λ∈Rn×k，将z映射到n维空间中，即： Λz(i)\\Lambda z^{(i)}Λz(i) 然后将Λz(i)\\Lambda z^{(i)}Λz(i)加上一个均值μ\\muμ，即： μ+Λz(i)\\mu+\\Lambda z^{(i)}μ+Λz(i) 对应的意义是将变换后的Λz(i)\\Lambda z^{(i)}Λz(i)（n 维向量）移动到样本的中心点μ\\muμ。 最后再加入一个噪声ϵ∼N(0,Ψ)\\epsilon \\sim N(0, \\Psi)ϵ∼N(0,Ψ)，从而得到： x(i)=μ+Λz(i)+ϵ\\mathrm{x}^{(i)}=\\mu+\\Lambda z^{(i)}+\\epsilonx(i)=μ+Λz(i)+ϵ 这个过程的直观解释是：在低维空间中的随机变量，通过一个仿射变换映射到样本的高维空间，然后再加入随机误差生成。因此，高维数据可以使用低维数据表示。 联合分布 我们可以通过之前的结论得到隐变量和目标变量的联合分布： [z]x]∼N(μzx,Σ) \\left[ \\begin{array}{l}{z} ]\\\\ {x}\\end{array}\\right] \\sim \\mathcal{N}\\left(\\mu_{z x}, \\Sigma\\right) [z]x​]∼N(μzx​,Σ) 不难求得： μzx=[0→μ] \\mu_{z x}=\\left[ \\begin{array}{c}{\\overrightarrow{0}} \\\\ {\\mu}\\end{array}\\right] μzx​=[0μ​] Σ=[IΛTΛΛΛT+Ψ] \\Sigma = \\left[ \\begin{array}{cc}{I} & {\\Lambda^{T}} \\\\ {\\Lambda} & {\\Lambda \\Lambda^{T}+\\Psi}\\end{array}\\right] Σ=[IΛ​ΛTΛΛT+Ψ​] 因此MLE为： ℓ(μ,Λ,Ψ)=log⁡∏i=1m1(2π)n/2∣ΛΛT+Ψ∣1/2exp⁡(−12(x(i)−μ)T(ΛΛT+Ψ)−1(x(i)−μ)) \\ell(\\mu, \\Lambda, \\Psi)=\\log \\prod_{i=1}^{m} \\frac{1}{(2 \\pi)^{n / 2}\\left|\\Lambda \\Lambda^{T}+\\Psi\\right|^{1 / 2}} \\exp \\left(-\\frac{1}{2}\\left(x^{(i)}-\\mu\\right)^{T}\\left(\\Lambda \\Lambda^{T}+\\Psi\\right)^{-1}\\left(x^{(i)}-\\mu\\right)\\right) ℓ(μ,Λ,Ψ)=logi=1∏m​(2π)n/2∣ΛΛT+Ψ∣1/21​exp(−21​(x(i)−μ)T(ΛΛT+Ψ)−1(x(i)−μ)) 很显然，直接求解这个式子是困难的，因此我们可以使用EM算法。 EM估计 求解过程相当繁琐，大家可以自行参考CS229的官方notes。这里只给出参数估计： Λ=(∑i=1m(x(i)−μ)Ez(i)∼Qi[z(i)T])(∑i=1mEz(i)∼Qi[z(i)z(i)T])−1 \\Lambda=\\left(\\sum_{i=1}^{m}\\left(x^{(i)}-\\mu\\right) \\mathrm{E}_{z^{(i)} \\sim Q_{i}}\\left[z^{(i)^{T}}\\right]\\right)\\left(\\sum_{i=1}^{m} \\mathrm{E}_{z^{(i)} \\sim Q_{i}}\\left[z^{(i)} z^{(i)^{T}}\\right]\\right)^{-1} Λ=(i=1∑m​(x(i)−μ)Ez(i)∼Qi​​[z(i)T])(i=1∑m​Ez(i)∼Qi​​[z(i)z(i)T])−1 实际上，我们对这个仿射变换的矩阵的估计，很类似于最小二乘的结果：θT=(yTX)(XTX)−1\\theta^{T}=\\left(y^{T} X\\right)\\left(X^{T} X\\right)^{-1}θT=(yTX)(XTX)−1 这是因为，我们希望通过这个矩阵得到z和x的线性关系，因此直观的可以认为其想法类似。 同时可求得其他的参数： μ=1m∑i=1mx(i) \\mu=\\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} μ=m1​i=1∑m​x(i) Φ=1m∑i=1mx(i)x(i)T−x(i)μz(i)∣x(i)TΛT−Λμz(i)∣x(i)x(i)T+Λ(μz(i)∣x(i)μz(i)∣x(i)T+Σz(i)∣x(i))ΛT \\Phi=\\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} x^{(i) T}-x^{(i)} \\mu_{z^{(i)}\\left|x^{(i)}\\right.}^{T} \\Lambda^{T}-\\Lambda \\mu_{z^{(i)} | x^{(i)}} x^{(i)^{T}}+\\Lambda\\left(\\mu_{z^{(i)}\\left|x^{(i)}\\right.} \\mu_{z^{(i)}\\left|x^{(i)}\\right.}^{T}+\\Sigma_{z^{(i)} | x^{(i)}}\\right) \\Lambda^{T} Φ=m1​i=1∑m​x(i)x(i)T−x(i)μz(i)∣x(i)T​ΛT−Λμz(i)∣x(i)​x(i)T+Λ(μz(i)∣x(i)​μz(i)∣x(i)T​+Σz(i)∣x(i)​)ΛT 思考 因子分析与回归分析不同，因子分析中的因子是一个比较抽象的概念，而回归因子有非常明确的实际意义； 主成分分析分析与因子分析也有不同，主成分分析仅仅是变量变换，而因子分析需要构造因子模型。 主成分分析：原始变量的线性组合表示新的综合变量，即主成分； 因子分析：潜在的假想变量和随机影响变量的线性组合表示原始变量。 Reference An Introduction to Probabilistic Graphical Models by Jordan Chapter 14 CS229 "},"pca.html":{"url":"pca.html","title":"PCA","keywords":"","body":"Motivation PCA与Factor analysis非常相似，都是主要用于reduction data dimensions。但PCA的想法相比于Factor analysis更简单，实现起来也更加直观和容易（只需要算特征值）。 PCA tries to identify the subspace in which the data approximately lies. 一个很简单的例子是，我们的数据中可能存在很多属性是高度相关的，那么这些属性实际上是有冗余的，如果我们直接用原始数据进行训练，有可能会受到Curse of dimensionality，同时增大计算量，增加模型过拟合的程度。 算法流程 数据预处理 首先，我们需要对数据进行标准化：  1. Let μ=1m∑i=1mx(i) 2. Replace each x(i) with x(i)−μ 3. Let σj2=1m∑i(xj(i))2 4. Replace each xj(i) with xj(i)/σj \\begin{array}{l}{\\text { 1. Let } \\mu=\\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}} \\\\ {\\text { 2. Replace each } x^{(i)} \\text { with } x^{(i)}-\\mu} \\\\ {\\text { 3. Let } \\sigma_{j}^{2}=\\frac{1}{m} \\sum_{i}\\left(x_{j}^{(i)}\\right)^{2}} \\\\ {\\text { 4. Replace each } x_{j}^{(i)} \\text { with } x_{j}^{(i)} / \\sigma_{j}}\\end{array}  1. Let μ=m1​∑i=1m​x(i) 2. Replace each x(i) with x(i)−μ 3. Let σj2​=m1​∑i​(xj(i)​)2 4. Replace each xj(i)​ with xj(i)​/σj​​ 这里让数据的每个维度的期望变为0，方差变为1，使得不同维度具有可比性。 推导 PCA有多种推导方式，最直观的方式是最大方差和最小平方误差。 最大方差理论 在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。 一个直观的想法是，我们从mmm维的feature space投影到m−1m-1m−1维到feature subspace时，希望使得其方差能最大化的得到保留（也就是数据之间的差异性保留越多越好） 设投影到新的单位向量uuu中，那么投影点和原点的距离是xTux^TuxTu。我们的目标是求最佳的uuu，使得投影后的样本点方差最大。 由于这些样本点（样例）的每一维特征均值都为 0，因此投影到 u 上的样本点（只 有一个到原点的距离值）的均值仍然是 0。 因此我们只需要方差最大化，而方差就是投影点到原点的距离的平方和： 1m∑i=1m(x(i)Tu)2=1m∑i=1muTx(i)x(i)Tu=uT(1m∑i=1mx(i)x(i)T)u \\begin{aligned} \\frac{1}{m} \\sum_{i=1}^{m}\\left(x^{(i)^{T}} u\\right)^{2} &=\\frac{1}{m} \\sum_{i=1}^{m} u^{T} x^{(i)} x^{(i) T} u \\\\ &=u^{T}\\left(\\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}\\right) u \\end{aligned} m1​i=1∑m​(x(i)Tu)2​=m1​i=1∑m​uTx(i)x(i)Tu=uT(m1​i=1∑m​x(i)x(i)T)u​ 可以通过简单的Lagrange变换，得到目标函数的最大值就等于求最大的特征向量。 因此，我们只需要对协方差矩阵进行特征值分解，得到的前 k 大特征值对应的特征向量 就是最佳的 k 维新特征，而且这 k 维新特征是正交的。得到前 k 个 u 以后，样例X(i)X^{(i)}X(i)通过以下变换可以得到新的样本。 y(i)=[u1Tx(i)u2Tx(i)⋮ukTx(i)]∈Rk y^{(i)}=\\left[ \\begin{array}{c}{u_{1}^{T} x^{(i)}} \\\\ {u_{2}^{T} x^{(i)}} \\\\ {\\vdots} \\\\ {u_{k}^{T} x^{(i)}}\\end{array}\\right] \\in \\mathbb{R}^{k} y(i)=⎣⎢⎢⎡​u1T​x(i)u2T​x(i)⋮ukT​x(i)​⎦⎥⎥⎤​∈Rk 通过选取最大的 k 个 u，使得方差较小的特征（如噪声）被丢弃。 最小平方误差理论 回想我们最开始学习的线性回归等，目的也是求一个线性函数使得直线能够最佳拟合样本点，那么我们能不能认为最佳的直线就是回归后的直线呢？回归时我们的最小二乘法度量的是样本点到直线的坐标轴距离。 我们打算选用另外一种评价直线好坏的方法，使用点到直线的距离 d’来度量。 将样本点xkx_kxk​在直线上的投影记为xk′x_k^{'}xk′​，那么我们就是要最小化 ∑k=1n∥(xk′−xk)∥2 \\sum_{\\mathrm{k}=1}^{n}\\left\\|\\left(\\mathrm{x}_{k}^{\\prime}-x_{\\mathrm{k}}\\right)\\right\\|^{2} k=1∑n​∥(xk′​−xk​)∥2 这个公式称作最小平方误差（Least Squared Error）。 而确定一条直线，一般只需要确定一个点，并且确定方向即可。（推导可参考这里） 应用 PCA 将 n 个特征降维到 k 个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10 维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA 要保证降维后，还要保证数据的特性损失最小。 可用于数据预处理，减少feature space的大小，从而减少运算，减少overfitting。 可用于noise reduction algorithm。 Match/define better calculation 例如在人脸相似度匹配中，一个图像中属于人脸的并不是主要部分，而大部分都是噪声，因此我们可以使用PCA降维，使得某些维度能够衡量脸的形状大小等等。 在计算相似度时，将两个图像通过PCA投影到子空间，再通过距离度量。 总结 PCA 的思想是将 n 维特征映射到 k 维上（k PCA 技术的一个很大的优点是，它是完全无参数限制的。在 PCA 的计算过程中完全不 需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关， 与用户是独立的。 但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了 数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的 效果，效率也不高。 有时数据的分布并不是满足高斯分布。如图表 5 所示，在非高斯分布的情况下，PCA 方法得出的主元可能并不是最优的。在寻找主元时不能将方差作为衡量重要性的标准。 Reference A TUTORIAL ON PRINCIPAL COMPONENT ANALYSIS CS229 notes10 http://www.cmlab.csie.ntu.edu.tw/~cyy/learning/tutorials/PCAMissingData.pdf "},"recap.html":{"url":"recap.html","title":"期末复习","keywords":"","body":"统计学习概述 分类 监督学习/非监督学习/半监督学习/强化学习（非监督学习的一种） 统计学习方法的三要素 模型：模型的假设空间 在监督学习中，就是所要学习的条件概率P(Y∣X)P(Y|X)P(Y∣X)或决策函数Y=f(X)Y = f(X)Y=f(X) 策略：模型选择的准则 对应风险函数（衡量平均预测的好坏）和损失函数（衡量一次预测的好坏） 经验风险最小化：极大似然估计（容易造成过拟合） 例如：极大似然估计 结构风险最小化（对应于正则化）：加入正则化项，减少模型的复杂性 例如：Bayes最大后验估计 算法：模型学习的算法。通常需要数值计算的方法。 问题分类 回归问题：输入变量与输出变量均为连续变量 分类问题：输出变量为有限个离散变量 标注问题：输入与输出变量都是变量序列​ 监督模型类型 生成模型 GDA/Naive Bayes/HMM 可以快速还原出联合概率分布P(X,Y)P(X,Y)P(X,Y)，学习收敛速度更快 判别模型 Logistic/SVM/条件随机场 可以直接对数据进行各种程度的抽象，定义特征，简化学习问题 模型选择 假设空间中有不同复杂度的模型，需要涉及到模型选择问题。模型选择主要考虑到过拟合问题。 主要的模型选择方法： 正则化 min⁡1N∑i=1NL(yi,f(x))+λJ(f)\\min \\frac{1}{N}\\sum\\limits_{i=1}^NL(y_i,f(x)) + \\lambda J(f)minN1​i=1∑N​L(yi​,f(x))+λJ(f) 在回归问题中一般使用L1L_1L1​或者L2L_2L2​范数 交叉验证 分为训练集（用于模型训练），验证集（用于模型选择）和测试集（用于最终对学习方法的评估） 验证集用于模型的选择，选择对于验证集有最小误差的模型 简单交叉验证（前70%为训练集），S折交叉验证（前S-1个子集的数据进行训练，重复S次 ），留一交叉验证 泛化能力 衡量对未知样本的预测能力 性质 样本容量增加， 泛化误差趋于0 假设空间容量越大，泛化误差越大 泛化误差上界 ddd为假设空间大小，呈正比；NNN为样本大小，呈反比 R(f)⩽R^(f)+ε(d,N,δ)R(f) \\leqslant \\hat{R}(f)+\\varepsilon(d, N, \\delta)R(f)⩽R^(f)+ε(d,N,δ) ε(d,N,δ)=12N(log⁡d+log⁡1δ)\\varepsilon(d, N, \\delta)=\\sqrt{\\frac{1}{2 N}\\left(\\log d+\\log \\frac{1}{\\delta}\\right)}ε(d,N,δ)=2N1​(logd+logδ1​)​ 生成模型与判别模型 生成模型 对联合概率进行建模P(X,Y)=P(X∣Y)P(Y)P(X,Y) = P(X|Y) P(Y)P(X,Y)=P(X∣Y)P(Y)，根据YYY不同来学习不同的模型，使用贝叶斯得到后验概率，然后取概率较大的那个 朴素贝叶斯、HMM 可还原出联合概率分布P(X,Y)P(X,Y)P(X,Y), 而判别方法不能。 生成方法的收敛速度更快，当样本容量增加的时候，学到的模型可以更快地收敛于真实模型 当存在隐变量时， 仍可以使用生成方法，而判别方法则不能用 判别模型 直接学习条件概率分布P(Y∣X)P(Y|X)P(Y∣X) KNN、感知机、决策树、logistic回归、最大熵、SVM、AdaBoost、CRF 直接学习到条件概率或决策函数，直接进行预测，往往学习的准确率更高 由于直接学习Y=f(X)Y=f(X)Y=f(X)或 P(Y∣X)P(Y|X)P(Y∣X), 可对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习过程 评价标准 准确率（accuracy）：给定的测试数据集，分类正确的样本数与总样本数的比 精确率（precision）：P=TPTP+FPP = \\frac{TP}{TP+FP}P=TP+FPTP​ 所有预测是正类中，本身也是正类的 召回率（recall）：R=TPTP+FNR = \\frac{TP}{TP+FN}R=TP+FNTP​ 所有正类中，正确预测的 F1值：F1=2PRP+R=2TP2TP+FP+FNF_{1}=\\frac{2 P R}{P+R}=\\frac{2 T P}{2 T P+F P+F N}F1​=P+R2PR​=2TP+FP+FN2TP​ 感知机 模型 将输入空间划分为正负两类的分离超平面，属于判别模型 输入空间到输出空间的函数 f(x)=sign⁡(w⋅x+b)f(x)=\\operatorname{sign}(w \\cdot x+b)f(x)=sign(w⋅x+b) sign⁡(x)={+1,x⩾0−1,x0\\operatorname{sign}(x)=\\{\\begin{array}{ll}{+1,} & {x \\geqslant 0} \\\\ {-1,} & {xsign(x)={+1,−1,​x⩾0x0​ 集合解释 www为法向量，bbb为截距 策略 误分类点到超平面的总距离最小 对于正确分类的样本：yi(w⋅xi+b)>0y_{i}\\left(w \\cdot x_{i}+b\\right)>0yi​(w⋅xi​+b)>0 对于错误分类的样本：−yi(w⋅xi+b)>0-y_{i}\\left(w \\cdot x_{i}+b\\right)>0−yi​(w⋅xi​+b)>0 总距离（当全部正确分类时最小）：−1∥w∥∑xi∈Myi(w⋅xi+b)-\\frac{1}{\\|w\\|} \\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right)−∥w∥1​∑xi​∈M​yi​(w⋅xi​+b) 损失函数 L(w,b)=−∑x∈Myi(w⋅xi+b)L(w, b)=-\\sum_{x_{\\in M}} y_{i}\\left(w \\cdot x_{i}+b\\right)L(w,b)=−∑x∈M​​yi​(w⋅xi​+b) MMM是误分类点的数目 算法 求解最优化问题 min⁡wi,bL(w,b)=−∑xi∈Myi(w⋅xi+b) \\min _{w_{i}, b} L(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right) wi​,bmin​L(w,b)=−xi​∈M∑​yi​(w⋅xi​+b) 随机梯度下降（每次只选择一个样本进行更新参数） ∇wL(w,b)=−∑xi∈Myixi\\nabla_{w} L(w, b)=-\\sum_{x_{i} \\in M} y_{i} x_{i}∇w​L(w,b)=−∑xi​∈M​yi​xi​ ∇bL(w,b)=−∑x∈Myi\\nabla_{b} L(w, b)=-\\sum_{x \\in M} y_{i}∇b​L(w,b)=−∑x∈M​yi​ 优化：w←w+ηyixib←b+ηyiw \\leftarrow w+\\eta y_{i} x_{i}\\quad b \\leftarrow b+\\eta y_{i}w←w+ηyi​xi​b←b+ηyi​ 解并不惟一，与初始点选择有关 要求：必须是线性可分的，则有限次迭代一定可以完全正确划分的分离超平面 看例子 对偶形式 我们可以看到，最后求得的参数w,bw,bw,b一定是y,xy,xy,x的线性组合，即w=∑i=1Nαiyixiw=\\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i}w=∑i=1N​αi​yi​xi​，b=∑i=1Nαiyib=\\sum_{i=1}^{N} \\alpha_{i} y_{i}b=∑i=1N​αi​yi​ 最后学习到的模型为：f(x)=sign⁡(∑j=1Nαjyjxj⋅x+b)f(x)=\\operatorname{sign}\\left(\\sum_{j=1}^{N} \\alpha_{j} y_{j} x_{j} \\cdot x+b\\right)f(x)=sign(∑j=1N​αj​yj​xj​⋅x+b) 算法步骤 初始值：α←0,b←0\\alpha \\leftarrow 0, \\quad b \\leftarrow 0α←0,b←0 选择某个数据点，若yi(∑j=1Nαjyjxj⋅xi+b)⩽0y_{i}\\left(\\sum_{j=1}^{N} \\alpha_{j} y_{j} x_{j} \\cdot x_{i}+b\\right) \\leqslant 0yi​(∑j=1N​αj​yj​xj​⋅xi​+b)⩽0，则更新： αi←αi+η\\alpha_{i} \\leftarrow \\alpha_{i}+\\etaαi​←αi​+η、b←b+ηyib \\leftarrow b+\\eta y_{i}b←b+ηyi​ 看例子 KNN 模型 优点 精度高 对异常值不敏感 无数据输入假定 缺点 计算复杂度高 空间复杂度高 适用数据范围 数值型（回归）和标称型（分类） 距离度量 欧式距离：L2(xi,xj)=(∑l=1n∣xi(l)−xj(l)∣2)12L_{2}\\left(x_{i}, x_{j}\\right)=\\left(\\sum_{l=1}^{n}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|^{2}\\right)^{\\frac{1}{2}}L2​(xi​,xj​)=(∑l=1n​∣∣∣​xi(l)​−xj(l)​∣∣∣​2)21​ 曼哈顿距离：L1(xi,xj)=∑l=1n∣xi(l)−xj(l)∣L_{1}\\left(x_{i}, x_{j}\\right)=\\sum_{l=1}^{n}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|L1​(xi​,xj​)=∑l=1n​∣∣∣​xi(l)​−xj(l)​∣∣∣​ L∞\\infty∞距离：L∞(xi,xj)=max⁡l∣xi(l)−xj(l)∣L_{\\infty}\\left(x_{i}, x_{j}\\right)=\\max _{l}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|L∞​(xi​,xj​)=maxl​∣∣∣​xi(l)​−xj(l)​∣∣∣​ K值的选择 较小的K值 学习的近似误差会减小 ，但学习的估计误差会增大 噪声敏感：意味着模型变得复杂，容易发生过拟合 较大的K值 减少学习的估计误差，学习的近似误差增加 模型变得简单 策略 多数表决规则（等价经验风险最小化） 算法实现 构造KD树 搜索KD树 朴素贝叶斯 模型 认为数据集由XXX和YYY的联合概率分布P(X,Y)P(X,Y)P(X,Y)独立同分布产生 独立性假设 P(X=x∣Y=ck)=P(X(1)=x(1),⋯,X(n)=x(n)∣Y=ck)=∏j=1nP(X(j)=x(j)∣Y=ck)\\begin{aligned} P\\left(X=x | Y=c_{k}\\right) &=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right) \\\\ &=\\prod_{j=1}^{n} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) \\end{aligned}P(X=x∣Y=ck​)​=P(X(1)=x(1),⋯,X(n)=x(n)∣Y=ck​)=j=1∏n​P(X(j)=x(j)∣Y=ck​)​ 可以减少大量的参数估计，牺牲分类的准确性 策略 构造贝叶斯分类器 y=f(x)=arg⁡max⁡ckP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)∑kP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)y=f(x)=\\arg \\max _{c_{k}} \\frac{P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)}{\\sum_{k} P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)}y=f(x)=argck​max​∑k​P(Y=ck​)∏j​P(X(j)=x(j)∣Y=ck​)P(Y=ck​)∏j​P(X(j)=x(j)∣Y=ck​)​ 分母对于所有的ckc_kck​都相同，因此只需要比较分子 y=arg⁡max⁡ckP(Y=ck)∏jP(X(j)=x(j)∣Y=ck)y=\\arg \\max _{c_{k}} P\\left(Y=c_{k}\\right) \\prod_{j} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)y=argck​max​P(Y=ck​)j∏​P(X(j)=x(j)∣Y=ck​) 后验概率最大化的含义 朴素贝叶斯法将实例分到后验概率最大的类中，等价于期望风险最小化 期望风险函数（在整个样本空间中，L(Y,f(X))L(Y,f(X))L(Y,f(X))是0-1损失） Rexp⁡(f)=E[L(Y,f(X))]R_{\\exp }(f)=E[L(Y, f(X))]Rexp​(f)=E[L(Y,f(X))] 取条件期望 Rexp(f)=EX∑k=1K[L(ck,f(X))]P(ck∣X)R_{\\mathrm{exp}}(f)=E_{X} \\sum_{k=1}^{K}\\left[L\\left(c_{k}, f(X)\\right)\\right] P\\left(c_{k} | X\\right)Rexp​(f)=EX​∑k=1K​[L(ck​,f(X))]P(ck​∣X) 可以推导出，期望风险最小化等价于最大后验概率 算法 参数估计 估计先验概率：P(Y=ck)=∑i=1NI(yi=ck)NsP\\left(Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)}{N}sP(Y=ck​)=N∑i=1N​I(yi​=ck​)​s 估计条件概率：P(X(j)=ajl∣Y=ck)=∑i=1NI(xi(j)=ajlyi=ck)∑i=1NI(yi=ck)P\\left(X^{(j)}=a_{j l} | Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(x_{i}^{(j)}=a_{j l} y_{i}=c_{k}\\right)}{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)}P(X(j)=ajl​∣Y=ck​)=∑i=1N​I(yi​=ck​)∑i=1N​I(xi(j)​=ajl​yi​=ck​)​ 对于给定的实例，确定XXX的类别 y=arg⁡max⁡aP(Y=ck)∏j=1nP(X(j)=x(j)∣Y=ck)y=\\arg \\max _{a} P\\left(Y=c_{k}\\right) \\prod_{j=1}^{n} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right)y=argamax​P(Y=ck​)j=1∏n​P(X(j)=x(j)∣Y=ck​) 为了防止概率为0的情况，采用贝叶斯估计 先验概率的贝叶斯估计：Pλ(Y=ck)=∑i=1NI(yi=ck)+λN+KλP_{\\lambda}\\left(Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)+\\lambda}{N+K \\lambda}Pλ​(Y=ck​)=N+Kλ∑i=1N​I(yi​=ck​)+λ​ 条件概率的贝叶斯估计：Pλ(X(j)=ajt∣Y=ck)=∑i=1NI(xi(j)=ajlyi=ck)+λ∑i=1NI(yi=ck)+SiλP_{\\lambda}\\left(X^{(j)}=a_{j t} | Y=c_{k}\\right)=\\frac{\\sum_{i=1}^{N} I\\left(x_{i}^{(j)}=a_{j l} y_{i}=c_{k}\\right)+\\lambda}{\\sum_{i=1}^{N} I\\left(y_{i}=c_{k}\\right)+S_{i} \\lambda}Pλ​(X(j)=ajt​∣Y=ck​)=∑i=1N​I(yi​=ck​)+Si​λ∑i=1N​I(xi(j)​=ajl​yi​=ck​)+λ​ 当λ=1\\lambda = 1λ=1时为拉普拉斯平滑 Logistic回归 模型 由条件概率P(Y∣X)P(Y|X)P(Y∣X)表示的分类模型 P(Y=1∣x)=exp⁡(w⋅x+b)1+exp⁡(w⋅x+b)P(Y=1 | x)=\\frac{\\exp (w \\cdot x+b)}{1+\\exp (w \\cdot x+b)}P(Y=1∣x)=1+exp(w⋅x+b)exp(w⋅x+b)​ P(Y=0∣x)=11+exp⁡(w⋅x+b)P(Y=0 | x)=\\frac{1}{1+\\exp (w \\cdot x+b)}P(Y=0∣x)=1+exp(w⋅x+b)1​ 对数几率 log⁡P(Y=1∣x)1−P(Y=1∣x)=w⋅x\\log \\frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \\cdot xlog1−P(Y=1∣x)P(Y=1∣x)​=w⋅x 对数几率是输入的线性组合 策略 基于似然函数得到模型的最优解 ∏i=1N[π(xi)]yi[1−π(xi)]1−yi\\prod_{i=1}^{N}\\left[\\pi\\left(x_{i}\\right)\\right]^{y_{i}}\\left[1-\\pi\\left(x_{i}\\right)\\right]^{1-y_{i}}∏i=1N​[π(xi​)]yi​[1−π(xi​)]1−yi​ 对数似然函数 L(w)=∑i=1N[yilog⁡π(xi)+(1−yi)log⁡(1−π(xi))]=∑i=1N[yilog⁡π(xi)1−π(xi)+log⁡(1−π(xi))]=∑i=1N[yi(w⋅xi)−log⁡(1+exp⁡(w⋅xi)] \\begin{aligned} L(w) &=\\sum_{i=1}^{N}\\left[y_{i} \\log \\pi\\left(x_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\pi\\left(x_{i}\\right)\\right)\\right] \\\\ &=\\sum_{i=1}^{N}\\left[y_{i} \\log \\frac{\\pi\\left(x_{i}\\right)}{1-\\pi\\left(x_{i}\\right)}+\\log \\left(1-\\pi\\left(x_{i}\\right)\\right)\\right] \\\\ &=\\sum_{i=1}^{N}\\left[y_{i}\\left(w \\cdot x_{i}\\right)-\\log \\left(1+\\exp \\left(w \\cdot x_{i}\\right)\\right]\\right.\\end{aligned} L(w)​=i=1∑N​[yi​logπ(xi​)+(1−yi​)log(1−π(xi​))]=i=1∑N​[yi​log1−π(xi​)π(xi​)​+log(1−π(xi​))]=i=1∑N​[yi​(w⋅xi​)−log(1+exp(w⋅xi​)]​ 算法 对对数似然函数求极大值，得到www的估计值。 扩展为多项logistic回归模型： P(Y=k∣x)=exp⁡(wk⋅x)1+∑k=1K−1exp⁡(wk⋅x)P(Y=k | x)=\\frac{\\exp \\left(w_{k} \\cdot x\\right)}{1+\\sum_{k=1}^{K-1} \\exp \\left(w_{k} \\cdot x\\right)}P(Y=k∣x)=1+∑k=1K−1​exp(wk​⋅x)exp(wk​⋅x)​ sigmoid是softmax的二维情形 很容易可以证明，无论是分类问题（multinomial）还是回归问题（正态分布），都可以转换为指数族的形式 通过指数族的形式，我们可以发现，在线性假设下，我们之前的logistic回归的sigmoid方程其实就是P(Y∣X)P(Y|X)P(Y∣X)为Bernoulli分布的指数族形式下的充分统计量的参数表示。 SVM 模型 学习目标 找到分类超平面 感知机：错分的样本尽可能少 分类 线性可分支持向量机：通过间隔最大化或等价地求解相应的凸二次规划问题学习得到分离超平面 非线性支持向量机：利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量 决策函数 f(x)=sign⁡(w∗⋅x+b∗)f(x)=\\operatorname{sign}\\left(w^* \\cdot x+b^*\\right)f(x)=sign(w∗⋅x+b∗) 策略 线性可分 思想 通过点到超平面的距离，得到硬间隔最大化，该超平面能把正负样本分的最开 函数间隔（γ(i)^=y(i)(wTx+b)\\hat{\\gamma^{(i)}} = y^{(i)}(w^Tx + b)γ(i)^​=y(i)(wTx+b)，用来衡量分类的确信程度和正确程度） -> 几何间隔（得到一个不随着scale变化的指标） 最大化集合间隔和最小化12∥w∥2\\frac{1}{2}\\|w\\|^{2}21​∥w∥2等价 当目标函数为二次函数，g函数为仿射函数时，为凸二次规划问题，局部最优解就是全局最优解。 原始问题建模（记） min⁡w,b12∥w∥2 s.t. yi(w⋅xi+b)−1⩾0,i=1,2,⋯,N \\begin{array}{cl}{\\min _{w, b}} & {\\frac{1}{2}\\|w\\|^{2}} \\\\ {\\text { s.t. }} & {y_{i}\\left(w \\cdot x_{i}+b\\right)-1 \\geqslant 0, \\quad i=1,2, \\cdots, N}\\end{array} minw,b​ s.t. ​21​∥w∥2yi​(w⋅xi​+b)−1⩾0,i=1,2,⋯,N​ 当yi(w⋅xi+b)=1y_i(w\\cdot x_i + b) =1yi​(w⋅xi​+b)=1时是support vector 对偶问题（记）（如何得到？） min⁡12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)−∑i=1Nαi s.t. ∑i=1Nαiyi=0αi⩾0,i=1,2,⋯,N \\begin{array}{l}{\\min \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i}} \\\\ {\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0} \\\\ {\\qquad \\alpha_{i} \\geqslant 0, \\quad i=1,2, \\cdots, N}\\end{array} min21​∑i=1N​∑j=1N​αi​αj​yi​yj​(xi​⋅xj​)−∑i=1N​αi​ s.t. ∑i=1N​αi​yi​=0αi​⩾0,i=1,2,⋯,N​ 由KKT条件，w∗=∑i=1Nαi∗yixiw^*=\\sum_{i=1}^{N} \\alpha_{i}^* y_{i} x_{i}w∗=∑i=1N​αi∗​yi​xi​，b∗=yj−∑i=1Nαi∗yi(xi⋅xj)b^*=y_{j}-\\sum_{i=1}^{N} \\alpha_{i}^* y_{i}\\left(x_{i} \\cdot x_{j}\\right)b∗=yj​−∑i=1N​αi∗​yi​(xi​⋅xj​) 决策函数：f(x)=sign⁡(∑i=1Nαi∗yi(x⋅xi)+b∗)f(x)=\\operatorname{sign}\\left(\\sum_{i=1}^{N} \\alpha_{i}^* y_{i}\\left(x \\cdot x_{i}\\right)+b^*\\right)f(x)=sign(∑i=1N​αi∗​yi​(x⋅xi​)+b∗) 特点：决策函数只依赖于输入样本的内积 求得了α\\alphaα，当αj>0\\alpha_j>0αj​>0时对应的jjj是support vector（真正对www有用的） 线性不可分 对于outlier，引进松弛变量，约束变为：yi(w⋅xi+b)⩾1−ξiy_{i}\\left(w \\cdot x_{i}+b\\right) \\geqslant 1-\\xi_{i}yi​(w⋅xi​+b)⩾1−ξi​ 目标函数变为：12∥w∥2+C∑j=1Nξi\\frac{1}{2}\\|w\\|^{2}+C \\sum_{j=1}^{N} \\xi_{i}21​∥w∥2+C∑j=1N​ξi​ 原始问题建模（记） min⁡w,b,ξ12∥w∥2+C∑i=1Nξi s.t. yi(w⋅xi+b)⩾1−ξi,i=1,2,⋯,Nξi⩾0,i=1,2,⋯,N \\begin{aligned} \\min _{w, b, \\xi} & \\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N} \\xi_{i} \\\\ \\text { s.t. } & y_{i}\\left(w \\cdot x_{i}+b\\right) \\geqslant 1-\\xi_{i}, \\quad i=1,2, \\cdots, N \\\\ & \\xi_{i} \\geqslant 0, \\quad i=1,2, \\cdots, N \\end{aligned} w,b,ξmin​ s.t. ​21​∥w∥2+Ci=1∑N​ξi​yi​(w⋅xi​+b)⩾1−ξi​,i=1,2,⋯,Nξi​⩾0,i=1,2,⋯,N​ 其中，www由唯一解，而bbb不是 分类超平面为：w∗⋅x+b∗=0w^* \\cdot x+b^*=0w∗⋅x+b∗=0 决策函数：f(x)=sign⁡(w∗⋅x+b∗)f(x)=\\operatorname{sign}\\left(w^* \\cdot x+b^*\\right)f(x)=sign(w∗⋅x+b∗) 对偶问题（记） min⁡α12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)−∑i=1Nαi s.t. ∑i=1Nαiyi=00⩽αi⩽C,i=1,2,⋯,N \\begin{array}{cl}{\\min _{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i}} \\\\ {\\text { s.t. } \\quad \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0} \\\\ {\\qquad 0 \\leqslant \\alpha_{i} \\leqslant C, \\quad i=1,2, \\cdots, N}\\end{array} minα​21​∑i=1N​∑j=1N​αi​αj​yi​yj​(xi​⋅xj​)−∑i=1N​αi​ s.t. ∑i=1N​αi​yi​=00⩽αi​⩽C,i=1,2,⋯,N​ 其中，www由唯一解，而bbb不是 若αi∗ C\\alpha_{i}^* αi∗​ C，则ξi=0\\xi_{i}=0ξi​=0 => 在支持向量上 若αi∗=C\\alpha_{i}^* = Cαi∗​=C，0ξi100ξi​1 => 在该分类同侧 若αi∗=C\\alpha_{i}^*=Cαi∗​=C，ξi=1\\xi_{i}=1ξi​=1 => 在超平面上 若αi∗=C\\alpha_{i}^*=Cαi∗​=C，ξi>1\\xi_{i}>1ξi​>1 => 在相反的分类上 Hinge loss 另外一种解释，最小化目标函数 ∑i=1N[1−yi(w⋅xi+b)]++λ∥w∥2\\sum_{i=1}^{N}\\left[1-y_{i}\\left(w \\cdot x_{i}+b\\right)\\right]_{+}+\\lambda\\|w\\|^{2}∑i=1N​[1−yi​(w⋅xi​+b)]+​+λ∥w∥2 其中L(y(w⋅x+b))=[1−y(w⋅x+b)]+L(y(w \\cdot x+b))=[1-y(w \\cdot x+b)]_{+}L(y(w⋅x+b))=[1−y(w⋅x+b)]+​为Hinge loss 与原始问题目标函数等价 非线性支持向量机 思想 进行一个非线性变换，将原空间的数据映射到新空间，在新空间内用线性分类学习方法学习模型 核方法 核函数：K(x,z)=ϕ(x)⋅ϕ(z)K(x, z)=\\phi(x) \\cdot \\phi(z)K(x,z)=ϕ(x)⋅ϕ(z) 在学习与预测中只定义核函数，而不显式的定义映射函数，直接计算核函数比较容易（也就是完成在高维空间中的点积运算） 由于在对偶问题中，如论是目标函数还是决策函数，都只涉及到内积运算，因此可直接用核函数替代 多项式核函数：K(x,z)=(x⋅z+1)pK(x, z)=(x \\cdot z+1)^{p}K(x,z)=(x⋅z+1)p 高斯核（RBF）（映射到无穷维）：K(x,z)=exp⁡(−∥x−z∥22σ2)K(x, z)=\\exp \\left(-\\frac{\\|x-z\\|^{2}}{2 \\sigma^{2}}\\right)K(x,z)=exp(−2σ2∥x−z∥2​) 算法 SMO 虽然SVM为凸二次规划问题，有全局最优解，但样本容量很大时难以求解。 思想 每次只同时优化两个变量：其中一个是违反KKT条件最严重的一个，另一个由约束条件自动确定 可以看作是Coordinate Ascent算法 AdaBoost 提升方法 思路：要找到一个比随机猜测略好的弱学习算法，就可以直接将其提升为强学习算法 实现思路 使用不同的弱学习算法得到不同的基学习器 参数估计、非参数估计 使用相同的弱学习算法，但用不同的参数 K-mean中的K 使用不同的训练集 bagging/boosting 如何组合 并行结构（voting） 串型结构（cascading） 模型 每一轮如何改变训练数据的权值或概率分布 提高哪些被前一轮弱分类器错误分类样本的权值，降低哪些被正确分类样本的权值。 如何将弱分类器组合成一个强分类器 加权多数表决。加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值。 算法步骤 初始化训练数据的起始权值分布 D1=(w11,⋯,w1i,⋯,w1N)w1i=1N,i=1,2,⋯,ND_{1}=\\left(w_{11}, \\cdots, w_{1 i}, \\cdots, w_{1 N}\\right) \\quad w_{1 i}=\\frac{1}{N}, \\quad i=1,2, \\cdots, ND1​=(w11​,⋯,w1i​,⋯,w1N​)w1i​=N1​,i=1,2,⋯,N 对m个弱分类器，在权值DmD_mDm​下训练数据集，得到弱分类器 计算当前弱分类器的训练误差（记）（即分类错误的加权数），选择误差分类率最低的进行划分 wmiw_{mi}wmi​表示第m轮迭代下，第i个样本的权重，则∑i=1Nwmi=1\\sum_{i=1}^{N} w_{m i}=1∑i=1N​wmi​=1 em=P(Gm(xi)≠yi)=∑Gm(xi)≠yiwmie_{m}=P\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right)=\\sum_{G_{m}\\left(x_{i}\\right) \\neq y_{i}} w_{m i}em​=P(Gm​(xi​)≠yi​)=∑Gm​(xi​)≠yi​​wmi​ 计算当前分类器的系数（记） αm=12log⁡1−emem\\alpha_{m}=\\frac{1}{2} \\log \\frac{1-e_{m}}{e_{m}}αm​=21​logem​1−em​​ 当em⩽12e_{m} \\leqslant \\frac{1}{2}em​⩽21​时，αm⩾0\\alpha_{m} \\geqslant 0αm​⩾0（也就是说，需要精度比随机猜测好，才能起正的作用） 更新训练集的权值分布（记） wm+1,i={wmiZme−αm,Gm(xi)=yiwmiZmeαm,Gm(xi)≠yiw_{m+1, i}=\\left\\{\\begin{array}{ll} \t\t {\\frac{w_{m i}}{Z_{m}} e^{-\\alpha_{m}},} & {G_{m}\\left(x_{i}\\right)=y_{i}} \\\\ \t\t {\\frac{w_{m i}}{Z_{m}} e^{\\alpha_{m}},} & {G_{m}\\left(x_{i}\\right) \\neq y_{i}} \t\t \\end{array}\\right.wm+1,i​={Zm​wmi​​e−αm​,Zm​wmi​​eαm​,​Gm​(xi​)=yi​Gm​(xi​)≠yi​​ 构建弱分类器的线性组合 f(x)=∑m=1MαmGm(x)f(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)f(x)=∑m=1M​αm​Gm​(x) 得到最终的分类器 G(x)=sign⁡(f(x))=sign⁡(∑m=1MαmGm(x))G(x)=\\operatorname{sign}(f(x))=\\operatorname{sign}\\left(\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)\\right)G(x)=sign(f(x))=sign(∑m=1M​αm​Gm​(x)) 策略 AdaBoost的训练误差为指数下降 目标是每次迭代，选择在当前数据权重下，误差分类率最低的。 如何得到权重和系数的计算公式？ 每次根据指数损失最小化可以得到 算法 AdaBoost是前向分布加法算法的特例，模型是由基分类器组成的加法模型，损失函数是指数函数 HMM 是关于时序的概率模型 描述由一个隐藏的马尔可夫链随机生成的不可观测的状态随机(state)序列，再由各个状态生成一个观测而产生观测随机序列(observe)的过程，序列的每个位置可以看作是一个时刻。 模型 组成 初始概率分布 πi=P(i1=qi),i=1,2,⋯,N\\pi_{i}=P\\left(i_{1}=q_{i}\\right), \\quad i=1,2, \\cdots, Nπi​=P(i1​=qi​),i=1,2,⋯,N 时刻t=1t = 1t=1处于状态qiq_iqi​的概率 状态转移概率矩阵 A=[aij]N×NA=\\left[a_{i j}\\right]_{N \\times N}A=[aij​]N×N​，其中NNN是状态集合元素个数 aij=P(it+1=qj∣it=qi),i=1,2,⋯,N;j=1,2,⋯,Na_{i j}=P\\left(i_{t+1}=q_{j} | i_{t}=q_{i}\\right), \\quad i=1,2, \\cdots, N ; j=1,2, \\cdots, Naij​=P(it+1​=qj​∣it​=qi​),i=1,2,⋯,N;j=1,2,⋯,N 时刻ttt处于状态qiq_iqi​的条件下，在时刻t+1t+1t+1转移到状态qjq_jqj​的概率 观测概率分布（发射概率） B=[bj(k)]N×MB=\\left[b_{j}(k)\\right]_{N \\times M}B=[bj​(k)]N×M​，其中MMM是观测集合元素个数 bj(k)=P(ot=vk∣it=qj),k=1,2,⋯,M;j=1,2,⋯,Nb_{j}(k)=P\\left(o_{t}=v_{k} | i_{t}=q_{j}\\right), \\quad k=1,2, \\cdots, M ; j=1,2, \\cdots, Nbj​(k)=P(ot​=vk​∣it​=qj​),k=1,2,⋯,M;j=1,2,⋯,N 在时刻ttt处于状态qjq_jqj​的条件下生成观测vkv_kvk​的概率 三要素：λ=(A,B,π)\\lambda=(A, B, \\pi)λ=(A,B,π) 基本假设 齐次马尔可夫假设：ttt状态只与t−1t-1t−1状态有关 观测独立性假设：观测只与当前时刻状态有关 盒子与球模型 状态集合：Q = {盒子1，盒子2，盒子3，盒子4}，N=4 观测集合：V = {红球，白球}，M=2 策略/算法 概率计算问题 给定λ\\lambdaλ和观测序列，求状态序列OOO 直接计算 算每一个state对应的观测序列：O(T)O(T)O(T) 乘积的形式 有多少个state序列：O(NT)O(N^T)O(NT) 不同状态序列可能产生相同的观测序列（求和） 复杂度O(TNT)O(TN^T)O(TNT)，太高 前向算法 定义到时刻ttt部分观测序列为o1,…,oto_1,…,o_to1​,…,ot​，且状态为qiq_iqi​到概率为前向概率 αt(i)=P(o1,o2,⋯,ot,it=qi∣λ)\\alpha_{t}(i)=P\\left(o_{1}, o_{2}, \\cdots, o_{t}, i_{t}=q_{i} | \\lambda\\right)αt​(i)=P(o1​,o2​,⋯,ot​,it​=qi​∣λ) t−1t-1t−1之前的状态不关心，且已经累加到αt(i)\\alpha_t(i)αt​(i)中 初始值 α1(i)=πibi(o1),i=1,2,⋯,N\\alpha_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\qquad i=1,2, \\cdots, Nα1​(i)=πi​bi​(o1​),i=1,2,⋯,N 递推 αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1),i=1,2,⋯,N\\alpha_{t+1}(i)=\\left[\\sum_{j=1}^{N} \\alpha_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, Nαt+1​(i)=[∑j=1N​αt​(j)aji​]bi​(ot+1​),i=1,2,⋯,N ttt时刻为jjj，转移到状态iii，再乘以发射概率 终止 P(O∣λ)=∑i=1NαT(i)P(O | \\lambda)=\\sum_{i=1}^{N} \\alpha_{T}(i)P(O∣λ)=∑i=1N​αT​(i) 看例子 后向算法 定义在时刻ttt状态为qiq_iqi​到条件下，从t+1t+1t+1到TTT的部分观测序列的概率为后向概率 βt(i)=P(ot+1,ot+2,⋯,oT∣it=qi,λ)\\beta_{t}(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_{T} | i_{t}=q_{i}, \\lambda\\right)βt​(i)=P(ot+1​,ot+2​,⋯,oT​∣it​=qi​,λ) 初始值 βT(i)=1,i=1,2,⋯,N\\beta_{T}(i)=1, \\quad i=1,2, \\cdots, NβT​(i)=1,i=1,2,⋯,N 递推 βt(i)=∑j=1Naijbj(ot+1)βt+1(j),i=1,2,⋯,N\\beta_{t}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad i=1,2, \\cdots, Nβt​(i)=∑j=1N​aij​bj​(ot+1​)βt+1​(j),i=1,2,⋯,N 对时刻t+1t+1t+1下的所有观测值求和，得到ttt时刻观测为iii的概率 终止 P(O∣λ)=∑i=1Nπibi(o1)β1(i)P(O | \\lambda)=\\sum_{i=1}^{N} \\pi_{i} b_{i}\\left(o_{1}\\right) \\beta_{1}(i)P(O∣λ)=∑i=1N​πi​bi​(o1​)β1​(i) 向前向后统一写为 P(O∣λ)=∑i=1N∑j=1Nαt(i)aijbj(ot+1)βt+1(j),t=1,2,⋯,T−1P(O | \\lambda)=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad t=1,2, \\cdots, T-1P(O∣λ)=∑i=1N​∑j=1N​αt​(i)aij​bj​(ot+1​)βt+1​(j),t=1,2,⋯,T−1 学习问题 已知状态序列OOO，估计λ\\lambdaλ，使得P(O∣λ)P(O|\\lambda)P(O∣λ)最大 监督学习 如果训练数据包含观测序列OOO个对应的状态序列III，则可以利用极大似然估计对参数进行估计 状态转移概率估计（直接用频率代替） a^ij=Ay∑j=1NAij,i=1,2,⋯,N;j=1,2,⋯,N\\hat{a}_{i j}=\\frac{A_{y}}{\\sum_{j=1}^{N} A_{i j}}, \\quad i=1,2, \\cdots, N ; j=1,2, \\cdots, Na^ij​=∑j=1N​Aij​Ay​​,i=1,2,⋯,N;j=1,2,⋯,N 观测概率估计（直接用频率代替） b^j(k)=Bjk∑k=1MBjk\\hat{b}_{j}(k)=\\frac{B_{j k}}{\\sum_{k=1}^{M} B_{j k}}b^j​(k)=∑k=1M​Bjk​Bjk​​ 初始状态概率估计 直接用S个样本中初始状态为qiq_iqi​的频率 非监督学习 假设训练数据只包括{O1,…,Os}\\{O_1,…,O_s\\}{O1​,…,Os​}，使用Baum-Welch算法 实际上是含有隐变量III的概率模型，使用EM算法 预测问题 求似的λ=(A,B,π)\\lambda=(A, B, \\pi)λ=(A,B,π)最大的状态序列O=(o1,o2,⋯,oT)O=\\left(o_{1}, o_{2}, \\cdots, o_{T}\\right)O=(o1​,o2​,⋯,oT​) 近似算法 在每个时刻ttt选择在该时刻最有可能出现的状态iti_tit​，从而得到一个状态序列 时刻ttt为状态iii的可能概率为 γt(i)=αt(i)βt(i)P(O∣λ)=αt(i)βt(i)∑j=1Nαt(j)βt(j)\\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O | \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)}γt​(i)=P(O∣λ)αt​(i)βt​(i)​=∑j=1N​αt​(j)βt​(j)αt​(i)βt​(i)​ 选择最有可能的状态序列 it∗=arg⁡max⁡1⩽i⩽N[γt(i)],t=1,2,⋯,Ti_{t}^*=\\arg \\max _{1 \\leqslant i \\leqslant N}\\left[\\gamma_{t}(i)\\right], \\quad t=1,2, \\cdots, Tit∗​=argmax1⩽i⩽N​[γt​(i)],t=1,2,⋯,T 因此： I∗=(i⃗1∗,i2∗,⋯,iT∗)I^{*}=\\left(\\vec{i}_{1}^{*}, i_{2}^{*}, \\cdots, i_{T}^{*}\\right)I∗=(i1∗​,i2∗​,⋯,iT∗​) 不是全局最优解 Viterbi算法 用动态规划求解概率最大的路径，一个路径对应一个状态序列 特点：当存在全局最优解时，局部一定最优 算法 从时刻t=1t=1t=1开始，递推的计算在时刻ttt状态为iii的各个部分路径的最大值，直至得到时刻t=Tt=Tt=T状态为iii的各条路径的最大概率，得到最优路径的概率和终点。 为了找出最优路径的各个节点，从终点开始，由后向前求得路径 定义在时刻ttt状态为iii的所有单个路径中概率最大值 δt(i)=max⁡i1,i2,⋯,ik−1P(it=i,it−1,⋯,i1,ot,⋯,o1∣λ),i=1,2,⋯,N\\boldsymbol{\\delta}_{t}(i)=\\max _{i_{1}, i_{2}, \\cdots, i_{k-1}} P\\left(i_{t}=i, i_{t-1}, \\cdots, i_{1}, o_{t}, \\cdots, o_{1} | \\lambda\\right), \\quad i=1,2, \\cdots, Nδt​(i)=i1​,i2​,⋯,ik−1​max​P(it​=i,it−1​,⋯,i1​,ot​,⋯,o1​∣λ),i=1,2,⋯,N 递推公式 δt+1(i)=max⁡i1,i2,⋯,itP(it+1=i,it,⋯,i1,ot+1,…,o1∣λ)=max⁡1⩽j⩽N[δt(j)aji]bi(ot+1)\\begin{aligned} \\delta_{t+1}(i) &=\\max _{i_{1}, i_{2}, \\cdots, i_t} P\\left(i_{t+1}=i, i_{t}, \\cdots, i_{1},o_{t+1},…,o_1|\\lambda)\\right.\\\\ &=\\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right) \\end{aligned}δt+1​(i)​=i1​,i2​,⋯,it​max​P(it+1​=i,it​,⋯,i1​,ot+1​,…,o1​∣λ)=1⩽j⩽Nmax​[δt​(j)aji​]bi​(ot+1​)​ 也就是从jjj到iii到所有状态转移中的最大值 定义在时刻ttt状态为iii的所有单个路径中概率最大的路径的第t−1t-1t−1个节点为 ψt(i)=arg⁡max⁡1≤j≤N[δt−1(j)aji],i=1,2,⋯,N\\psi_{t}(i)=\\arg \\max _{1 \\leq j \\leq N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, Nψt​(i)=argmax1≤j≤N​[δt−1​(j)aji​],i=1,2,⋯,N 流程 初始化 δ1(i)=πibi(o1),i=1,2,⋯,Nψ1(i)=0,i=1,2,⋯,N\\begin{array}{cc}{\\delta_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right),} & {i=1,2, \\cdots, N} \\\\ {\\psi_{1}(i)=0,} & {i=1,2, \\cdots, N}\\end{array}δ1​(i)=πi​bi​(o1​),ψ1​(i)=0,​i=1,2,⋯,Ni=1,2,⋯,N​ 递推 δt(i)=max⁡1⩽j≤N[δt−1(j)aji]bi(ot)\\delta_{t}(i)=\\max _{1 \\leqslant j \\leq N}\\left[\\delta_{t-1}(j) a_{j i}\\right] b_{i}\\left(o_{t}\\right)δt​(i)=1⩽j≤Nmax​[δt−1​(j)aji​]bi​(ot​) ψt(i)=arg⁡max⁡1⩽j⩽N[δt−1(j)aji]\\psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right]ψt​(i)=arg1⩽j⩽Nmax​[δt−1​(j)aji​] 终止 P∗=max⁡1≤i⩽NδT(i)P^{_*}=\\max _{\\mathbf{1} \\leq i \\leqslant N} \\delta_{T}(i)P∗​=1≤i⩽Nmax​δT​(i) iT∗=arg⁡max⁡1⩽i≤N[δT(i)]i_{T}^{_*}=\\arg \\max _{1 \\leqslant i \\leq N}\\left[\\delta_{T}(i)\\right]iT∗​​=arg1⩽i≤Nmax​[δT​(i)] 最优路径回朔 it∗=ψt+1(it+1∗)i_{t}^{_*}=\\psi_{t+1}\\left(i_{t+1}^{_*}\\right)it∗​​=ψt+1​(it+1∗​​) 例子 一些概念和结论 EM算法 适用于存在隐变量的情况 GMM / HMM 中的Baum-Welch算法 / E step 每次得到似然函数lll的下界（Jessen不等式） 实际上是计算后验概率，也就是QQQ函数 M step 使用极大似然估计对该下界求最大化 也就是固定QQQ函数，迭代求解参数的估计值 如果目标函数不是凸函数，EM算法不能保证找到全局最优解。 神经网络 感知机 若两类模式线性可分，则感知机的学习过程一定会收敛，否则学习过程会发生振荡。 多层感知机 解决异或问题 多层前馈网络 容易发生过拟合 解决方法： 早停：若训练误差降低，验证误差升高，则停止训练 正则化：在误差目标函数中增加一项描述网络复杂度的部分 跳出局部最小的策略 多组不同的初始参数优化神经网络，选择误差最小的解作为最终参数 模拟退火技术：每一步都以一定的概率接受比当前解更差的结果 随机梯度下降：在计算梯度时加入了随机因素 遗传算法：更好的逼近全局极小值 RBF网络 单隐层前馈神经网络，使用径向基函数作为隐层神经元激活函数 ART网络 用于无监督学习，网络的输出神经元相互竞争，每一时刻仅有一个神经元被激活，其他神经元的状态被抑制 ART网络可以增量学习或在线学习 级联相关网络 级联：建立层次连接的层级结构 相关：最大化神经元的输出和网络误差时间的相关性来训练参数 Elman网络 递归神经网络：使得神经元的输出反馈回来作为输入信号 深度学习 模型复杂度 模型宽度/模型深度 增加隐层的数量比增加隐层神经元的数目更有效 难点 难以训练 预训练+微调 每次训练一层隐层节点，训练时将上一层隐层节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入 在预训练完成后，再对整个网络进行微调 降维与度量学习 多维缩放 要求原始空间样本之间的距离在低维空间中得以保持。 对原始高维空间进行线性变换，实现降维 主成分分析 对正交属性空间中的样本点，如何用一个超平面对所有的样本进行表达？ 最近重构性：样本点到这个超平面的距离都足够近 最大可分性：样本点在这个超平面上的投影都能尽可能分开（方差尽可能大） 核化主成分分析 基于核技巧对线性降维方法进行核化 特征选择 特征的分类 相关特征：对当前学习任务有用的属性 无关特征：与当前学习任务无关的属性 冗余特征：其所包含信息能由其他特征推演出来（有用的冗余特征不需要去除） 特征选择 从给定的特征集合中选择出任务相关特征子集 减轻维度灾难/降低学习难度 子集搜索 前向搜索：逐渐增加相关特征 后向搜索：从完整的特征集合开始，逐渐减少特征 双向搜索：每一轮逐渐增加相关特征，同时减少无关特征 常用特征选择方法 过滤式 先用特征选择过程过滤原始数据，再用过滤后的特征来训练模型 特征选择过程与后续学习器无关 包裹式 直接把最终将要使用的学习器的性能作 为特征子集的评价准则 多次训练学习器，计算开销大，性能更好 嵌入式 嵌入式特征选择是将特征选择过程与学习器训练 程融为一体，两者在同一个优化过程中完成，在学习器训练过程中自动地进行特征选择 例如使用L1范数，更容易获得稀疏解 计算学习理论 概念 从样本空间到标记空间到映射，决定x的真实标记y 目标概念：如果对任何样例都有c(x)=yc(x) = yc(x)=y成立，则称ccc为目标概念 概念类：希望我们学得对目标概念所构成的集合 假设空间 所有可能概念的集合 学习过程时在假设空间中的搜索过程 可分和不可分 若目标概念中存在假设能将所有的示例完全正确分开，则是可分的 若目标概念不在嫁谁空间中，则是不可分的（例如线性空间不可分非线性空间） 概率近似正确（PAC） 希望以较大的把握学得比较好的模型，即以较大概率学得误 满足预设上限的模型 PAC可学习的 能存在一个学习算法，在有限的样本下PAC概念类 把对复杂算法的时间复杂度的分析转为对样本复杂度的分析 有限假设空间 可分情况 直接枚举 不可分情况 使用经验风险最小化原则 无限假设空间 实数域中的所有区间 / RRR空间中的所有线性超平面 VC维 增长函数：表示假设空间对m个示例所能赋予标记的最大kennel结果数 表述了假设空间的表示能力，由此反映出假设空间的复杂度 VC维是能被假设空间打散的最大示例集的大小d，超过d的示例集至少有一个不能被打散 二维平面的线性划分的假设空间的VC维是3 结论 基于VC维的泛化误差界：与数据分布无关 & 只与样例数目有关 稳定性 考察算法在输入训练集(发生)变化时，输出是否发生较大的变化 若学习算法是ERM（经验风险最小化）且稳定的，则假设空间是可学习的。 "},"cs229 keynotes.html":{"url":"cs229 keynotes.html","title":"Keynotes For CS 229","keywords":"","body":"Logistic & GDA 区别 Logistic是判别模型（对条件概率进行建模P(y∣x)P(y|x)P(y∣x)），GDA是生成模型（对联合概率进行建模P(x∣y)P(x|y)P(x∣y)） GDA实际上有着更强的假设，同时，其后验概率分布就是sigmoid函数 在模型符合该假设的前提下，效果比logistic更好 logistic模型的robust更好 可以发现，如果p(x∣y)p(x|y)p(x∣y)的条件概率不满足正态分布，而是posisson分布，也能得到sigmoid函数（用Logistic效果也不错） 生成模型的好处就是：需要的数据更少（因为对数据的假设更强） SVM SVM的主要思想是，对一个超平面而言，希望对分类边界的geometry margin最大。写出函数的约束和求解目标，通过Lagrange函数转为对偶问题，同时用KKT条件使得对偶问题的最优解和原始问题的最优解相同。同时，为了处理非线性的情况，引入了kenrel的概念，通过coordinate ascent（SMO）算法求解对偶问题的最优解。 原始问题建模 min⁡12∣∣w∣∣2\\min \\frac{1}{2} ||w||^2min21​∣∣w∣∣2 s.t.yi(w⋅xi+b)−1≥0,i=1,2,...,Ns.t. \\quad y_i(w\\cdot x_i + b ) -1 \\ge 0, i = 1,2,...,Ns.t.yi​(w⋅xi​+b)−1≥0,i=1,2,...,N 当yi(w⋅xi+b)=1y_i(w\\cdot x_i + b) =1yi​(w⋅xi​+b)=1时是support vector 对偶问题 min⁡12∑i∑jαiαjyiyj(xi⋅xj)−∑iαi\\min \\frac{1}{2}\\sum_i\\sum_j \\alpha_i \\alpha_j y_i y_j(x_i\\cdot x_j) - \\sum_i \\alpha_imin21​∑i​∑j​αi​αj​yi​yj​(xi​⋅xj​)−∑i​αi​ s.t.∑αiyi=0αi≥0s.t. \\quad \\sum \\alpha_iy_i = 0\\quad \\alpha_i \\ge 0s.t.∑αi​yi​=0αi​≥0 由KKT条件，w=∑αiyixiw = \\sum \\alpha_iy_ix_iw=∑αi​yi​xi​，同时可求得对应的b 决策函数只依赖于输入样本的内积 求得了α\\alphaα，当αj>0\\alpha_j>0αj​>0时对应的jjj是support vector（真正对www有用的） Kernel 将attributes -> feature 的过程定义为feature mapping，例如 ϕ(x)=[xx2x3]\\phi(x) = \\begin{bmatrix}x\\\\x^2 \\\\ x^3\\end{bmatrix}ϕ(x)=⎣⎡​xx2x3​⎦⎤​ 因此，我们想从feature中进行学习，而不是原始的attributes。而注意到，我们对样本的预测只与内积有关，因此可以定义Kernel：K(x,z)=ϕ(x)Tϕ(z)K(x,z) = \\phi(x)^T\\phi(z)K(x,z)=ϕ(x)Tϕ(z) 这样，在原始算法中的所有内积都用Kernel代替，这样就实现了从feature中学习 这种kernel的思想并不仅仅适用于SVM，只要有内积的形式，都可以使用，可以大大减少feature空间的维度 Gaussian kernel：K(x,z)=exp⁡(−∣∣x−z∣∣2wσ2)K(x,z) = \\exp(-\\frac{||x-z||^2}{w\\sigma^2})K(x,z)=exp(−wσ2∣∣x−z∣∣2​) SMO 使用coordinate ascent进行优化，也就是每次对一个序列中的某些参数更新。由于对偶问题是二次型的形式，因此很容易进行求解。 需要注意的是，由于我们的每个α\\alphaα存在约束，因此不能每次只对一个参数进行优化（至少需要两个），约束条件可以从等式中得到。 EM algorithm EM是一种用于处理隐变量的迭代算法，它基于局部最优解进行求解，而不能保证全局最优。（可以认为是coordinate ascent方法） Kmeans kmeans是一种相当简单和直观的聚类算法，主要分类两步： 对于每个点，选择离他最近的聚类中心作为他的类别：c(i):=arg⁡min⁡j∣∣x(i)−μj∣∣2c^{(i)} :=\\arg \\min _{j}||x^{(i)}-\\mu_j||^2c(i):=argminj​∣∣x(i)−μj​∣∣2 对于每个类别，求解聚类这个类的聚类中心：μj:=∑i=1m1{c(i)=j}x(i)∑i=1m1{c(i)=j}\\mu_{j} :=\\frac{\\sum_{i=1}^{m} 1\\{c^{(i)}=j\\} x^{(i)}}{\\sum_{i=1}^{m} 1\\{c^{(i)}=j\\}}μj​:=∑i=1m​1{c(i)=j}∑i=1m​1{c(i)=j}x(i)​ Kmeans算法的两步可以与EM对应起来，唯一的不同是，在E步时，Kmeans用了硬分类，而不是得到属于某个类的概率。 GMM 一种密度估计的算法，我们使用EM思想来处理，主要有两个步骤： E步：通过期望去guss zzz的最可能的值 wj(i):=p(z(i)=j∣x(i);ϕ,μ,Σ)w_{j}^{(i)} :=p\\left(z^{(i)}=j | x^{(i)} ; \\phi, \\mu, \\Sigma\\right)wj(i)​:=p(z(i)=j∣x(i);ϕ,μ,Σ) 实际上我们是通过后验概率来进行估计 p(z(i)=j∣x(i);ϕ,μ,Σ)=p(x(i)∣z(i)=j;μ,Σ)p(z(i)=j;ϕ)∑l=1kp(x(i)∣z(i)=l;μ,Σ)p(z(i)=l;ϕ)p\\left(z^{(i)}=j | x^{(i)} ; \\phi, \\mu, \\Sigma\\right)=\\frac{p\\left(x^{(i)} | z^{(i)}=j ; \\mu, \\Sigma\\right) p\\left(z^{(i)}=j ; \\phi\\right)}{\\sum_{l=1}^{k} p\\left(x^{(i)} | z^{(i)}=l ; \\mu, \\Sigma\\right) p\\left(z^{(i)}=l ; \\phi\\right)}p(z(i)=j∣x(i);ϕ,μ,Σ)=∑l=1k​p(x(i)∣z(i)=l;μ,Σ)p(z(i)=l;ϕ)p(x(i)∣z(i)=j;μ,Σ)p(z(i)=j;ϕ)​ 在这里，我们分子上的概率都可以直接得到，因此可以得到x(i)=jx^{(i)} = jx(i)=j的概率，也就是soft assignments wj(i)w^{(i)}_jwj(i)​ M步：通过已知的zzz来对模型参数进行估计（与MLE相同） ϕj:=1m∑i=1mwj(i)\\phi_{j} :=\\frac{1}{m} \\sum_{i=1}^{m} w_{j}^{(i)}ϕj​:=m1​∑i=1m​wj(i)​ μj:=∑i=1mwj(i)x(i)∑i=1mwj(i)\\mu_{j} :=\\frac{\\sum_{i=1}^{m} w_{j}^{(i)} x^{(i)}}{\\sum_{i=1}^{m} w_{j}^{(i)}}μj​:=∑i=1m​wj(i)​∑i=1m​wj(i)​x(i)​ Σj:=∑i=1mwj(i)(x(i)−μj)(x(i)−μj)T∑i=1mwj(i)\\Sigma_{j} :=\\frac{\\sum_{i=1}^{m} w_{j}^{(i)}\\left(x^{(i)}-\\mu_{j}\\right)\\left(x^{(i)}-\\mu_{j}\\right)^{T}}{\\sum_{i=1}^{m} w_{j}^{(i)}}Σj​:=∑i=1m​wj(i)​∑i=1m​wj(i)​(x(i)−μj​)(x(i)−μj​)T​ 注意与GDA的区别，在GDA中，是已知隐变量的（也就是监督学习），但它们在都是通过贝叶斯公式MLE进行参数估计，但实际上GDA与logistic更像。 EM 我们可以通过Jensen’s inequality来证明EM算法总是能使得似然函数单调非减，同时说明了我们为什么在E步时需要求后验概率（QQQ函数，一般来说对于M步是带入它的期望，所以称为E步）：这是因为该zzz的分布函数能够使得不等式的等式成立，从而得到一个tight bound。 ∑ilog⁡p(x(i);θ)=∑ilog⁡∑z(i)p(x(i),z(i);θ) =∑ilog⁡∑z(i)Qi(z(i))p(x(i),z(i);θ)Qi(z(i)) ≥∑i∑z(i)Qi(z(i))log⁡p(x(i),z(i);θ)Qi(z(i)) \\begin{aligned} \\sum_{i} \\log p\\left(x^{(i)} ; \\theta\\right) &=\\sum_{i} \\log \\sum_{z^{(i)}} p\\left(x^{(i)}, z^{(i)} ; \\theta\\right) \\ &=\\sum_{i} \\log \\sum_{z^{(i)}} Q_{i}\\left(z^{(i)}\\right) \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{Q_{i}\\left(z^{(i)}\\right)} \\ & \\geq \\sum_{i} \\sum_{z^{(i)}} Q_{i}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta\\right)}{Q_{i}\\left(z^{(i)}\\right)} \\end{aligned} i∑​logp(x(i);θ)​=i∑​logz(i)∑​p(x(i),z(i);θ) ​=i∑​logz(i)∑​Qi​(z(i))Qi​(z(i))p(x(i),z(i);θ)​ ​≥i∑​z(i)∑​Qi​(z(i))logQi​(z(i))p(x(i),z(i);θ)​​ 在M 步时，我们实际上是对最后一个式子求MLE估计，这样我们可以得到GMM的地推公式。 Factor analysis 当数据的维度远远大于数据的数量时，这时候如果我们还是用多维高斯来拟合，会发现其协方差矩阵是奇异矩阵，因此无法计算矩阵的逆。一种方法是，从低维空间中生成一系列的随机变量zzz，然后通过一个仿射变换到高维空间，同时加上误差项，就可以得到高维空间中的样本，例如： z∼N(0,I) ϵ∼N(0,Ψ) x=μ+Λz+ϵ \\begin{aligned} z & \\sim \\mathcal{N}(0, I) \\ \\epsilon & \\sim \\mathcal{N}(0, \\Psi) \\ x &=\\mu+\\Lambda z+\\epsilon \\end{aligned} z​∼N(0,I) ϵ​∼N(0,Ψ) x​=μ+Λz+ϵ​ 这种方法被称为Factor analysis，其中zzz为factor。 可以认为zzz是隐变量，因此使用EM算法进行迭代求解（求解相对比较复杂，见CS229-note9）。需要注意的是，这里的E步并不仅仅是求期望，还需要协方差） "}}