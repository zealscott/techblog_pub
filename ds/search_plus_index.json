{"./":{"url":"./","title":"Introduction","keywords":"","body":"Distribution System Syllabus Lecture Reading Material/hints Notes 1. Prepare knowledge and coding in Java - reflect in Java- anonymous inner class- Proxy in Java- abstract and interface- thread start() and runnable RPC and serialization 2. Distribution File System（DFS） Distribution File System (DFS) 3. MapReduce model MapReduce处理系统 4. MapReduce Coding - 组合式MapReduce- 链式MapReduce- 迭代式MapReduce- C/C++ 与 python通信 MapReduce编程 5. Spark model Spark 处理框架 6. Spark Coding - Spark Shell简单使用- RDD操作- Spark Submit Spark 编程 7. YARN and ZooKeeper - Yarn之日志分析 - Yarn 资源管理框架- ZooKeeper 元数据管理系统 8. DataFlow Framework - Async-loop-died- Storm处理Stream Join的简单实例 流计算系统概述 9. Flink - Flink 原理与实现：架构和拓扑概览 批流融合系统--Flink 10. Spark v2 & Beam 批流融合系统--展望 11. 分布式图处理系统 分布式图处理系统--Pregel 12. 机器学习系统--mahout 机器学习系统--mahout 13. 机器学习系统--GraphLab 机器学习系统--GraphLab 14. 机器学习系统-- Parameter Server 机器学习系统--Parameter Server Lab RPC model in Java hadoop安装与配置 hadoop编程实践（一） hadoop编程实践（二） hadoop编程练习 Hadoop 编程总结 spark安装与配置 spark编程实践 spark编程练习 Spark 编程总结 使用 Docker 配置 hadoop/spark 使用 docker 搭建 spark(2.3.1) 集群 ZooKeeper配置及简单使用 Yarn框架下的系统部署 Storm部署与运行 Storm编程练习 Storm 编程总结 SparkSteaming使用 Flink安装及使用 Flink编程练习（一） Flink编程练习（二） 常用图算法实现--Hadoop 常用图算法实现--Spark 常用图算法实现--Flink Giraph配置及使用 Flink迭代小记 Recap 分布式系统基础 批处理系统 支持数据管理的底层系统 流处理系统 批流融合系统 图处理系统 机器学习系统 总结与对比 This GitBook notes are maintained by zealscott. @Last updated at 1/27/2021 "},"notes/Distribution-File-System-DFS.html":{"url":"notes/Distribution-File-System-DFS.html","title":"Distribution File System DFS","keywords":"","body":" 以HDFS为例，介绍分布式文件系统 文件系统 文件系统的功能 文件的按名存取（基本功能） 文件目录的建立和维护（用于实现上述基本功能） 实现逻辑文件到物理文件的转换（核心内容） 文件存储空间的分配和管理 数据保密、保护和共享 提供一组用户使用的操作 文件 文件是由文件名字标识的一组信息的集合 各操作系统的文件命名规则略有不同 实现按名存取的文件系统的优点 将用户从复杂的物理存储地址管理中解放出来 可方便地对文件提供各种安全、保密和保护措施 实现文件的共享（同名共享、异名共享） 如何实现“按名存取”？ 当用户要求存取某个文件时，系统查找目录文件，获得对应的文件目录 在文件目录中，根据用户给定的文件名寻找到对应该文件的文件控制块（文件目录项） 通过文件控制块所记录的该文件的相关信息（如文件信息存放的相对位置或文件信息首块的物理位置）依次存取该文件的内容。 文件目录 概念 文件目录：建立和维护的关于系统的所有文件的清单 文件控制块：每个目录项对应一个文件的信息描述 目录文件：目录信息也以文件的形式存放 文件控制块的基本内容 文件存取控制信息：如文件名、用户名、文件主存取权限等 文件结构信息：文件逻辑结构、文件的物理结构等 文件使用信息：已打开该文件的进程数、文件的修改 情况等 文件管理信息：文件建立日期、文件访问日期等 文件的物理结构 顺序文件，连续存储 链接文件 索引文件 分布式文件系统 体系架构 主节点进行管理，从节点存储数据 文件切分成块，分散存储在从节点上 文件访问 单机多进程访问同一文件 读写锁 不同机器上进程访问同一文件 加锁效率太低，Hadoop使用一种immutable file 备份与一致性 客服端备份 Client -server DFS 在客户端进行备份，将更改过的文件传到server，改变时client和server不一致 服务器端备份 Cluster-based DFS HDFS 设计目标 假设硬件的异常比软件的异常更加常见 应用程序关注的是吞吐量，而不是响应时间。 文件仅支持追加，而不允许修改。 计算和存储采用就近原则，把代码放在远程计算，不搬运数据 数据块 对于Hadoop来说，都是处理的大文件 文件由数据块集合组成 通常每块的大小为64MB 每个数据块在本地文件系统已单独的文件进行存储（Linux文件系统） 与操作系统中文件block的区别 操作系统的block是读取的物理单元 目的是节省I/O block总是一样大 而在HDFS中是为了切大文件 若最后一块小于64M，则保持原有大小 体系结构 NameNode 每个集群只有一个 负责文件系统元数据操作、数据块的复制和定位 SecondaryNameNode 用于NameNode的备份节点 DataNode 集群中每个节点一个数据节点 负责数据块的存储 为客户端提供实际文件数据 NameNode 作用 管理节点、接收用户的操作请求 核心数据文件包括 元数据 保存在内存中 镜像文件fsimage: 维护文件系统树以及文件树中所有的文件/目录的元数据（包括文件中块所在的数据节点的位置信息） 操作日志文件EditLog：记录所有针对文件的创建、删除、重命名等操作 SecondaryNameNode 执行过程： 定期从NameNode上下载fsimage,edits。二者合并，生成新的fsimage在本地保存，并写回NameNode 是“检查点”，不是“热备份” 并不能保证实时都一样 若直接操作image，代价很高，总是先更改日志（记录操作） DataNode 作用 提供文件的存储 文件块（block）：最基本的存储单位，一个Linux文件 HDFS默认Block大小是64MB， 以一个256MB文件，共有256/64=4个Block 不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间 数据备份：默认是三个 因此，NamNode处理控制流，告诉客户存在哪里；而DataNode处理数据流，客户端直接与其进行传输。 文件访问 文件写入HDFS 可以并行写入不同节点 数据存放策略 目标：负载均衡，快速访问，支持容错 第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点(快速写入) 第二个副本：放置在与第一个副本不同的机架rack的节点上(减少跨rack的网络流量) 第三个副本：与第一个副本相同机架的其他节点上(应对交换机故障) 更多副本：随机节点 从HDFS读取文件 从各个数据节点上传数据块，可以并行 数据读取策略 当客户端读取数据时，从NameNode获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点 可以调用API来确定客户端和这些数据节点所属的机架ID 最近者优先原则：当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据 文件访问模型 一次写入多次读取 一个文件经过创建、写入和关闭后就不得改变文件中的内容 仅容许追加append（） 直接对文件增加一个block 对于单文件，不支持并发写，只支持并发读 append同一个文件只允许同时执行一个操作 修改内容需删除，重新写入 好处：避免读写冲突、无需文件锁 备份与一致性 一个文件有若干备份 备份之间是否可能存在不一致？ 写入成功的备份之间是强一致的 一次写入多次读取 容错机制 DataNode故障 “宕机”，节点上面的所有数据都会被标记为“不可读” 数据块自动复制到剩余的节点上以保证满足备份因子 由NameNode监控 定期检查备份因子 NameNode故障 根据SecondaryNameNode中的FsImage和Editlog数据进行恢复 HDFS功能 HDFS适合做什么 大文件存储 流式数据访问 HDFS不适合做什么 大量小文件 随机读取 延迟读取 "},"notes/mapreduce.html":{"url":"notes/mapreduce.html","title":"MapReduce处理系统","keywords":"","body":"MPI MPI是一个信息传递应用程序接口，包括协议和和语义说明。 是为了处理进程间通讯的协议。 问题： 在MapReduce出现之前，已经有像MPI这样非常成熟的并行计算框架了，那么为什么Google还需要MapReduce？ 这是因为，传统并行计算框架基本上都还是在单机上，为了保证不易宕机，一般都选择昂贵的刀片服务器，而MapReduce只需要普通的PC机，容错性好。 Hadoop架构 生态圈 MapReduce在Hadoop中位置 MapReduce 体系架构 Client 提交作业：用户编写的MapReduce程序通过Client提交到JobTracker端 作业监控：用户可通过Client提供的一些接口查看作业运行状态 JobTracker 资源管理：监控TaskTracker与Job的状况。一旦发现失败，就将Task转移到其它节点 作业调度：将Job拆分成Task，跟踪Task的执行进度、资源使用量等信息，由TaskScheduler调度（不是一个单独的进程，是一个模块） TaskTracker 执行操作：接收JobTracker发送过来的命令并执行（如启动新Task、杀死Task等） 划分资源：使用“slot”等量划分本节点上的资源量（CPU、内存等），一个Task 获取到一个slot 后才有机会运行 汇报信息：通过“心跳”将本节点上资源使用情况和任务运行进度汇报给JobTracker Map slot-> Map Task Reduce slot -> Reduce Task Task 任务执行 Map task Reduce task 在Hadoop上，每个task为一个进程；而在spark上，为线程。 如何执行任务？ 使用Java的反射和代理机制动态加载代码 工作流程 不同的Map任务之间不会进行通信 不同的Reduce任务之间也不会发生任何信息交换 所有的数据交换都是通过MapReduce框架自身去实现的(Shuffle) 有多少个Reduce，就有多少个输出 物理流程 InputFormat Split: 逻辑概念，包含一些元数据信息，如数据起始位置、数据长度、数据所在节点等。 划分方法由用户决定。 InputFormat： 定义了怎样与物理存储（如，HDFS block）之间的映射，理想的分片大小是一个HDFS块 如何划分？ 如果map task的物理机上有需要的split，则采取就近原则 因为HDFS为强一致性，每个备份都完全相同 Map Map任务的数量 Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。 虽然用户在配置中可以自定义Map的数量，但并不起作用 Mapper必须完成后才能被Reduce利用 指一个节点完成后，数据才可以被Reduce读取 但并不是所有的Map都要完成才能开始Reduce Shuffle Map端 分区 一个数据块分成几块部分 按照key分区，有几个reduce就分成几份 为shuffle做准备 排序 使得每个分区局部有序 让reduce可以直接归并（按照key），提高效率 合并 Combine函数，由用户定义，不一定需要 可以对数据进行压缩，提高效率 Reduce端 领取数据 Map完成后，jobtracker通知reduce领取数据 在hadoop中，是reduce主动去拉去数据 归并 当来自不同map处理后的数据放到reduce节点后，对其value进行归并，生成待处理的list。 如何确定shuffle到哪个Reduce？ 通过哈希partition（对key哈希），选择对应的reduce物理机。 只要shuffle给Reduce数据，reduce就可以开始 但一定在shuffle之后结束 Reduce Reduce任务的数量 程序指定 最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误） 序列化 序列化（Serialization）是指把结构化对象转化为字节流以便在网络上传输或在磁盘上永久存储的过程 反序列化（Deserialization）是指将将字节流转化为结构化对象的逆过程 Java实现了序列化，但占用空间很大，因此，hadoop实现了自己的序列化方式:Writable 与HDFS的关系 容错机制 MapReduce容错和HDFS容错是两回事 由于Map的中间数据必须写入磁盘（为了容错），导致hadoop不能实时流计算。 Map Task失败 重新执行Map任务 去HDFS重新读入数据 Reduce Task失败 重新执行Reduce任务 去map重新读入数据 TaskTracker失败 JobTracker不会接收到“心跳” JobTracker会安排其他TaskTracker重新运行失败TaskTracker的任务 JobTracker失败 最严重的失败，Hadoop没有处理JobTracker失败的机制，是个单点故障 所有任务需要重新运行 "},"notes/mrcoding.html":{"url":"notes/mrcoding.html","title":"MapReduce编程","keywords":"","body":"单个MapReduce 单元运算 以WordCount为例 分别编写Map和Reduce函数 编写main方法，设置环境变量，进行注册： 二元编程 Join 对于 input，来自不同的关系表，对于MapReduce而言都是文件 在Map过程中，需要标记来自哪个关系表 把来自 R的每个元组 转换成一个键值对 > ，其中的键就是属性 B的值 把来自 S的每个元组 ，转换成一个键值对> Reduce 过程 具有相同 B值的元组被发送到同一个 Reduce中 来自 关系 R和S的、具有相同属性 B值的元组进行合并 输出则是连接后的元组 ，通常写到一个单独的输出文件中 对于二元运算，例如Join、交集、并集都差不多，首先需要标记来自哪个关系表，然后再处理。 组合式MapReduce 将任务划分为若干子任务，各任务之间存在依赖关系 多次Join也可以认为是组合式的任务 程序实现 隐式依赖描述 如何表示Job之间有依赖关系 自己编程实现： 显式依赖描述 好处： 系统能拿到调度信息，避免上个程序运行失败导致后面出错 如果自己编程，例如J4/J5都依赖于J3，其中J4/J5一定会有一个顺序，而如果让系统调度，可以利用调度策略效率最大化（通常短作业优先） 在config中实现： 链式MapReduce 例子：词频统计后，过滤掉词频高于10的 WordCount程序已经写好，不能修改 Map可以串很多ChainMapper，Reducer也可以串很多ChainReducer 注意，这里的ChainReducer为Mapper 规则 整个Job只有一个Reduce 整个框架只允许一次Shuffle 进行Map不会造成数据重新排列，不会改变MapReduce整体框架 编程实现 迭代MapReduce 许多机器学习算法都需要进行迭代（牛顿迭代、EM算法） 迭代式任务的特征： 整个任务一系列子的循环构成 子任务的执行操作是完全相同的 一个子任务的输出是下一个子任务的输入 一个子任务是一个MapReduce Job 迭代多少次，就相当于运行多少次MapReduce 迭代MapReduce示意 每一迭代结束时才将结果写入HDFS，下一步将结果读出 非常浪费资源和IO 编程 runlteration()实现一个MapReduce Job 判断条件为满足阈值或者迭代次数 有时候并不关心具体的精确数值，只关心偏序关系（PageRank） Distribute Cache 当表的大小差异很大时，使用Join会导致大量的数据移动： 编程时将小表广播出去（每个节点上发一份，移动计算） 例如，在Kmeans中，可以将中心点广播出去 编程实现 声明 Job job= new Job(); job.addCacheFile (new Path(filename).toUri ()); 使用 Path[] localPaths = context.getLocalCacheFiles(); Hadoop Streaming Hadoop基于Java开发，但MapReduce编程不仅限于Java语言 提供一个编程工具，可以允许用户使用任何可执行文件 但可能会有bug 多种语言混合编程 C/C++ 与 python通信 原理 "},"notes/RPC-and-serialization.html":{"url":"notes/RPC-and-serialization.html","title":"RPC And Serialization","keywords":"","body":"Socket using TCP TCP (Transmission Control Protocol) is a standard that defines how to establish and maintain a network conversation via which application programs can exchange data. In Java need java.net.Socket and java.net.ServerSocket create Server and Client Models Using UDP UDP (User Datagram Protocol) is an alternative communications protocol to Transmission Control Protocol (TCP) used primarily for establishing low-latency and loss-tolerating connections between applications on the internet. In Java need java.net.* using DatagramPacket and DatagramSocket RPC Remote Procedure Call，a pro computer program causes a procedure (subroutine) to execute in a different address space (commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction. Procedure：Client Callee procedure：Server Function or Method：Service call remote function (Method) Behavior like local call How to call function using function name? reflect Proxy Server need a thread of this method to wait for response from client(service) need the class implement need a interface to register this service Client need the interface of class needed need a new Proxy Instance to proxy this method for client Serialization Serialization is the process of converting an object into a stream of bytes to store the object or transmit it to memory, a database, or a file. module IDL（Interface description language）文件 参与通讯的各方需要对通讯的内容需要做相关的约定 IDL Compiler 将IDL文件转换成各语言对应的动态库 Stub/Skeleton Lib 负责序列化和反序列化的工作代码 Client/Server 指的是应用层程序代码 底层协议栈和互联网 Protocol XML JSON Protobuf Thrift Avro Reference reflect in Java anonymous inner class Proxy in Java abstract and interface thread start() and runnable "},"notes/spark.html":{"url":"notes/spark.html","title":"Spark 处理框架","keywords":"","body":" Spark: an unified analytics engine for large-scale data processing Spark VS MapReduce MapReduce implementation principles 对于一个MapReduce作业来说，大致操作为： 可以发现，其主要的改变就是文件内容的变化 中间结果需要写磁盘，开销非常大 Hadoop MapReduce局限性 表达能力有限 仅map和reduce函数，无法直接用join等操作 磁盘IO开销大（单个job） 输入、输出及shuffle中间结果（可以优化）都需要读写磁盘 延迟高（多个job） 有依赖关系：job之间的衔接涉及IO开销，每次结果需要写入HDFS，在将其从HDFS中读出 无依赖关系：在前一个job执行完成之前，其他job依然无法开始（不利于并行） Spark改进 表达能力有限 增加join等更多复杂的函数，可以串联为DAG（有向无环图） 磁盘IO开销大（单个job） 非shuffle阶段避免中间结果写磁盘 中间结果写磁盘 MapReduce将Map的写入磁盘 Spark优先使用内存（但也不能保证不用磁盘） 延迟高（多个job作为一整个job） 将原来的多个job作为一个job的多个阶段 有依赖关系：各个阶段之间的衔接尽量写内存 无依赖关系：多个阶段可以同时执行 特点 运行速度快 使用DAG执行引擎以支持循环数据流与内存计算 容易使用 支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程 Spark Shell可以直接命令行就可以运行 通用性 Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件 运行模式多样 可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源 如何做到查询之前将数据拿到内存： 对于MapReduce，输入和输出都是文件 对于Spark，需要对文件再包装 告诉系统文件已经在内存中 cache了多少 RDD抽象 概念 RDD Resilient Distributed Dataset（弹性分布式数据集） 这里的弹性主要是指容错 Resilient：具有可恢复的容错特性 Distributed：每个RDD可分成多个分区，一个RDD的不同分区可以存到集群中不同的节点上 Dataset：每个分区就是一个数据集片段 分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 区别于hadoop核心的东西 作为分布式系统，RDD必须要分布 创建RDD的过程就是将普通文件增加元信息 RDD特性 RDD只读(Immutable)（为什么不是Readonly?） 本质上一个只读的对象集合 很重要，不能修改 文件R1->R2，是新建了一个R2 只能基于稳定的物理存储中的数据集创建RDD 通过在其他RDD上执行确定的转换操作（如map、join group by）而得到新的RDD RDD支持运算操作 RDD运算操作 转换Transformation：描述RDD的转换逻辑 group by 动作Action：标志转换结束，触发DAG生成 惰性求值：只有遇到action操作时，才会真正计算。 DAG DAG：是Directed Acyclic Graph（有向无环图）的简称 反映RDD之间的依赖关系 RDD Lineage RDD Lineage，即DAG拓扑结构 RDD读入外部数据源进行创建 RDD经过一系列的转换（Transformation）操作，每一次都会产生不同的RDD，供给下一个转换操作使用 最后一个RDD经过action操作进行转换，并输出到外部数据源 Spark系统保留RDD Lineage的信息 用于容错和恢复 RDD依赖关系 `RDD分区 一个RDD可以在不同节点 若已经对key值进行了hash partition join操作一一对应 窄依赖 表现为一个父RDD的分区对应于一个子RDD的分区 或多个父RDD的分区对应于一个子RDD的分区 宽依赖 表现为存在一个父RDD的一个分区对应一个子RDD的多个分区 区别 窄依赖只需要恢复某一个分区 宽依赖需要所有分区 为什么关系依赖关系 分析各个RDD的偏序关系生成DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage 具体划分方法 在DAG中进行反向解析，遇到宽依赖就断开 为什么反向解析 因为action只有一个，更加方便 遇到窄依赖就把当前的RDD加入到Stage中 将窄依赖尽量划分在同一个Stage中，可以实现流水线计算pipeline 窄依赖之间可以并行执行 窄依赖的不同分区可以用pipeline，不用等待，直接执行 例子 如图被分成三个Stage，在Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作 stage类型 ShuffleMapStage 输入/输出 输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出 以Shuffle为输出边界，作为另一个Stage开始 特点 不是最终的Stage，在它之后还有其他Stage 它的输出一定需要经过Shuffle过程，并作为后续Stage的输入 在一个Job里可能有该类型的Stage，也可能没有该类型Stage 如果没有，其实就没太大意义 ResultStage 输入/输出 其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出 输出直接产生结果或存储 特点 在一个Job里必定有该类型Stage 最终的Stage 一个DAG含有一个或多个Stage，其中至少含有一个ResultStage 体系结构 Spark架构设计 Master：管理整个系统 集群资源管理器（Cluster Manager） 资源管理器可以自带或Mesos或YARN（可以使用其他的资源管理器，与计算框架分离） Worker：运行作业的工作节点 负责任务执行的进程(Executor） 负责任务执行的线程(Task) 层次结构 Application 用户编写的Spark应用程序 Job 一个Job包含多个RDD及作用于相应RDD上的各种操作 Stage 一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet Job的基本调度单位 代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集 Task 运行在Executor上的工作单元 概念 逻辑执行角度 一个Application=一个或多个DAG 一个DAG=一个或多个Stage 一个Stage=若干窄依赖的RDD转换 物理执行角度 一个Application=一个或多个Job 一个Job=一个或多个TaskSet 一个TaskSet=多个没有Shuffle关系的Task 工作流程 执行Application的一般过程 Driver向集群管理器申请资源 集群管理器启动Executor Driver向Executor发送应用程序代码和文件 Executor上执行Task，运行结束后，执行结果会返回给Driver，或写到HDFS等 Spark运行流程特点 Application有专属的Executor进程，并且该进程在Application运行期间一直驻留 Executor进程以多线程的方式运行Task Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可 Task采用了数据本地性和推测执行等优化机制推测执行将较慢的任务再次在其它节点启动 与Hadoop一样，计算向数据靠拢 认为分配的Task是均匀的，若出现一个节点特别慢，则在另外的节点上重新计算 DAG运行过程 RDD在Spark架构中的运行过程： 创建RDD对象； SparkContext负责计算RDD之间的依赖关系，构建DAG； DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行 Spark Executor 与MapReduce相比，Spark所采用的Executor有两个优点： 利用多线程来执行具体的任务，减少任务的启动开销 Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销 容错机制 Master故障 几乎没有办法 Worker故障 Lineage机制 RDD容错机制：血缘关系（Lineage）、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作 根据依赖关系重新计算上一分区： 窄依赖(narrow dependency) 执行某个partition时，检查父亲RDD对应的partition是否存在 存在，即可执行当前RDD对应的操作 不存在，则重构父亲RDD对应的partition 宽依赖(widedependency) 执行某个partition时，检查父亲RDD对应的partition是否存在 存在，即可执行当前RDD对应的操作 不存在，则重构整个父亲RDD 例子 HadoopRDD 分区：对应于HDFS输入文件的文件块 依赖：无依赖 函数：读取每个文件块 最佳位置：HDFS的文件块对应的位置 分区策略：无 FilteredRDD 分区：与父亲RDD一致 依赖：与父亲RDD存在窄依赖 函数：计算父亲的每个分区并执行用户提供的过滤函数 最佳位置：无，与父亲RDD一致 分区策略：无 JoinedRDD 分区：每个reduce任务（个数可指定）对应一个分区 依赖：常常与所有父亲RDD存在宽依赖 函数：计算shuffle数据，并且执行join操作 最佳位置：无 分区策略：HashPartitioner RDD存储机制 RDD提供的持久化（缓存）接口 persist()：对一个RDD标记为持久化 接受StorageLevel类型参数，可配置各种级别 持久化后的RDD将会被保留在计算节点的中被后面的行动操作重复使用 cache() 相当于persist(MEMORY_ONLY) 可以使用unpersist()方法手动地把持久化的RDD从缓存中移除 没有标记持久化的RDD会被回收，节约内存 检查点机制 前述机制的不足之处 Lineage可能非常长 RDD存储机制主要面向本地磁盘的存储 检查点机制将RDD写入可靠的外部分布式文件系统，例如HDFS 在实现层面，写检查点的过程是一个独立job，作为后台作业运行 "},"notes/sparkcoding.html":{"url":"notes/sparkcoding.html","title":"Spark 编程","keywords":"","body":"Spark shell Spark Shell 提供了简单的方式来学习Spark API Spark Shell可以以实时、交互的方式来分析数据 Spark Shell支持Scala和Python 一个Driver就包括main方法和分布式数据集 Spark Shell本身就是一个Driver，里面已经包含了main方法 Spark RDD RDD操作 RDD创建 从文件系统中加载数据创建RDD，并指定分区的个数 本地文件系统 HDFS 其它 通过并行集合（数组）创建RDD 可以将本地的Java对象变为RDD 创建RDD时手动指定分区个数 在调用textFile()和parallelize()方法的时候手动指定分区个数即可，语法格式如下： sc.textFile(path, partitionNum) 其中，path参数用于指定要加载的文件的地址，partitionNum参数用于指定分区个数。 RDD Transformation RDD Repartition 通过转换操作得到新RDD 时，直接调用repartition 方法或自定义分区方法 什么使用用到该方法？ 当遇到某些操作，如join，则可将宽依赖Partition之后变为窄依赖，便于pipeline执行 RDD Action 惰性机制：整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到action操作时，才会发生真正的计算，开始从血缘关系源头开始，进行物理的转换操作操 RDD保存 RDD写入到本地文本文件 RDD中的数据保存到HDFS文件中 Spark SQL Spark SQL增加了DataFrame（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据 Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范 Schema RDD的局限性：RDD是分布式Java对象的集合，但是对象内部结构，即数据Schema不可知 DataFrame 无论读取什么数据，都写成DataSet DataSet 相比DataFrame，DataSet明确声明类型 在源码中，可以将DataFrame理解为DataSet的别名 若查询语句中有一列不存在，则可以在编译时检查出来 SQL Query sql(“”)括号中的SQL语句对于该函数来说仅仅是一条字符串 编译时不会进行任何语法检查 比较 "},"notes/yarn.html":{"url":"notes/yarn.html","title":"Yarn 资源管理框架","keywords":"","body":"背景介绍 MapReduce v1 JobTracker Manage Cluster Resources & Job Scheduling TaskTracker Per-node agent Manage Task 其中，JobTracker承担了太多的任务 作业管理 状态监控，信息汇总 任务管理 调度，监控 资源管理 MapReduce 1.0 缺陷 JobTracker存在单点故障风险 JobTracker“大包大揽”内存开销大 资源分配只考虑MapReduce任务数量，不考虑任务需要 的CPU、内存，TaskTracker所在节点容易内存溢出 资源划分不灵活(强制等量划分为slot ，包括Map slot和 Reduce slot) 资源管理不单单是MapReduce系统所需要的，而是通用的 YARN Yet Another Resource Negotiator 设计思路 资源管理与计算相分离 MapReduce 1.0既是计算框架，也是资源管理框架 Yarn是独立出来的资源管理框架 YARN体系结构 ResourceManager ResourceManager(RM)是一个全局的资源管理器，包括 资源调度器(Resource Scheduler) 资源分配 应用程序管理器(Application Manager) 所有应用程序的管理工作(轻量作业管理) 检查是否由足够资源部署application master 维护application id Web UI展示信息 NodeManage 监控所在节点每个Container(容器)的资源 (CPU、内存等)使用情况、健康状况 向ResourceManager汇报作业的资源使用 情况和每个容器的运行状态 NodeManager主要负责管理抽象的容器， 只处理与容器相关的事情，而不具体负责每个任务(Map任务或Reduce任务)自身状态的管理 ApplicationMaster 与ResourceManager(RM)的交互 与RM协商获取资源，把获得的资源进一步分配给内部的各个任务(Map或Reduce任务) 定时向RM报告资源使用情况和应用进度信息 作业完成时，向RM注销容器，执行周期完成 与NodeManager保持通信进行应用程序的 管理 监控任务的执行进度和状态，并在任务发生失败时重新申请资源重启任务 Container 容器(Container)作为动态资源分配单位 ，每个容器中都封装了一定数量的CPU、 内存、磁盘等资源，从而限定每个应用程 序可以使用的资源量 执行计算任务 无论是MapReduce还是Spark，都可以执行 YARN工作流程 YARN发展目标 一个框架多个系统 在一个集群上部署一个统一的资源调度管理框架YARN，在 YARN之上可以部署其他各种计算系统 由YARN为这些计算系统提供统一的资源调度管理框架，并且能够根据各种计算系 统的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩 可以实现一个集群上不同应用负载混搭，有效提高了集群的利用率 不同计算系统可以共享底层存储，避免了数据集跨集群移动 "},"notes/zookeeper.html":{"url":"notes/zookeeper.html","title":"ZooKeeper 元数据管理系统","keywords":"","body":"背景介绍 HDFS1.0 名称节点保存元数据: 在磁盘上 FsImage和EditLog 在内存中 映射信息，即文件包含哪些块，每个块存储在哪个数据节点 单点故障 SecondaryNameNode会定期和NameNode通信 从NameNode上获取到FsImage和EditLog文件，并下载到本地的 相应目录下 执行EditLog和FsImage文件合并 将新的FsImage文件发送到NameNode节点上 NameNode使用新的FsImage和EditLog(缩小了) 第二名称节点用途 不是热备份 主要是防止日志文件EditLog过大，导致名称节点失败恢复时消耗过多时间 附带起到冷备份功能 因此， NameNode保存元数据： 如何解决单点故障的问题？ 如何借助系统实现热备份？ HDFS HA（High Availability） NameNode之间的数据同步 借助共享存储系统或JNs(Journal Nodes)实现 NameNode之间的状态感知 一旦Active出现故障，立即切换Standby ZooKeeper简介 ZooKeeper 轻量级的分布式系统 作用 用于解决分布式应用中通用的协作问题 命名管理 Naming 基本功能 配置管理 Configuration Management 集群管理 Group Membership 监控集群状态 同步管理 Synchronization 系统架构 单机提供服务，多机组成集群来提供服务或一台机器多个ZooKeeper Server Zookeeper角色 Server：负责管理元数据 Leader 某一个server，保证分布式数据一致性的关键，何时需要选取leader? 服务器初始化启动 服务器运行期间无法和Leader保持连接 Client 需要进行协作的用户(系统) 提供元数据 获取元数据，并据此执行相应操作 ZooKeeper数据模型 维护类似文件系统的层次数据结构 Znode Znode doesn’t not design for data storage, instead it store meta-data or configuration Can store information like timestamp version Znode类型 Regular(常规) Ephemeral(临时) Znode标识 Sequential flag ZooKeeper Service Client Client可以在某个Znode上设置一个Watcher，来Watch该Znode上的变化 Server 一旦这个Znode中存储的数据的修改，子节点目录的变化等，可以通知设置监控的客户端 Session A connection to server from client is a session Timeout mechanism Znode 可根据属性分为四种类型 是否会自动删除 Regular Znode 用户需要显式的创建、删除 Ephemeral Znode 用户创建后，可以显式的删除，也可以在Session结束后，由ZooKeeper Server自动删除 是否带有顺序号 如果创建的时候指定Sequential，该Znode的名字后面会自动Append一个不断增加SequenceNo ZooKeeper典型应用 命名管理 统一命名服务 分布式应用通常需要一套命名规则，既能够产生唯一的名称又便于人识别和记住 树形的名称结构:通常情况下树形的名称结构 既对人友好，又不会重复 Name Service已经是Zookeeper内置的功能，只要调用Zookeeper的API就能实现 动态配置管理 场景 用户命令行修改了默认的参数  方法: 配置信息存在 ZooKeeper 某个目录节点，所有机器watch该目录节点 一旦发生变化，每台机器收到通知，然后从 ZooKeeper获取新的配置信息应用到系统中 集群管理 监控集群状态 新增或减少slave 对每个进程创建一个Znode在统一的一个文件目录下，并以EPHEMERAL mode 如何做到？ 当新建文件时，文件个数发生变化，可以感知 当机器宕机时，由于时临时的，文件个数减少，可以感知 选主（Leader Election） 对每一个进程创建一个Znode，并以EPHEMERAL_SEQUENTIAL mode 每次选择序号最小的那个为leader 同步管理 共享锁 Simple Lock Create a Znode l for locking If one gets to create l, he gets the lock Others who fail to create watch l Herd effect(羊群效应) 大量客户端监听同一个Znode，ZooKeeper需要发出大量的事件通知 解决方案:分散 因此，需要实现负载均衡，具体算法可以参考这里。 队列管理 两种类型的队列 FIFO队列 按照FIFO方式进行入队和出队操作， 例如实现生产者和消费者模型 同步队列 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达 也会出现羊群效应 双屏障 允许客户端在计算的开始和结束时同步。 当足够的进程加入到双屏障时，进程开始计算。当计算完成时，离开屏障。 实现方法 进入屏障:创建/ready和/process节点，每个 进程都先到ready中注册，注册数量达到要求时 ，通知所有进程启动开始执行 离开屏障 在/process下把注册ready的进程都 建立节点，每个进程执行结束后删掉/process 下对应节点。当/process为空时，任务全结束 "},"notes/pregel.html":{"url":"notes/pregel.html","title":"分布式图处理系统--Pregel","keywords":"","body":"图数据处理简介 图数据的应用 图数据 数据本身以图的形式呈现 社交网络 传染病传播途径 交通路网 某些非图结构的数据，也可以转换为图模型后进行处理 网页链接 机器学习训练数据 关联性分析 图数据结构表达了数据之间的关联性 通过获得数据的关联性，抽取有用的信息 购物通过为购物者之间的关系建模，就能很快找到口味相似的用户，并为之推荐商品 图在社交网络中，通过传播关系发现意见领袖 图数据处理解决方案 使用单机的图算法库 BGL、LEAD、NetworkX、JDSL、StandfordGraphBase和FGL等 如何运行大规模的图？ 单机算法实现相应的分布式 通用性不好，每个算法都需要重写 并行图计算系统 Parallel BGL和CGM Graph，实现了很多并行图算法 对大规模分布式系统容错等没有很好的支持 基于现有分布式数据处理系统进行图计算 使用MapReduce/Spark/FlinkAPI编写图算法 编写困难，系统未针对图计算进行优化 图处理系统 图数据库： 基于遍历算法的数据存储、用于实时图查询 Neo4j、OrientDB、DEX和Infinite Graph 图处理系统： 以图顶点为中心的计算、用于离线图分析 基于消息传递的并行引擎：如GoldenOrb、Pregel/Giraph 利用Dataflow系统构建的工具包：MapReduceHama、Spark GraphX、FlinkGelly Pregel/Giraph系统 计算模型 基于BSP模型实现的分布式图处理系统 一套可扩展的、有容错机制的平台 提供一套灵活的API，描述图计算，比如图遍历、最短路径、PageRank计算等 注意，Giraph是利用MapReduce开源框架实现，不是基于MapReduce API计算 图结构 顶点 顶点ID：唯一标识 自定义值：存储顶点的“状态值” 例如PageRank、最短路径中的属性值 边 和其源顶点关联，并记录了其目标顶点ID 边上有一个可修改的用户自定义值与之关联 图算法共性 共性：顶点给邻居传递消息，不断进行更新，此过程迭代，直到最终收敛 集中式算法：限制参与运算的顶点，例如Dijkstra总是挑最近的顶点加入source 分布式算法：所有顶点同时参与运算 BSP模型 一系列全局超步（superstep） 局部计算：每个参与的处理器独立计算 通讯：处理器群相互交换数据 栅栏同步(Barrier Synchronization)：等到其他所有处理器完成计算，再继续下一个超步 Vertex-centric计算模型 “边”并不是核心对象，在边上面不会运行计算，只有顶点才会执行用户自定义函数进行相应计算 顶点的状态 活跃active：该顶点参与当前的计算 非活跃inactive：该顶点不执行计算，除非被其他顶点发来的消息激活 当一个非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架根据条件判断是否将其显式唤醒进入活跃状态 什么时候结束 当所有顶点都是非活跃状态的时候 编程模型 Compute() 用户定义的计算函数 SendMessageTo() 消息传递给哪些顶点 通常给邻居节点发送 Combiner 将发往同一顶点的多个消息合并成一个消息，减少了传输和缓存的开销 与Hadoop中的Combiner作用相同 Aggregator 一种全局通信、监控和数据查看的机制 超步S中，每个顶点都可以向某个Aggregator提供数据，Pregel计算框架对这些值进行聚合操作产生一个值 超步S+1中，所有顶点都可以看见这个值 例如，可以用来求图中边数 体系结构 Master：协调各个Worker执行任务 Worker：维护图的状态信息，负责计算 BSP计算模型 计算：worker自身 通讯：worker之间 同步：master Worker Worker内部 维护图顶点的描述信息 一般保存在内存中 包括顶点的当前值 以该顶点为起点的初射列表，每条出射边包含了目标顶点ID和边的值 执行图顶点上的计算：Compute() 在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并计算 管理图顶点的控制信息 需要存两份（接收的队列，发送的队列）！ 输入消息队列：接收到、发送给顶点的消息， S已经接收到的消息（来自于S-1），S中需要处理 S中接收到来自其他Worker的消息，S+1处理 标志位：用来标记顶点是否活跃状态 S中标记顶点是否活跃 S中记录顶点在S+1是否活跃 Worker之间 消息传输：SendMessageTo() 发送消息前会首先判断目标顶点U是否位于本地（根据内部描述信息） 本地：直接把消息放入到与目标顶点U对应的输入消息队列中 远程：暂时缓存消息到本地输出消息队列，当缓存中的消息数目达到阈值时，传输到目标顶点所在的Worker上 若存在用户自定义的Combiner操作，则消息被加入到输出队列或者到达输入队列时，就可以对消息执行合并操作 Master 维护worker的状态 协调worker的计算 同步控制：Superstep 对外服务 Master维护的数据信息的大小，只与分区的数量有关，而与顶点和边的数量无关 Master协调计算 在每个SuperStep中需要进行两次同步（双屏障） 开始时同步发送相同的指令，等待Worker回应 结束后，进行路障Barrier同步，一旦成功Master就会进入下一个超步的执行 MapReduce与Giraph Giraph的主从结构在MapReduce中都是Task 利用zookeeper选主 只利用了MapReduce中的mapper节点，没有Reduce节点 只是利用了MapReduce框架Run函数将Giraph启动 工作流程 数据加载 基于顶点的图分区 哈希函数或者用户自定义函数 目标是使得跨界点的通信减少 读取数据 Master只需要知道图的位置 将输入的图划分为多个部分，如基于文件边界 每个部分都是一系列记录（顶点和边）的集合 Master会为每个Worker分配一部分图数据 Worker取真正读数据 将部分图数据加载到内存 SuperStep计算 Worker为管辖的每个分区分配一个线程 对于分区中的每个顶点，Worker根据来自上一个超步的、发给该顶点的消息并调用处于“活跃”状态的顶点上的Compute()函数 SuperStep结束 在执行计算过程中，顶点可以对外发送消息，但是必须在本超步结束之前完成 超步完成后（barrier），Worker把在下一个超步还处于“活跃”状态的顶点数量报告给Master 容错机制 不能使用MapReduce的容错机制 Master故障 意味协调节点丢失 Zookeeper选主 Worker故障 意味计算节点丢失 设置检查点 检查点机制 设置检查点：每隔一定的superstep 在设置检查点的超步开始时，Master通知所有的Worker把管辖的分区状态（顶点、边、接收到的消息等），写入到持久化存储 Worker发生故障 Pregel 重新启动一个Worker 局部恢复（confined recover） 将失效节点从检查点恢复到当前时刻 Giraph Master把失效Worker的分区分配到其他处于正常状态的Worker上 全局恢复，所有节点退回到检查点 乐观容错 不一定需要检查点 "},"notes/flink.html":{"url":"notes/flink.html","title":"批流融合系统--Flink","keywords":"","body":"工作原理 系统流程 当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Clinent Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。 任务 将用户程序翻译成逻辑执行计划(类似sparkDAG) 逻辑执行计划的优化 JobManager JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 任务 管理TaskManager 调度 协调 TaskManager TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。 任务 与JobManager、TaskManager保持通信 启动Task线程，实际执行任务 相同节点或不同节点上Task线程进行数据交换 Graph 批处理:Program → BatchGraph → Optimized BatchGraph → JobGraph 流计算:Program → StreamGraph → JobGraph StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。 JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。 ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。 涵义 流计算 批处理 部件 用户程序 Stream API程序 Batch API程序 Client 程序的拓扑结构 StreamGraph DAG/Optimizer Client 逻辑执行计划 JobGraph JobGraph Client 物理执行计划 ExecutionGraph ExecutionGraph JobManager 物理实现 分布式执行 分布式执行 TaskManager Flink API 与Spark对比 Process Function 理论上实现了以上三个函数，就可以定义任意算子 DatatStream/DataSet DataStream API DataStream Iteration 官方文档参考这里 在Flink中，不显式定义循环，而是通过iterationBody实现循环操作 这里的iterationBody接收X的输入，并根据最后的closeWith完成一次迭代，另外的数据操作Z退出迭代。 DataSet API 定义的算子与DataStream类似，主要还是考虑迭代过程。 DataSet Bulk Iteration 可以发现与DataStream类似，但必须要迭代结束才能有输出。 同时，除了设置最大迭代次数，在closeWith中还可以添加第二个DataSet，当其为空时，则退出循环。 与流计算的区别： Input会有源源不断的来，且迭代过程中会有数据进入 Output随时都可以输出 DataSet Delta Iteration 由于在图计算中有很多算法在每次迭代中，不是所有TrainData都参与运算，所以定义Delta算子。 workset是每次迭代都需要进行更新的，而solution set不一定更新。 可以通过iteration.getWorkset/getSolutionSet得到对应的DataSet，同时分别进行计算，当nextWorkset为空时，退出循环。 在迭代过程中，nextWorkSet是直接替代当前的workset，而deltaSet是merge到solutionSet。 key positions是什么？ 指定生成的值的键是哪一个属性。 Table&SQL Table API 与SQL基本相同，只是在SQL中，由于是字符串语句，在编译时无法查错。 批流等价转换 主要是想要让SQL在Steam上运行。 为了实现这种功能，Flink设计了Dynamic Table对Stream进行相互转换。 Stream → Dynamic Table Append mode 很简单，直接添加一项即可 Upsert mode 需要将相同key的值更新或者插入 同样支持SQL和Window操作 Dynamic Table → Stream 为了容错，使用Undo/Redo日志 容错机制 使用checkpoint的方式 非迭代过程的容错 对于批处理和流计算是一样的 Pipelined Snapshots Asynchronous approache Light checkpoint Distributed snapshots Exactly-once guarantee 迭代过程的容错 流式迭代 属于有环的Pipelined Snapshots 将迭代的数据看作虚拟的输入 批式迭代 特点：循环不结束，没有输出 两种CheckPointing方法： Tail checkpoint Head checkpoint Reference Flink 原理与实现：架构和拓扑概览 "},"notes/beam.html":{"url":"notes/beam.html","title":"批流融合系统--展望","keywords":"","body":"SparkV2 回顾 Feature 在第一代的Spark Streaming系统中，其主要特点为： 以批处理核心，使用micro-batch模型将流计算转换为批处理 流计算和批处理API可以互用 DStream（特殊的RDD) RDD Spark Streaming局限性 Spark streaming难以处理的需求 Event-time Late Data 流数据的三个特征 乱序 延迟 无界 Session windows 比较难处理，与batch框架相矛盾 Structured Streaming思路 类似Flink，流向表转换 流与表的操作统一到DataSet/DataFrameAPI 底层引擎依然是批处理，继续使用micro-batch的模型 Continuous query模型还在开发中 处理模型 Unbounded Table 借鉴了Spark中的Dynamic Table实现批流等价转换 Event time 将Event Time 作为表中的列参与到Window运算中 Late Data 引入流水线机制 Beam Beam系统需要注意什么？ 同一API 会不会造成严重的性能差异 同一编程 低层的两个系统如何实现统一 WWWH模型 只需要管需要进行说明操作，不关心谁去执行 What results are calculated? 计算什么结果? (read, map, reduce) 批处理系统可实现 Where in event time are results calculated? 在哪儿切分数据? (event time windowing) Windowed Batch When in processing time are results materialized? 什么时候计算数据? (triggers) Streaming How do refinements of results relate? 如何修正相关的数据?(Accumulation) Streaming + Accumulation BeamPipeline 数据处理流水线 表示抽象的流程 与“Flink流水线机制”不是一个概念 "},"notes/ps.html":{"url":"notes/ps.html","title":"机器学习系统--Parameter Server","keywords":"","body":"GraphLab简介 是一个以参数为中心的机器学习系统。 Observations 某些learning算法的模型复杂，参数很大 Complex Models with Billions and Trillions of Parameters e.g. LDA 某些Learning过程呈现线性，需要同步 Sequential ML jobs require barriers and hurt performance by blocking BSP model是我们想要的，但如何平衡性能？ 容错很重要，尤其是参数的容错 At scale, Fault Tolerance is required as these jobs run in a cloud where machines are unreliable and jobs can be preempted 大规模的机器学习算法参数很多，需要进行容错 设计思路 参数与训练数据分开存放 Server:负责参数 Worker:负责训练数据 提供同步计算与异步计算模式 灵活的consistency 用户选择 参数看作key-value pair进行备份 Consistent hashing 体系架构 主节点用来存放参数 取参数是按需pull，再将更新后的参数放回去 计算模式 Asynchronous tasks and Dependency Flexible Consistency Up to the algorithm designer to choose the flexible consistency model 其实让编程变得更复杂 Trade-off between Algorithm Efficiency and System Performance 计算考虑 异步可能是错的 性能方面 异步更好 容错机制 使用一致性哈希和备份的方式 为什么不能用zookeeper？ 数据量太大 前提：这个model是key-value pair，才能够被hash 讨论 GraphLab和PS中sequential一样吗? GraphLab强调数据点之间的顺序计算关系 PS不考察训练数据点之间的关系，强调多次迭代之间的顺序关系 GraphLab中的consistency和PS中的 consistency是一样的吗? GraphLab中的consistency解决可串行问题 PS中的consistency解决同步/异步计算问题 "},"notes/graphlab.html":{"url":"notes/graphlab.html","title":"机器学习系统--GraphLab","keywords":"","body":"GraphLab简介 是一个以数据为中心的机器学习系统。 实际中很多模型都可以转化为图的形式： 社交网络 推荐系统 文本分析 概率图模型 我们之前已经有了基于BSP Model的Pregel，其主要特点是需要进行同步（双屏障），而同步是由最慢的节点决定，造成： 资源的浪费（大部分节点会等待少部分节点收敛） 某些算法可能并不需要同步更新 某些机器学习计算的特点 异步迭代（Asynchronous Iterative） 参数不一定需要同步更新 某些梯度下降算法可能不需要同步 动态计算（Dynamic Computation） 某些参数可能快速收敛 大部分顶点在很少的次数就已经收敛 在Pregel中可以通过设置inactive实现 可串行化（Serializability） 计算过程存在一定的顺序依赖关系，或者 顺序计算的效果(收敛速度、精确度)更 在有些特征的图中（线性/链式），用BSP会造成消息传输冗余 并行会造成计算量更大，传输消息更多 但也不能直接单线程执行 考虑事务，并发的运行过程等价于某种串行化的结果 应用场景 求最小值，最大值 马尔可夫过程/随机场 BSP model中简化了这个问题，但可能影响迭代次数 因此，在GraphLab中，针对这三点分别进行了改进： 异步迭代 Pull model Update function 动态计算 Pull model 可串行化 Scheduler Consistency Model 计算模型 UpdateFunction 与Giraph中的compute相似，需要： 当前节点的信息 邻居节点信息 最重要的不同点： 在Giraph中是节点主动发送消息，获取消息是一个被动的过程 而在GraphLab中，是主动拉去消息 Pull vs. Push Model Push model handle messages by pushing them from source vertices at the sender side to destination vertices at the receiver side Pull model handle messages by pulling them from source vertices on demand when destination vertices are updating themselves 哪一个更好？ 在Pull model中，想要计算，直接拉去消息即可，可用来实现异步迭代 Pregel vs. GraphLab 异步迭代 Pregel:不支持，BSP模型是同步迭代 GraphLab:通过pull model可以支持异步 动态计算 Pregel:根据判定条件让某些顶点votetohalt，但是后续可能还会收到消息 GraphLab:根据判定条件停止计算 updatefunction，不再pull消息 The Scheduler 决定了迭代中顶点处理的顺序 Consistency Rules Full Consistency 最强的一致性，但并发度很低 Edge Consistency 只对边和中心点进行一致性保证 Vertex Consistency 只对顶点进行一致性保证 讨论 GraphLab vs. DB transactions Consistency类似事务可串行化中的什么概念? Pregel vs. GraphLab BSP模型是哪种consistency? Pregel的vertex-centric编程模型服从vertex consistency model吗? "},"notes/mahout.html":{"url":"notes/mahout.html","title":"机器学习系统--mahout","keywords":"","body":"机器学习系统 机器学习三要素 模型 用参数来构造描述客观世界的数学模型 线性回归模型 对于参数很多的模型，还需要考虑参数容错（LDA） CNN/Tensorflow -> 适用于从参数入手 策略 基于已有的观测数据进行经验风险最小化 选择什么参数在代价函数中是最好的？（例如，训练数据的误差平方和） 对于数据之间有关联的算法（PageRank） -> 适用于从数据入手 算法 优化问题的计算求解 适用于Model较为简单，数据没有关联性 -> 适用于从计算入手 梯度下降 要求必须每次迭代同时更新参数 与BSP模型很像 在某些算法中可以异步更新 机器学习系统的设计思路 机器学习vs机器学习系统 机器学习 强调机器学习方法的精确程度 评价标准:accuracy/precision/recall 机器学习系统 强调机器学习过程的运算速度 评价标准:performance 机器学习 机器学习系统 模型 参数 策略 数据 算法 计算 应该从哪个角度来设计机器学习系统？ 考虑模型的复杂度/数据关联性/参数个数 分别从参数/数据/计算入手 机器学习中的“图” 训练模型可以构成一张图 顶点:参数 边:参数 训练数据可以构成一张图 顶点:数据点 边:数据点之间的关联性 训练过程可以构成一张图 顶点:数据处理过程中的计算 边:计算结果 因此，机器学习算法理论上来说也可以通过图计算系统实现 对于之前提到的梯度下降，可以通过两方面来看： 从计算入手 由于数据没有关联，因此从计算入手是很直接的方式 从数据入手 将训练数据项看作一个顶点，构成一张无边的图 只需要每个顶点同时计算（compute），不需要进行sendMessage，最后使用aggregator更新参数 从参数入手 考虑局部梯度下降和局部参数调整 最后再更新全局参数 设计思路 以计算为中心 Mahout SystemML 以数据为中心 GraphLab PowerGraph GraphX 以参数为中心 Parameter Server TensorFlow 以计算为中心的机器学习系统 Mahout简介 最开始是作为Hadoop的机器学习库 使用map和reduce实现所有机器学习算法 MapReduce模型不适合做机器学习 执行很慢，尤其是迭代过程 编程模型让代码很难写和调试 缺乏声明式/缺乏join操作 机器学习系统编程环境 R/Matlab-like semantics Modern programming language qualities Scalability 机器学习DSL DSL：Domain-Specific Language Scala DSL Declarativity! Algebraic expression optimizer for distributed linear algebra（优化后送到各种引擎） provides a translation layer to distributed engines supports Apache Spark support Apache Flink 底层引擎及优化 Runtime & Optimization Execution is defered, user composes logical operators Computational actions implicitly trigger optimization (= selection of physical plan) and execution Optimization example 首先将矩阵按照行partition，再对每一行进行矩阵运算得到一个新的矩阵，再将新的矩阵按行parition存起来，这样对相同的一行相加就得到了计算结果的矩阵。 对于slim矩阵，可以不在最后做partition（因为最终的矩阵很小） "},"notes/stream.html":{"url":"notes/stream.html","title":"流计算系统概述","keywords":"","body":"流计算应用需求 静态数据 很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。技术人员可以利用数据挖掘和OLAP（On-Line Analytical Processing）分析工具从静态数据中找到对企业有价值的信息。 OLTP：主要面向事务处理（数据库） OLAP：数据仓库 基于HDFS的数据仓库：Hive 流数据 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达 实例：PM2.5检测、电子商务网站用户点击流、监控路网视频 数据特征 数据快速持续到达，潜在大小也许是无穷无尽的 数据来源众多，格式复杂 数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储 注重数据的整体价值，不过分关注个别数据 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序 流计算特征 无界（Unbounded） 数据记录(record)在计算过程中不断地动态到达 与批处理不同，计算过程开始之前就知道数据大小与边界，更容易优化 乱序（Out-of-order） record的原始顺序和在处理节点上的处理顺序可能不一致 shuffle过程（数据传递）也可能导致顺序改变 延迟（Delay） record的产生时间和在处理节点上的处理时间可能差别很大 流计算基本概念 状态 Data Stream Basics Input data is structured (has a schema) and there is no clear beginning or end Each record holds its origin-creation timestamp Application Logic is encapsulated within Processing Elements (PEs) Records travel across different PEs and invoke application logic Processing Model Data Stream sequence of records. Processing Element (PE) basic computational element (think of a tiny program / one function) PEs consume and produce data streams. Events are consumed only once in sequence -no back tracking! Summary States Synopsis(梗概) A summary of an infinite stream 是一种数据结构，记录一定时间内的数据状态 窗口 Stream Windows We often need to apply grouped aggregationon relevant sets of records (e.g. a user session). A stream window is a relevant slice(切片)in the space-time continuum(连续统一体) Range Most data stream processing systems allow window operations on the most recent history(eg.,1 minute, 1000 tuples) Trigger/Slide The frequency/granularity f is evaluated on a given range Stream Window types 时间 Time Semantics 如何定义时间？ Processing Time Local time in each PE Event-Time Origin-Time (remember? events carry time) 我们更需要Event-Time，但会有不少问题产生 How do we measure Event-Time? No total ordering by event timestamp 如何根据event-time进行window操作？ Low Watermarks Low watermark indicates that all data up to a given timestamp has been received. Low Watermarks indicate every PE of the “Reality” time ~ lowest origin time in the system. 系统可以确认该时间之前的数据全部已经收到并完成处理 More on Time Low Watermark局限性 在一定程度上保证数据的完整性、时效性 无法完全避免数据比low watermark晚到达 比如数据在没有进入到流式计算系统之前就延误了，low watermark根本不得而知 不同的系统处理方式不一样 超时机制，如目前为1:30，超时3h，则10:30之前的数据没到达，就不管了 "},"project/rpc.html":{"url":"project/rpc.html","title":"RPC model in Java","keywords":"","body":"Socket TCP 基于TCP的Socket通信，实现前后端通信 服务器端 服务器端需要先运行，指定监听端口，等待客户端接入 package TCP; import java.io.*; import java.net.ServerSocket; import java.net.Socket; public class TCPServer { public static void main(String[] args) { try { // create server socket, listening port ServerSocket serverSocket = new ServerSocket(8888); // call accept() function to get message System.out.println(\"---- TCP server is running, waiting for connecting\"); Socket socket = serverSocket.accept(); // get input stream InputStream in = socket.getInputStream(); // convert to string input stream InputStreamReader inreader = new InputStreamReader(in); // add buffer for input stream BufferedReader br = new BufferedReader(inreader); String info = null; while((info = br.readLine())!=null){ System.out.println(\"This is TCP server, client says: \"+info); } socket.shutdownInput(); // get output stream, send to client OutputStream outputStream = socket.getOutputStream(); PrintWriter printWriter = new PrintWriter(outputStream); printWriter.write(\"message from TCP server, welcome! \"); printWriter.flush(); socket.shutdownOutput(); //close resource printWriter.close(); outputStream.close(); br.close(); inreader.close(); in.close(); socket.close(); serverSocket.close(); } catch (IOException e) { e.printStackTrace(); } } } 客户端 客户端要指定IP地址和接口，并使用序列化完成发送消息和接收消息。 package TCP; import java.io.*; import java.net.Socket; public class TCPClient { public static void main(String[] args) { try { // create socket to connect, assign host and port Socket socket = new Socket(\"127.0.0.1\",8888); // create output stream, send message to server OutputStream outputStream = socket.getOutputStream(); // convert to print stream PrintWriter pw = new PrintWriter(outputStream); // send message pw.write(\"message from TCP client\"); pw.flush(); socket.shutdownOutput(); // get server stream InputStream inputStream = socket.getInputStream(); // add buffer for input stream BufferedReader br = new BufferedReader(new InputStreamReader(inputStream)); String info = null; while((info = br.readLine())!=null){ System.out.println(\"This is TCP client, server says: \"+info); } socket.shutdownInput(); // close resource br.close(); inputStream.close(); pw.close(); outputStream.close(); socket.close(); } catch (IOException e) { e.printStackTrace(); } } } UDP 与TCP类似，需要DatagramSocket和DatagramPacket库 服务器端 服务器端需要先运行，指定监听端口，等待客户端接入 package UDP; import java.io.IOException; import java.net.DatagramPacket; import java.net.DatagramSocket; import java.net.InetAddress; import java.net.SocketException; public class UDPServer { public static void main(String[] args) { try { // create DatagramSocket, port:8800 DatagramSocket socket = new DatagramSocket(8800); // byte array to receive client message byte[] data = new byte[1024]; DatagramPacket packet = new DatagramPacket(data,data.length); // receive data package socket.receive(packet); // read data String info = new String(data,0,packet.getLength()); System.out.println(\"I am Server,Client say:\"+info); // define port and ip InetAddress address = packet.getAddress(); int port = packet.getPort(); byte[] data2 = \"This is UTP server, welcome \".getBytes(); DatagramPacket packet1 = new DatagramPacket(data2,data2.length,address,port); socket.send(packet1); socket.close(); } catch (SocketException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } } 客户端 需要指定IP地址和端口，使用DatagramPacket包装消息发送 package UDP; import java.io.IOException; import java.net.*; public class UDPClient { public static void main(String[] args) { try { // define IP and port InetAddress inetAddress = InetAddress.getByName(\"localhost\"); int port = 8800; byte[] data = \"UDP send message, socket connect successful!\".getBytes(); // create DatagramSocket, port:8800 DatagramPacket packet = new DatagramPacket(data,data.length,inetAddress,port); // create data socket DatagramSocket socket = new DatagramSocket(); // send data pocket to server socket.send(packet); //socket.close(); // receive server response byte[] dataR = new byte[1024]; DatagramPacket packet1 = new DatagramPacket(dataR,dataR.length); socket.receive(packet1); // read message String info = new String(dataR,0,packet1.getLength()); System.out.println(\"This is UDP client,sever says:\"+info); socket.close(); } catch (UnknownHostException e) { e.printStackTrace(); } catch (SocketException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } } RPC RPC通信比较复杂，但主要思路还是很简单：进程通过参数传递的方式调用另一进程（通常在远程机器）中的一个函数或方法，并得到返回的结果 进程：客户端 另一进程：服务器 函数或方法：服务 RPC在使用形式上像调用本地函数（或方法）一样去调用远程的函数（或方法） 服务器端 主要由四个java程序构成，分别为main（主入口），RPCServer（具体实现），HelloService（示例函数） RPCServer 对需要的函数（服务）分一个线程进行阻塞等待 主要接收客户端传来的函数名和参数，并使用反射进行调用，返回结果 import java.io.IOException; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; import java.net.ServerSocket; import java.net.Socket; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class RPCServer { private ExecutorService threadPool; private static final int DEFAULT_THREAD_NUM = 10; public RPCServer(){ threadPool = Executors.newFixedThreadPool(DEFAULT_THREAD_NUM); } public void register(Object service, int port){ try { System.out.println(\"server starts...\"); ServerSocket server = new ServerSocket(port); Socket socket = null; while((socket = server.accept()) != null){ System.out.println(\"client connected...\"); // thread execute new service threadPool.execute(new Processor(socket, service)); } } catch (IOException e) { e.printStackTrace(); } } // implement Runnable interface(thread) class Processor implements Runnable{ Socket socket; Object service; public Processor(Socket socket, Object service){ this.socket = socket; this.service = service; } public void process(){ } @Override public void run() { try { // ObjectInputStream in = new ObjectInputStream(socket.getInputStream()); String methodName = in.readUTF(); Class[] parameterTypes = (Class[]) in.readObject(); Object[] parameters = (Object[]) in.readObject(); Method method = service.getClass().getMethod(methodName, parameterTypes); try { Object result = method.invoke(service, parameters); ObjectOutputStream out = new ObjectOutputStream(socket.getOutputStream()); out.writeObject(result); } catch (IllegalAccessException e) { e.printStackTrace(); } catch (IllegalArgumentException e) { e.printStackTrace(); } catch (InvocationTargetException e) { e.printStackTrace(); } } catch (IOException e) { e.printStackTrace(); } catch (NoSuchMethodException e) { e.printStackTrace(); } catch (SecurityException e) { e.printStackTrace(); } catch (ClassNotFoundException e1) { e1.printStackTrace(); } } } } HelloService 一个简单的函数实现，不过多介绍 public interface HelloService{ public String hello(String name); } public class HelloServiceImpl implements HelloService{ @Override public String hello(String name) { return \"Hello, \" + name; } } Main Server端的入口，首先需要新建一个RPCServer类，然后对需要的函数进行注册 public class Main { public static void main(String args[]){ HelloService helloService = new HelloServiceImpl(); RPCServer server = new RPCServer(); server.register(helloService,50001); } } 客户端 主要由RPCClient（主体功能）和HelloService（需要服务的接口）组成 RPCClient 使用newProxyInstance进行代理，当客户端访问该服务（函数）时，自动调用改写的invoke方法，代表客户端与服务器端进行通信。 注意，这里使用了匿名内部类和模板类实现。 import java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.net.Socket; public class RPCClient { public static void main(String args[]){ // This helloService is a proxy object,it will call remote real object to finish method // need helloService interface HelloService helloService = getClient(HelloService.class, \"127.0.0.1\", 50001); System.out.println(helloService.hello(\"duyuntao\")); } @SuppressWarnings(\"unchecked\") public static T getClient(Class clazz, String ip, int port){ return (T) Proxy.newProxyInstance(RPCClient.class.getClassLoader(), new Class[]{clazz}, new InvocationHandler() { // this is a anonymous inner class @Override public Object invoke(Object arg0, Method method, Object[] arg) throws Throwable { Socket socket = new Socket(ip, port); ObjectOutputStream out = new ObjectOutputStream(socket.getOutputStream()); out.writeUTF(method.getName()); out.writeObject(method.getParameterTypes()); out.writeObject(arg); ObjectInputStream in = new ObjectInputStream(socket.getInputStream()); return in.readObject(); } }); } } HelloService 只提供一个接口用于告诉java这个所需的类里面有什么服务 实际的函数调用在服务器端 public interface HelloService{ public String hello(String name); } Reference 在Ubuntu下安装Java Java Socket编程实例 rpc原理 java实现 Java的代理 匿名内部类 接口和抽象类的区别 线程的启动 "},"project/hadoop.html":{"url":"project/hadoop.html","title":"hadoop安装与配置","keywords":"","body":"准备 创建Hadoop用户 创建新用户 sudo useradd –m hadoop –s /bin/bash 设置密码 sudo passwd hadoop 增加管理员权限 sudo adduser hadoop sudo 最后，切换到该用户进行登录 SSH登录权限设置 配置SSH的原因 Hadoop名称节点（NameNode）需要启动集群中所有机器的Hadoop守护进程，这个过程需要通过SSH登录来实现 Hadoop并没有提供SSH输入密码登录的形式，因此，为了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们 SSH本机免密钥登录 首先安装openssh 查看当前/home/hadoop目录下有无.ssh文件夹，若无，则创建mkdir ~/.ssh，修改权限 chmod 700 ~/.ssh 执行指令ssh-keygen –t rsa生成公钥和私钥 (一路回车) 执行 cat ./id_rsa.pub >> ./authorized_keys将密钥加入授权 执行指令 ssh localhost 进行测试 Java 首先确保Linux系统中已经装好Java 在oracle官网安装最新版本。 默认下载到download目录 当前目录下进行解压 tar -xvf jdk-8u161-linux-x64.tar.gz 移动到目录/usr/local/Java 配置环境变量（vim ~/.bashrc） #JAVA export JAVA_HOME=/usr/local/Java/jdk1.8.0_181/ export JRE_HOME=JAVAHOME/jreexportPATH=JAVA_HOME/jre export PATH=JAVAH​OME/jreexportPATH=JAVA_HOME/bin:JREHOME/bin:JRE_HOME/bin:JREH​OME/bin:PATH export CLASSPATH=JAVAHOME/lib:JAVA_HOME/lib:JAVAH​OME/lib:JRE_HOME/lib:. 退出后刷新source ~/.bashrc 测试java -version Hadoop 下载清华的镜像 默认下载到download 解压到/usr/local sudo tar -zxf ./hadoop-3.1.1.tar.gz -C /usr/local 切换到解压目录并修改文件权限 cd /usr/local sudo mv ./hadoop-3.1.1 ./hadoop #重命名 sudo chown -R hadoop ./hadoop # 修改文件权限 查看版本号及是否安装好 cd /usr/local/hadoop ./bin/hadoop version 设置环境变量，以便直接使用hadoop命令 进入vim ~/.bashrc export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin 直接输入hadoop看是否成功 后面，也可以直接输入hdfs dfs进入HDFS文件系统 Hadoop目录结构 bin：Hadoop最基本的管理脚本和使用脚本的目录，这些脚本是sbin目录下管理 脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop etc：Hadoop配置文件所在的目录，包括core-site,xml、hdfs-site.xml、mapredsite.xml等 include：对外提供的编程库头文件（具体动态库和静态库在lib目录中），这些头 文件均是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序 lib：该目录包含了Hadoop对外提供的编程动态库和静态库，与include目录中的 头文件结合使用 libexec：各个服务对用的shell配置文件所在的目录，可用于配置日志输出、启动 参数（比如JVM参数）等基本信息 sbin：Hadoop管理脚本所在的目录，主要包含HDFS和YARN中各类服务的启动/关 闭脚本 share：Hadoop各个模块编译后的jar包所在的目录 Hadoop单机部署 默认为非分布式模式，无须进行其他配置即可运行。附带了很多例子，可以直接查看所有例子： cd /usr/local/hadoop hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar 会显示grep，join，wordcount等例子 这里选择grep例子，流程为先建一个input文件夹，并复制一些文件到该文件；然后运行grep程序，将input文件夹的所有文件作为grep的输入，让grep程序从所有文件中筛选出符合正则表达式的单词，并输出结果到output mkdir input cp ./etc/hadoop/*.xml ./input hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar grep ./input ./output 'dfs[a-z.]+' 这里需要注意的是，hadoop默认要求输出output不存在，若存在则会报错 查看运行结果cat ./output/* Hadoop伪分布式部署 Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/中，伪分布式需要修改2个配置文件 core-site.xml和 hdfs-site.xml 配置文件是 xml格式，每个配置以声明 property的 name 和 value的方式来实现 Hadoop在启动时会读取配置文件，根据配置文件来决定运行在什么模式下 修改配置文件 vim core-site.xml 打开后如图所示： 修改为 hadoop.tmp.dir file:/usr/local/hadoop/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也包括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可 name为fs.defaultFS的值，表示hdfs路径的逻辑名称 vim hdfs-site.xml 修改为： dfs.replication 1 dfs.namenode.name.dir file:/usr/local/hadoop/tmp/dfs/name dfs.datanode.data.dir file:/usr/local/hadoop/tmp/dfs/data dfs.replication表示副本的数量，伪分布式要设置为1 dfs.namenode.name.dir表示名称节点的元数据保存目录 dfs.datanode.data.dir表示数据节点的数据保存目录 格式化节点 cd /usr/local/hadoop/ ./bin/hdfs namenode -format 若成功，则显示 启动Hadoop 执行命令： cd /usr/local/hadoop ./sbin/start-dfs.sh 若出现报错：则表示JAVA_HOME目录没有设置对，如果之前使用Java没有问题，则直接进入Hadoop环境中设置 cd /usr/local/hadoop/etc/hadoop vim hadoop-env.sh 直接添加自己电脑里的Java_HOME路径，如： 再次启动 判断是否启动成功 Web界面查看HDFS信息 启动Hadoop后，在浏览器输入http://localhost:9870/，可访问NameNode。 如图所示，表示成功： 关闭Hadoop cd /usr/local/hadoop ./sbin/stop-dfs.sh 下次启动时，无须再进行名称节点的格式化 关于三种Shell命令方式的区别 hadoop fs hadoop fs适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统 hadoop dfs hadoop dfs只能适用于HDFS文件系统 hdfs dfs hdfs dfs跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统 操作示例 还原单机模型的grep例子 cd /usr/local/hadoop ./bin/hdfs dfs -mkdir -p /user/hadoop # 在HDFS中为hadoop用户创建目录（Linux文件系统中不可见） ./bin/hdfs dfs -mkdir input # 在HDFS中创建hadoop用户对应的input目录 ./bin/hdfs dfs -put ./etc/hadoop/*.xml input # 将本地文件复制到HDFS中 ./bin/hdfs dfs –ls input # 查看HDFS中的文件列表 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output ‘dfs[a-z.]+’ ./bin/hdfs dfs -cat output/* #查看运行结果 可能出现警告信息，可忽略 查看文件列表 最后结果 "},"project/hadoop1.html":{"url":"project/hadoop1.html","title":"hadoop编程实践（一）","keywords":"","body":"Hadoop操作 目录操作 在操作之前，需要在hadoop根目录下创建与Linux用户同名的user目录 ./bin/hdfs dfs -mkdir -p /user/hadoop 之后，所有的文件都默认放入这个目录下面，很多命令与Linux命令一致，比如查看当前文件夹： 这个input是这样创建的： ./bin/hfs dfs -mkdir input 若/input，表示在HDFS的根目录创建input目录 文件操作 本地->Hadoop 将本地文件移动到hadoop的input文件夹下： 查看Hadoop的input文件夹下的文件： Hadoop->本地 同时，也可以将Hadoop上的文件下载到本地： Hadoop之间 在Hadoop的文件之间进行传输，与Linux上文件传输无异 注意，要使用-cp命令，一定要确保目标文件夹存在： 配置IDE环境 下载IDEA 首先在官网下载IDEA到Download： 然后执行解压命令，解压到/usr/local sudo tar -xvf ideaIU-2018.2.4.tar.gz -C /usr/local 进入该目录，执行idea.sh，进行安装： 导入依赖包 /usr/local/hadoop/share/hadoop/common目录下的： hadoop-common-xxxx.jar hadoop-nfs-xxx.jar /usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client.xx.jar(3.xx版本需要新加入) /usr/local/hadoop/share/hadoop/hdfs目录下： hadoop-hdfs-xxx.jar hadoop-hdfs-nfs-xxx.jar usr/local/hadoop/share/hadoop/common/lib目录下的所有JAR包 /usr/local/hadoop/share/hadoop/hdfs/lib目录下的所有JAR包 设置Global Libraries 在setting里面设置好Global Libraries后，每次新建工程，若需要这些库（例如Hadoop库），那么需要添加依赖： 可以参考这里 运行Hadoop文件 测试文件 以下文件用于测试HDFS中是否存在一个文件。 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; public class HDFSFileExist { public static void main(String[] args){ try { String fileName = \"test\"; Configuration conf = new Configuration(); conf.set(\"fs.defaultFS\",\"hdfs://localhost:9000\"); conf.set(\"fs.hdfs.impl\",\"org.apache.hadoop.hdfs.DistributedFileSystem\"); FileSystem fs = FileSystem.get(conf); if(fs.exists(new Path(fileName))){ System.out.println(\"file exist!\"); }else{ System.out.println(\"file not exist\"); } }catch (Exception e){ e.printStackTrace(); } } } 这里，需要检测的文件名称问test，没有给出路径全称，则表示采用了相对路径，就是当前登录Linux系统的用户Hadoop，在对应的目录下是否存在test，也就是/usr/hadoop目录下是否存在test文件。 结果 在IDEA中直接运行，可得到如下结果： 将项目打包成JAR包 在IDEA中，右键项目，选择open module setting，进入Artifact，点+号： 选择with dependencied，新建一个： 选择Main class，其余默认 然后build artifact： build之后，在对应文件夹中找到打包的JAR包： 在本地尝试是否打包成功： 出现如图结果就表示成功！ "},"project/hadoop2.html":{"url":"project/hadoop2.html","title":"hadoop编程实践（二）","keywords":"","body":"集群上使用 jar包 首先将之前FileExist文件进行打包，得到.jar文件： 将其拷贝到集群中，并使用hadoop jar命令运行： WordCount 添加依赖 首先我们需要新建一个WordCount项目，首先要添加Hadoop的包依赖 /usr/local/hadoop/share/hadoop/common hadoop-common-xxx.jar hadoop-nfs-xxx.jar /usr/local/hadoop/share/hadoop/common/lib 下的所有Jar包 /usr/local/hadoop/share/hadoop/mapreduce该目录下所有JAR包 /usr/local/hadoop/share/hadoop/mapreduce/lib目录下所有JAR包 编写程序 import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class WordCount { public WordCount () { } public static class TokenizerMapper extends Mapper{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public TokenizerMapper () { } public void map(Object key, Text value, Mapper.Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { this.word.set(itr.nextToken()); context.write(this.word, one); } } } public static class IntSumReducer extends Reducer { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable values, Reducer.Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } this.result.set(sum); context.write(key, this.result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length [...] \"); System.exit(2); } Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(WordCount.TokenizerMapper.class); job.setCombinerClass(WordCount.IntSumReducer.class); job.setReducerClass(WordCount.IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); for (int i = 0; i 打包成JAR包 打开Project Structure： 进行编译： 生成并查看JAR包： 本地伪分布式运行 创建两个文件作为输入，内容为： I love Spark I love Hadoop Hadoop is good Spark is fast 将本地文件放入hdfs中： hdfs dfs -mkdir -p /user/hadoop/input hdfs dfs -put ./wordfile1.txt input hdfs dfs -put ./wordfile2.txt input 在hdfs中查看： hdfs dfs -ls input 运行： hadoop jar WordCount.jar input output 查看结果： hdfs dfs -cat output/* 集群上运行 首先将JAR包和文件放入集群： 将其拷贝到HDFS中： hdfs dfs -mkdir -p /user/hadoop7/input hdfs dfs -put ./wordfile1.txt input hdfs dfs -put ./wordfile2.txt input 查看文件： 运行： hadoop jar WordCount.jar input output 运行成功 查看生成文件 hdfs dfs -cat /user/hadoop7/output/* 查看集群运行情况 在连接VPN时，在浏览器中输入10.11.6.91:50070 "},"project/hadoop-coding.html":{"url":"project/hadoop-coding.html","title":"hadoop编程练习","keywords":"","body":"基础MapReduce 问题 班级学生成绩的随机生成 输入：本班同学的学号 输出： 数据准备 首先需要一个stuID.csv文件，每一列为一个学号： 然后将文件放入HDFS中： hdfs dfs put stuID.csv input 编写程序 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; import java.io.IOException; import java.util.Random; public class Score { public Score() { } public static class StuIDMapper extends Mapper { private final static IntWritable one = new IntWritable(1); LongWritable ID = new LongWritable(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException { ID.set(Long.valueOf(value.toString())); context.write(this.ID, one); } } public static class ScoreReducer extends Reducer { private IntWritable score = new IntWritable(); private Random rand = new Random(); public void reduce(LongWritable ID, Iterable values, Context context) throws IOException, InterruptedException { score.set(rand.nextInt(100) + 1); context.write(ID, this.score); } } public static void main(String[] args) throws Exception { org.apache.hadoop.conf.Configuration conf = new Configuration(); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length [...] \"); System.exit(2); } Job job = Job.getInstance(conf, \"student score\"); job.setJarByClass(Score.class); job.setMapperClass(Score.StuIDMapper.class); job.setReducerClass(Score.ScoreReducer.class); job.setOutputKeyClass(LongWritable.class); job.setOutputValueClass(IntWritable.class); for (int i = 0; i 这里需要注意随机数的生成new Random() 同时，由于学号比较长，必须用LongWritable类型输入 运行 hadoop jar Score.jar input/stuID.csv output hdfs dfs -ls output/* 分组MapReduce 问题 求平均成绩：将全班同学每隔5号分为一组，求每组的平均成绩 输入： 输出： 数据准备 首先需要一个score.csv文件，每一列为学号和学生成绩： 然后将文件放入HDFS中： hdfs dfs put score.csv input 编写程序 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.FloatWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Partitioner; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader; import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; import java.io.IOException; import java.util.Random; public class GetAvgScore { public GetAvgScore() { } public static class StuScoreMapper extends Mapper { IntWritable Group = new IntWritable(); IntWritable Score = new IntWritable(); public void map(Text key, Text value, Context context) throws IOException, InterruptedException { // every 5 students as a group,[1,2,3,4,5] => group1 Group.set((Integer.parseInt(key.toString()) - 1) / 5 + 1); Score.set(Integer.parseInt(value.toString())); context.write(this.Group, this.Score); } } public static class AvgScoreReducer extends Reducer { private FloatWritable avgscore = new FloatWritable(); public void reduce(IntWritable Group, Iterable Score, Context context) throws IOException, InterruptedException { int sum = 0; int count = 0; for (IntWritable val : Score) { sum += val.get(); count++; } avgscore.set((float) sum / count); context.write(Group, avgscore); } } public static void main(String[] args) throws Exception { org.apache.hadoop.conf.Configuration conf = new Configuration(); // set seperator conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \",\"); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length [...] \"); System.exit(2); } Job job = Job.getInstance(conf, \"student avg score\"); job.setInputFormatClass(KeyValueTextInputFormat.class); job.setJarByClass(GetAvgScore.class); job.setMapperClass(GetAvgScore.StuScoreMapper.class); job.setReducerClass(GetAvgScore.AvgScoreReducer.class); job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(FloatWritable.class); for (int i = 0; i 这里需要注意，使用KeyValueTextInputFormat代替了默认的TextInputFormat，在进入mapper前就对数据进行了分割处理。 同时，在map过程中，将每个key（学生的学号）得到对应的组，作为key传入reduce中计算。 这里需要注意，必须指明map的输出类型：job.setOutputKeyClass 运行 hadoop jar avgcore.jar input/score.csv output/avgscore/ hdfs dfs -cat output/avgscore/* 结果为： hadoop jar Natural\\ join.jar input/person.txt input/address.txt output/natural_join 自然连接（natural join） 数据准备 有两个文件 person.txt 1 Aaron 210000 2 Abbott 214000 3 Abel 221000 4 Abner 215000 5 Abraham 226000 6 Adair 225300 7 Adam 223800 8 Addison 224000 9 Adolph 223001 address.txt 210000 Nanjing 214000 Wuxi 221000 Xuzhou 213000 Changzhou 要求以code为连接属性，匹配出person中每个人所在的位置信息；每条记录各个字段之间以空格为分隔符。 编写程序 import org.apache.commons.lang.StringUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.FileSplit; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; import java.io.IOException; import java.util.ArrayList; import java.util.List; public class natural_join { public natural_join() { } public static class joinMapper extends Mapper { private static final String PERSON_FLAG = \"person\"; private static final String ADDRESS_FLAG = \"address\"; private FileSplit fileSplit; private Text outKey = new Text(); private Text outValue = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException { fileSplit = (FileSplit) context.getInputSplit(); String filePath = fileSplit.getPath().toString(); String line = value.toString(); String[] fields = StringUtils.split(line, \" \"); // 判断记录来自哪个文件 if (filePath.contains(PERSON_FLAG)) { if (fields.length { private static final String PERSON_FLAG = \"person\"; private static final String ADDRESS_FLAG = \"address\"; private String fileFlag = null; private String cityName = null; private Text outCity = new Text(); private Text outPerson = new Text(); public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { List perosonInfo = new ArrayList<>(); for (Text val : values) { String[] fields = StringUtils.split(val.toString(), \",\"); fileFlag = fields[0]; // choose what file it is if (fileFlag.equals(ADDRESS_FLAG)) { cityName = fields[1]; outCity.set(cityName); } else if (fileFlag.equals(PERSON_FLAG)) { perosonInfo.add(fields[1]); } } // Cartesian product for (String person : perosonInfo) { outPerson.set(person); context.write(outPerson, outCity); } } } public static void main(String[] args) throws Exception { org.apache.hadoop.conf.Configuration conf = new Configuration(); conf.set(\"mapreduce.output.textoutputformat.separator\", \" \"); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length \"); System.exit(2); } Job job = Job.getInstance(conf, \"natural join\"); job.setJarByClass(natural_join.class); job.setMapperClass(natural_join.joinMapper.class); job.setReducerClass(natural_join.joinReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); for (int i = 0; i 程序逻辑主要是以code作为key进行reduce，将文件来源信息放入value中，使用笛卡尔积进行匹配。 这里需要注意一点的是，person中的code可能是缺省的，因此不能进行匹配！ 同时，需要注意Reducer定义的perosonInfo，只能在类里面定义，不能定义成类成员变量。 运行 hadoop jar Natural\\ join.jar input/person.txt input/address.txt output/natural_join hdfs dfs -cat output/natural_join/* Kmeans实现 数据准备 输入数据（k-means.dat）： 4,400 96,826 606,776 474,866 400,768 2,920 356,766 36,687 -26,824 第一行标明K的值和数据个数N, 均为整形, 由\",\"隔开 (如 3,10 表示K=3, N=10)。 之后N行中每行代表一个二维向量, 向量元素均为整形, 由\",\"隔开 (如 1,2 表示向量(1, 2))。 输出: K行, 每行是一个聚类图心的二维向量, 向量元素均为浮点型 (如 1.1,2.3)。 编程思路 首先需要初始化中心点，这里使用前四行作为初始中心点，现将其写入cache中。 每次map时使用setup函数读入变量中，依据所有点与中心店的距离选择其属于的类。 在reduce中，根据类别进行分组，对每组聚类重新选择中心点，将中心点输出到目标文件中。 判断两次的中心点是否满足阈值条件，若不满足，则将新生成的中心点移动到cache中，作为下一次迭代的中心点。 迭代结束的标志为：满足最大迭代次数或满足阈值条件。 Point类 由于我们要对点进行操作，更方便的方法是先新建一个Point类： class Point{ double x; double y; Point(){ } Point(double x,double y){ this.x=x; this.y=y; } public double EuclideanDis(Point other) { double distance = 0; distance = Math.pow((this.x - other.getX()),2) + Math.pow((this.y - other.getY()),2); return Math.sqrt(distance); } public double getX() { return x; } public double getY(){ return y; } } kmeansMapper 这里使用Distribution cache将小文件（中心点）广播出去，避免了大量的数据移动。 注意，在使用Hadoop2.7.3中，只能使用旧的Distribution cacheAPI，新的API会报错： String localCacheFiles = context.getLocalCacheFiles()[0].getName(); BufferedReader br = new BufferedReader(new FileReader(localCacheFiles)); Mapper程序为： import org.apache.commons.lang.StringUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import java.io.BufferedReader; import java.io.FileReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.URI; import java.util.ArrayList; import java.util.List; public class kmeansMapper extends Mapper { private List means; /** * reading the data from the distributed cache */ public void setup(Context context) throws IOException, InterruptedException { means = new ArrayList(); // // URI[] cacheFiles = context.getCacheFiles(); // BufferedReader br = new BufferedReader(new FileReader(cacheFiles[0].toString())); Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); FSDataInputStream hdfsInStream = fs.open(new Path(\"output/cache/part-r-00000\")); InputStreamReader isr = new InputStreamReader(hdfsInStream, \"utf-8\"); BufferedReader br = new BufferedReader(isr); String lineString = null; while((lineString = br.readLine()) != null){ String[] keyValue = StringUtils.split(lineString,\",\"); Point tmpCluster = new Point(Double.parseDouble(keyValue[0]),Double.parseDouble(keyValue[1])); means.add(tmpCluster); } br.close(); } public void map(LongWritable key, Text keyvalue, Context context) throws IOException, InterruptedException{ // ignore first line if (key.get() == 0) return; String[] keyValue = StringUtils.split(keyvalue.toString(),\",\"); String X = keyValue[0]; String Y = keyValue[1]; Point tmpPoint = new Point(Double.parseDouble(X),Double.parseDouble(Y)); context.write(new IntWritable(findClosest(tmpPoint)), new Text(X + \",\" + Y)); } /** * method that returns the closest mean from the point * @param value * @return */ private int findClosest(Point value){ int argmin = 0; double minimalDistance = Double.MAX_VALUE ; for(int i = 0; i kmeansReducer import org.apache.commons.lang.StringUtils; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; import java.io.IOException; public class kmeansReducer extends Reducer { public void reduce(IntWritable key, Iterable values, Context context) throws IOException, InterruptedException{ double sumX = 0.0; double sumY = 0.0; int count = 0; for(Text value : values){ String[] keyValue = StringUtils.split(value.toString(),\",\"); sumX += Double.parseDouble(keyValue[0]); sumY += Double.parseDouble(keyValue[1]); count ++; } context.write(new DoubleWritable(sumX/count), new DoubleWritable(sumY/count)); } } kmeansMain 这里需要注意的是，我们知道每次MapReduce生成的文件后缀为part-r-00000，因此需要指定文件名进行移动。 同时，需要注意在读写文件时，需要指定编码格式为UTF-8 import org.apache.commons.lang.StringUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.util.GenericOptionsParser; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.URI; import java.net.URISyntaxException; public class kmeansMain { private static final String CACHED_PATH = \"output/cache\"; private static final String ACTUAL_PATH = \"output/means\"; // directory store acutal result private static final String CACHED_MEANS = \"output/cache/part-r-00000\"; private static final String ACTUAL_MEANS = \"output/means/part-r-00000\"; public static void writeFileByline(String dst, String contents) throws IOException{ Configuration conf = new Configuration(); Path dstPath = new Path(dst); FileSystem fs = dstPath.getFileSystem(conf); FSDataOutputStream outputStream = null; if (!fs.exists(dstPath)) { outputStream = fs.create(dstPath); }else{ outputStream = fs.append(dstPath); } contents = contents + \"\\n\"; outputStream.write(contents.getBytes(\"utf-8\")); outputStream.close(); } public static int readFileByLines(String fileName,String meansPath) throws IOException { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(fileName), conf); FSDataInputStream hdfsInStream = fs.open(new Path(fileName)); InputStreamReader isr = new InputStreamReader(hdfsInStream, \"utf-8\"); BufferedReader br = new BufferedReader(isr); // get first line k String line = br.readLine(); int k = Integer.parseInt(StringUtils.split(line, \",\")[0]); int count = 0; while ((line = br.readLine()) != null && count \"); System.exit(2); } int code = 0; Path inputPath = new Path(otherArgs[0]); Path outputDir = new Path(otherArgs[1] + \"\"); Path cacheMeansPath = new Path(CACHED_MEANS); Path actualMeansPath = new Path(ACTUAL_MEANS); Path cachePath = new Path(CACHED_PATH); Path actualPath = new Path(ACTUAL_PATH); int k = readFileByLines(otherArgs[0],ACTUAL_MEANS); int maxIterations = 500; double threshold = 0.000001; // Delete output if exists FileSystem hdfs = FileSystem.get(conf); if (hdfs.exists(outputDir)) hdfs.delete(outputDir, true); // recursive delete boolean changed = false; int counter = 0; while(!changed && counter > \"+counter + \" || means stable: \"+ changed); } hdfs.rename(actualPath, outputDir); System.exit(code); } } 运行 同样的方法，打包成JAR包运行， hadoop jar Kmeans.jar input/k-means.dat output/kmeans 结果为： 说明只用了8次迭代就达到稳定状态了。 查看运行结果，也就是中心点： hdfs dfs -cat output/kmeans/* "},"project/hadoop-recap.html":{"url":"project/hadoop-recap.html","title":"Hadoop 编程总结","keywords":"","body":"基本类型 Text 主要接口和方法可以参考这里。 构造 private Text word = new Text() 这里的Text类与String类似 方法 set(String string) 复制一个String类型到当前Text中 IntWritable 构造 private IntWritable one = new IntWritable(1) 方法 get() 得到这个值 set(int) 设置值 文件操作 HDFS文件 得到一个HDFS文件系统，这样就可以使用常规的文件操作方式进行操作 Configuration conf = new Configuration(); FileSystem hdfs = FileSystem.get(conf); 文件CRUD 创建或追加文件并写入 outputStream = fs.create(dstPath)； outputStream = fs.append(dstPath); outputStream.write(contents.getBytes(\"utf-8\")); 删除文件（夹） hdfs.delete(outputDir, true); // recursive delete 移动文件（重命名） hdfs.rename(actualPath, cachePath); MapReduce InputFormat TextInputFormat TextInputFormat是默认的InputFormat，每条记录是一行输入 Key是LongWriteable类型 ，存储该行在整个文件中的字节偏移量 一般来说，很难讲字节偏移量转为行号，除非每一行的偏移量都相同 Value是这行的内容，不包含任何行终止符（回车符和换行符），被打包成一个Text对象 KeyValueTextInputFormat 一般来说，偏移量没有太大用，通常文件时暗转键值对组织的，使用某个分界符进行分割。 Hadoop的默认分隔符是制表符，可以这样修改： // 创建配置信息 Configuration conf = new Configuration(); // 设置行的分隔符，这里是制表符，第一个制表符前面的是Key，第一个制表符后面的内容都是value conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \"\\t\"); // 设置输入数据格式化的类 job.setInputFormatClass(KeyValueTextInputFormat.class); NLineInputFormat 通过TextInputFormat和KeyValueTextInputFormat，每个mapper受到的输入行数不同，取决于分片的大小和行的长度。 如果希望每个mapper受到的行数相同，需要将NLineInputFormat作为InputFormat。 与TextInputFormat一样，键是文件中的字节偏移量，值是行本身。 Mapper public class TokenCounterMapper extends Mapper{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } 这里以WordCount为例，首先，TokenCounterMapper继承了Mapper泛型类，并确定keyIn, valueIn,keyOut,valueOut的类型。 然后定义需要的变量 同时重写map函数，其中的参数的类型要与之前的类的类型一致，注意，context类是用来传递数据以及其他运行状态信息，这里Context context自动匹配到Text, IntWritable，也可以这样写： public void map(Object key, Text value, Mapper.Context context) 这里直接指定本Mapper类中的Context类型。 如论如何，程序最终都需要向Context.write()中传递两个参数，也就是map的keyOut,valueOut。 注意，当mapper段的输出kv与最后reducer的kv数据类型不一致时，需要在主程序中指明输出类型：setMapOutputKeyClass，setMapOutputValueClass。 Setup 在mapper端可以使用setup函数，该函数在进入map阶段前运行，每个job只会运行一次： public void setup(Context context) throws IOException, InterruptedException {} Context 在Mapper中的map、以及Reducer中的reduce都有一个Context的类型。 Context应该是用来传递数据以及其他运行状态信息，map中的key、value写入context，让它传递给Reducer进行reduce，而reduce进行处理之后数据继续写入context，继续交给Hadoop写入hdfs系统。 理解： mapreduce中的context类 The Context object allows the Mapper/Reducer to interact with the rest of the Hadoop system. Configuration Configuration(boolean loadDefaults) Configuration可以用来在MapReduce任务之间共享信息，当然这样共享的信息是在作业中配置，一旦作业中的map或者reduce任务启动了，configuration对象就完全独立。所以共享信息是在作业中设置的。 当loadDefaults为false时，Configuration对象就不会将通过addDefaultResource(String resource)加载的配置文件载入内存。但是会将通过addResource(...)加载的配置文件载入内存。 参考这里 Reduce public static class IntSumReducer extends Reducer { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable values, Reducer.Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } this.result.set(sum); context.write(key, this.result); } } 同样以WordCount为例，这里与map很相似，IntSumReducer继承了Reducer泛型类，并确定keyIn, valueIn,keyOut,valueOut的类型。 注意，在定义reduce函数时，这里的输入为形式，因此需要Iterable类型作为valueIn。 最后与map一样，写入context中。 可以设置Reducer的个数： job.setNumReduceTasks(6); Main public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); // 程序运行时参数 String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length [...] \"); System.exit(2); } Job job = Job.getInstance(conf, \"word count\"); // 设置环境参数 job.setJarByClass(WordCount.class); // 设置整个程序的类名 job.setMapperClass(WordCount.TokenizerMapper.class); // 添加Mapper类 job.setCombinerClass(WordCount.IntSumReducer.class); // 添加Combiner类 job.setReducerClass(WordCount.IntSumReducer.class); // 添加Reducer类 job.setMapOutputValueClass(IntWritable.class); // 设置mapper输出类型 job.setOutputKeyClass(Text.class); // 设置mapper输出类型 job.setOutputKeyClass(Text.class); // 设置reducer输出类型 job.setOutputValueClass(IntWritable.class); // 设置reducer输出类型 for (int i = 0; i 设置 conf.set(\"mapreduce.output.textoutputformat.separator\", \" \"); 可以更改输出的kv分隔符，默认为\\t distributed cache 可设置Distribution cache job.addCacheFile(cacheMeansPath.toUri()); 使用时： String localCacheFiles = context.getLocalCacheFiles()[0].getName(); BufferedReader br = new BufferedReader(new FileReader(localCacheFiles)); 组合式MapReduce 隐式依赖描述（不建议） private void execute() throws Exception{ String tempOutPutPath = OutPutPath + \"_tmp\"; runWordCountJob(inputPath,tempOutPutPath); runFrequenciesJob(tempOutPutPath,OutPutPath); } private int runWordCountJob(String inputPath, String tempOutPutPath) throws Exception{ Configuration conf = new Configuration(); // 程序运行时参数 .... } private int runFrequenciesJob(String tempOutPutPath, String OutPutPath) throws Exception{ Configuration conf = new Configuration(); // 程序运行时参数 .... } 显示依赖描述 Configuration job1conf = new Configuration(); Job job1 = new Job(job1conf,\"Job1\"); .........//job1 其他设置 Configuration job2conf = new Configuration(); Job job2 = new Job(job2conf,\"Job2\"); .........//job2 其他设置 Configuration job3conf = new Configuration(); Job job3 = new Job(job3conf,\"Job3\"); .........//job3 其他设置 job3.addDepending(job1);//设置job3和job1的依赖关系 job3.addDepending(job2); JobControl JC = new JobControl(\"123\"); JC.addJob(job1);//把三个job加入到jobcontorl中 JC.addJob(job2); JC.addJob(job3); JC.run(); 链式MapReduce public void function throws IOException { Configuration conf = new Configuration(); Job job = new Job(conf); job.setJobName(\"ChianJOb\"); // 在ChainMapper里面添加Map1 Configuration map1conf = new Configuration(false); ChainMapper.addMapper(job, Map1.class, LongWritable.class, Text.class, Text.class, Text.class, true, map1conf); // 在ChainReduce中加入Reducer，Map2； Configuration reduceConf = new Configuration(false); ChainReducer.setReducer(job, Reduce.class, LongWritable.class, Text.class, Text.class, Text.class, true, map1conf); Configuration map2Conf = new Configuration(); ChainReducer.addMapper(job, Map2.class, LongWritable.class, Text.class, Text.class, Text.class, true, map1conf); job.waitForCompletion(true); } "},"project/spark-installation.html":{"url":"project/spark-installation.html","title":"spark安装与配置","keywords":"","body":"安装Spark 首先在官网上安装对应版本，因为已经安装了hadoop，选择without hadoop版本。 执行解压、修改文件名、配置文件等操作： sudo tar -zxf spark-2.3.2-bin-without-hadoop.tgz -C /usr/local cd /usr/local sudo mv ./spark-2.3.2-bin-without-hadoop/ ./spark sudo chown -R hadoop:hadoop ./spark cd spark/ cp ./conf/spark-env.sh.template ./conf/spark-env.sh export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath) vim conf/spark-env.sh 同时，将/usr/local/spark/bin目录加入系统PATH：~/.bashrc，并刷新source ~/.bashrc。 Spark Shell 执行spark shell： bin/run-example SparkPi bin/spark-shell 出现如下界面： 测试Spark shell 浏览器查看 启动Spark shell时后，在浏览器中输入localhost:4040： 文件访问 首先访问本地的文件： val textFile = sc.textFile(\"file:///home/hadoop/Documents/distribution/Kmeans2/README.txt\") textFile.first() 访问HDFS上的文件 val textFile = sc.textFile(\"hdfs://localhost:9000/user/hadoop/input/k-means.dat\") textFile.first() 在这里也可以不指定localhost，以下三种方式都是等价的： val textFile = sc.textFile(\"hdfs://localhost:9000/user/hadoop/input/k-means.dat\") val textFile = sc.textFile(\"/user/hadoop/input/k-means.dat\") val textFile = sc.textFile(\"input/k-means.dat\") WordCount val textFile = sc.textFile(\"file:///usr/local/spark/README.md\") val wordCount = textFile.flatMap(line=>line.split(\"\")).map(word=>(word,1)).reduceByKey((a,b)=>a+b) wordCount.collect() 在这里，textFile包含多行文本内容，flatMap会遍历其中的每行文本内容，当遍历当一行文本内容时，会把本行内容赋值给变量line，并执行Lambda表达式line => line.split(\"\")。 这里采用每个单词分隔符，切分得到拍扁的单词集合。 然后执行map函数，遍历这个集合中的每个单词，将输入的word构建得到一个映射（Map是一种数据结构），这个映射key是word，value为1. 得到映射后，包含很多(key，value)，执行reduce，按照key进行分布，然后使用给定的函数对相同key的多个value进行聚合操作，得到聚合后的(key，value) 在集群中访问 spark-shell --master spark://10.11.6.91:7077 val textFile = sc.textFile(\"hdfs://10.11.6.91:9000/README.md\") textFile.count() "},"project/spark1.html":{"url":"project/spark1.html","title":"spark编程实践","keywords":"","body":"Spark编程 Scala实现 sbt 首先安装sbt unzip sbt-1.2.6.zip -d /usr/local/ 赋予权限 sudo chown –R hadoop /usr/local/ 添加进环境变量PATH中vim ~/.bashrc。 执行sbt version看是否安装成功，此时会下载一些依赖，确保联网。 编程 在/home/hadoop目录下创建sparkapp文件夹，作为程序根目录 mkdir ./sparkapp mkdir –p ./sparkapp/src/main/scala 接着在sparkapp/src/main/scala下创建SimpleApp.scala： /* SimpleApp.scala */ import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object SimpleApp { def main (args: Array[String]) val logFile = \"file:///usr/local/spark/README.md\" val conf = new SparkConf().setAppName(\"Simple Application\") val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line => line.contains(\"a\")).count() val numBs = logData.filter(line => line.contains(\"b\")).count() println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs)) } 使用sbt打包scala程序，在sparkapp目录下新建simple.sbt： name := \"Simple Project\" version := \"1.0\" scalaVersion := \"2.11.8\" libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.3.2\" 打包成JAR包： sbt package 运行 将生成的jar包通过spark-submit提交到Spark中运行： /usr/local/spark/bin/spark-submit --class \"SimpleApp\" ~/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar 2>&1 | grep \"Lines with a:\" Java实现 maven 安装maven，解压安装到/usr/local/maven目录下，并给予hadoop用户该目录的权限： sudo unzip apache-maven-3.3.9-bin.zip -d /usr/local mv apache-maven-3.3.9 maven 将usr/local/maven/bin加入PATH：./bashrc并source。 mkdir -p ./sparkapp2/src/main/java 编程 在/home/hadoop目录下创建sparkapp2文件夹，作为应用程序的根目录，并创建子目录mkdir –p ./sparkapp2/src/main/java vim ./sparkapp2/src/main/java/SimpleApp.java 在该子目录下新建SimpleApp.java文件 /*** SimpleApp.java ***/ import org.apache.spark.api.java.*; import org.apache.spark.api.java.function.Function; public class SimpleApp { public static void main(String[] args) { String logFile = \"file:///usr/local/spark/README.md\"; // Should be some file on your system JavaSparkContext sc = new JavaSparkContext(\"local\", \"Simple App\", \"file:///usr/local/spark/\", new String[]{\"target/simple-project-1.0.jar\"}); JavaRDD logData = sc.textFile(logFile).cache(); long numAs = logData.filter(new Function() { public Boolean call(String s) { return s.contains(\"a\"); } }).count(); long numBs = logData.filter(new Function() { public Boolean call(String s) { return s.contains(\"b\"); } }).count(); System.out.println(\"Lines with a: \" + numAs + \", lines with b: \" + numBs); } } 在sparkapp2目录下新建pom.xml： edu.berkeley simple-project 4.0.0 Simple Project jar 1.0 alimaven http://maven.aliyun.com/nexus/content/groups/public/ org.apache.spark spark-core_2.11 2.3.2 查看项目结构： 在sparkapp2目录下build： 成功build： 运行程序 spark-submit --class \"SimpleApp\" ~/sparkapp2/target/simple-project-1.0.jar 2>&1 | grep \"Lines with a\" 运行成功 集群上运行 在集群上运行，只需要修改IP地址即可， 对于java程序，修改为： /*** SimpleApp.java ***/ import org.apache.spark.api.java.*; import org.apache.spark.api.java.function.Function; public class SimpleApp { public static void main(String[] args) { String logFile = \"hdfs://hadoop1:9000/README.md\"; // Should be some file on your system JavaSparkContext sc = new JavaSparkContext(\"spark://hadoop1:7077\", \"Simple App\", \"file:///usr/local/spark/\", new String[]{\"simple-project-1.0.jar\"}); JavaRDD logData = sc.textFile(logFile).cache(); long numAs = logData.filter(new Function() { public Boolean call(String s) { return s.contains(\"a\"); } }).count(); long numBs = logData.filter(new Function() { public Boolean call(String s) { return s.contains(\"b\"); } }).count(); System.out.println(\"Lines with a: \" + numAs + \", lines with b: \" + numBs); } } 对于Scala程序，修改为： /* SimpleApp.scala */ import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.apache.spark.SparkConf object SimpleApp { def main (args: Array[String]){ val logFile = \"hdfs://10.11.6.91:9000/README.md\" val conf = new SparkConf().setAppName(\"Simple Application\") val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line => line.contains(\"a\")).count() val numBs = logData.filter(line => line.contains(\"b\")).count() println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs)) } } 运行程序 Java spark-submit --class \"SimpleApp\" simple-project-1.0.jar 2>&1 | grep \"Lines with a\" Scala spark-submit --class \"SimpleApp\" simple-project_2.11-1.0.jar 2>&1 | grep \"Lines with a\" "},"project/spark2.html":{"url":"project/spark2.html","title":"spark编程练习","keywords":"","body":"Map 班级学生成绩的随机生成 输入：本班同学的学号 输出： 数据准备 首先需要一个stuID.csv文件，每一列为一个学号： 然后将文件放入HDFS中： hdfs dfs put stuID.csv input 编写程序 import org.apache.spark.SparkConf; import org.apache.spark.api.java.*; import org.apache.spark.api.java.function.Function; import java.util.ArrayList; import java.util.List; import java.util.Random; public class StuScore { private static Random rand = new Random(); public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(\"StuScore\"); JavaSparkContext sc = new JavaSparkContext(conf); String logFile = \"hdfs:///user/hadoop/input/stuID.csv\"; JavaRDD stuID = sc.textFile(logFile); JavaRDD stuScore = stuID.map( new Function() { @Override public String call(String s) throws Exception { String tmp = s + \" \" + String.valueOf( rand.nextInt(100) +1); return tmp; } } ); stuScore.saveAsTextFile(\"hdfs:///user/hadoop/output/spark/StuRandomScore\"); } } 注意点 注意在用Spark连接HDFS时，需要指明完整路径，如/user/hadoop，不能像hadoop一样省略。 使用map函数将其transform到另一个RDD是常用的操作方式。 运行 本地运行 首先确保已经打开了HDFS，在target目录下，提交作业： spark-submit --class StuScore StuScore-1.0.jar 查看结果 hdfs dfs -cat output/spark/StuRandomScore/* 集群上运行 同样，需要将HDFS地址更改为集群的地址，例如： hdfs:///user/hadoop/input/stuID.csv => hdfs://10.11.6.91:9000/user/hadoop7/input/stuID.csv 提交作业并运行： spark-submit --class StuScore StuScore-1.0.jar 查看运行结果 hdfs dfs -cat output/spark/StuRandomScore/* reduceByKey 问题 求平均成绩：将全班同学每隔5号分为一组，求每组的平均成绩 输入： 输出： 数据准备 首先需要一个score.csv文件，每一列为学号和学生成绩： 然后将文件放入HDFS中： hdfs dfs put score.csv input 编写程序 import org.apache.spark.SparkConf; import org.apache.spark.api.java.*; import org.apache.spark.api.java.function.Function; import org.apache.spark.api.java.function.Function2; import org.apache.spark.api.java.function.PairFunction; import scala.Tuple2; import java.util.Iterator; import java.util.Random; public class AVGScore { private static Integer groupSize = 5; public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(\"AVGScore\"); JavaSparkContext sc = new JavaSparkContext(conf); sc.setLogLevel(\"WARN\"); //http://stackoverflow.com/questions/27781187/how-to-stop-messages-displaying-on-spark-console String logFile = \"hdfs:///user/hadoop/input/score.csv\"; JavaRDD fileRDD = sc.textFile(logFile); /** * map string to (id, score) and convert to (group_id, (score,1)) * reduceByKey => (group_id,(sumScore, count) * and then mapValues to avg score */ JavaPairRDD stuScore = fileRDD.mapToPair( line -> new Tuple2<>( (Integer.parseInt(line.split(\",\")[0]) + 1 )/5, new Tuple2<>(Double.parseDouble(line.split(\",\")[1]),1))) .reduceByKey( (x,y) -> new Tuple2<>(x._1 + y._1,x._2+y._2)) .mapValues(x -> x._1/x._2); stuScore.saveAsTextFile(\"hdfs:///user/hadoop/output/spark/AVGscore\"); } } //参考 : https://blog.csdn.net/gx304419380/article/details/79455833 注意点 求平均值有很多种方法，这里选择的是reduceByKey，效率较高，更多方法参考这里。 多使用Lambda函数，简单直观。 运行 本地运行 运行程序 spark-submit --class AVGScore AVGScore-1.0.jar 查看结果，同时查看行号： hdfs dfs -cat output/spark/AVGscore/* | wc -l 集群上运行 需要将HDFS地址更改为集群的地址，例如： hdfs:///user/hadoop/input/score.csv => hdfs://10.11.6.91:9000/user/hadoop7/input/score.csv 提交作业并运行： spark-submit --master spark://10.11.6.91:7077 --class AVGScore AVGScore-1.0.jar 查看结果 hdfs dfs -cat output/spark/AVGscore/* Natural join 数据准备 有两个文件 person.txt 1 Aaron 210000 2 Abbott 214000 3 Abel 221000 4 Abner 215000 5 Abraham 226000 6 Adair 225300 7 Adam 223800 8 Addison 224000 9 Adolph 223001 address.txt 210000 Nanjing 214000 Wuxi 221000 Xuzhou 213000 Changzhou 要求以code为连接属性，匹配出person中每个人所在的位置信息；每条记录各个字段之间以空格为分隔符。 编写程序 import org.apache.commons.lang.StringUtils; import org.apache.hadoop.hdfs.protocol.DirectoryListing; import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.function.Function; import org.apache.spark.api.java.function.PairFunction; import scala.Tuple2; public class NaturalJoin { public static void main(String args[]) { SparkConf conf = new SparkConf().setAppName(\"NaturalJoin\"); JavaSparkContext sc = new JavaSparkContext(conf); sc.setLogLevel(\"WARN\"); //http://stackoverflow.com/questions/27781187/how-to-stop-messages-displaying-on-spark-console String addFile = \"hdfs:///user/hadoop/input/address.txt\"; String personFile = \"hdfs:///user/hadoop/input/person.txt\"; /*** * return code,city */ JavaPairRDD addRDD = sc.textFile(addFile).mapToPair( line -> new Tuple2<>( Integer.parseInt(line.split(\" \")[0]), line.split(\" \")[1])); /** * return return code,{ id + name } */ JavaPairRDD personRDD = sc.textFile(personFile).mapToPair( new PairFunction() { @Override public Tuple2 call(String s) throws Exception { String[] splitLines = StringUtils.split(s, \" \"); if (splitLines.length (Integer.parseInt(splitLines[2]), splitLines[0] + splitLines[1]); } } ); /** * return code, [{id + name}, city] */ JavaPairRDD> resultRDD = personRDD.join(addRDD); resultRDD.saveAsTextFile(\"hdfs:///user/hadoop/output/spark/NaturalJoin\"); } } 注意点 本操作比较繁琐，需要先生成JavaPairRDD，再使用Join，未来可以尝试使用DataFrame执行。 运行 本地运行 运行程序并查看结果 spark-submit --class NaturalJoin NaturalJoin-1.0.jar hdfs dfs -cat output/spark/NaturalJoin/* 集群上运行 需要将HDFS地址更改为集群的地址，例如： hdfs:///user/hadoop/input/address.txt => hdfs://10.11.6.91:9000/user/hadoop7/input/address.txt 提交作业并运行： spark-submit --master spark://10.11.6.91:7077 --class NaturalJoin NaturalJoin-1.0.jar 查看结果 hdfs dfs -cat output/spark/NaturalJoin/* Kmeans 数据准备 输入数据（k-means.dat）： 4,400 96,826 606,776 474,866 400,768 2,920 356,766 36,687 -26,824 第一行标明K的值和数据个数N, 均为整形, 由\",\"隔开 (如 3,10 表示K=3, N=10)。 之后N行中每行代表一个二维向量, 向量元素均为整形, 由\",\"隔开 (如 1,2 表示向量(1, 2))。 输出: K行, 每行是一个聚类图心的二维向量, 向量元素均为浮点型 (如 1.1,2.3)。 编写程序 Kmeans.java import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.function.Function2; import scala.Serializable; import scala.Tuple2; import java.util.ArrayList; import java.util.List; public class Kmeans implements KmeansInterface, Serializable { // every point has a cluster number and point(x,y) private List> oldCenterList = new ArrayList<>(); private List> newCenterList = new ArrayList<>(); private double threshold = 0.000001; /** * @param point * @return cluster belonged * @Method get the closest cluster for the point */ public int findClosest(Point point) { int argmin = -1; double minimalDistance = Double.MAX_VALUE; for (Tuple2 i : oldCenterList) { double distance = point.EuclideanDis(i._2); if (distance outCenterList = new ArrayList<>(); // format center points for (Tuple2 tmp : newCenterList) { outCenterList.add(String.valueOf(tmp._2.getX()) + \" \" + String.valueOf(tmp._2.getY())); } JavaRDD center = sc.parallelize(outCenterList); center.saveAsTextFile(outFile); } /** * @return False for not stable * @Method compare two cluster center with threshold */ public boolean clusterCompare() { for (Tuple2 oldCenter : oldCenterList) { int clusterNum = oldCenter._1; for (Tuple2 newCenter : newCenterList) { if (newCenter._1 == clusterNum) { double dis = oldCenter._2.EuclideanDis(newCenter._2); if (dis > threshold) return false; break; } } } return true; } /** * @param kmeansRDD * @return init pointsRDD * @Method prepare Points RDD and select clusters randomly */ public JavaPairRDD Prepare(JavaRDD kmeansRDD) { // get the number of cluster String fisrtLine = kmeansRDD.first(); int clusterCount = Integer.parseInt(fisrtLine.split(\",\")[0]); // filter first line and convert to , init set all cluster number 1 JavaPairRDD pointsRDD = kmeansRDD.filter(line -> !line.equals(fisrtLine)).mapToPair( line -> { String[] splitLine = line.split(\",\"); double X = Double.parseDouble(splitLine[0]); double Y = Double.parseDouble(splitLine[1]); return new Tuple2<>(0, new Point(X, Y)); } ); // init center list oldCenterList.addAll(pointsRDD.take(clusterCount)); for (int i = 0; i tmp = oldCenterList.get(i); oldCenterList.set(i, new Tuple2<>(i, tmp._2)); } newCenterList.addAll(oldCenterList); return pointsRDD; } /** * @param pointsRDD to cluster * @return new classify PointsRDD * @method cluster and update new cluster center */ public JavaPairRDD cluster(JavaPairRDD pointsRDD) { JavaPairRDD newPointsRDD = pointsRDD.mapToPair( kv -> new Tuple2<>(findClosest(kv._2), kv._2) ); JavaPairRDD newClusterRDD = newPointsRDD .mapValues( value -> new Tuple2<>(value, 1)) .reduceByKey( new Function2, Tuple2, Tuple2>() { @Override public Tuple2 call(Tuple2 value1, Tuple2 value2) throws Exception { Point tmp = new Point(value1._1.getX() + value2._1.getX(), value1._1.getY() + value2._1.getY()); int count = value1._2 + value2._2; return new Tuple2<>(tmp, count); } } ).mapValues( v -> new Point(v._1.getX() / v._2, v._1.getY() / v._2) ); oldCenterList.clear(); oldCenterList.addAll(newCenterList); // convert to list to store newCenterList.clear(); newCenterList.addAll(newClusterRDD.collect()); return newPointsRDD; } } kmeansRun.java import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import scala.Tuple2; import java.util.ArrayList; import java.util.List; public class kmeansRun { public static void main(String args[]) { Kmeans kmeans = new Kmeans(); SparkConf conf = new SparkConf().setAppName(\"Kmeans\"); JavaSparkContext sc = new JavaSparkContext(conf); String kmeansFile = \"hdfs://10.11.6.91:9000/user/hadoop7/input/k-means.dat\"; String outFile = \"hdfs://10.11.6.91:9000/user/hadoop7/output/spark/kmeans\"; int counter = 1, maxIteration = 500; boolean result = false; JavaRDD kmeansRDD = sc.textFile(kmeansFile).cache(); // init centerList and Points RDD JavaPairRDD PointsRDD = kmeans.Prepare(kmeansRDD); while (!result && counter > \" + counter + \" || means stable: \" + result); counter++; } kmeans.saveToFile(outFile,sc); } } 注意事项 将类成员函数的声明和定义分开写，用interface来定义抽象函数。 在循环过程中，虽然变量名没有改变，但只是指向那一个RDD，而每次循环RDD都在变化。 运行 本地运行 运行程序 spark-submit --class kmeansRun Kmeans-1.0.jar 2>&1 | grep \"KMEANS finished iteration:\" 查看结果 hdfs dfs -cat output/spark/kmeans/* 集群上运行 需要将HDFS地址更改为集群的地址，例如： hdfs:///user/hadoop/input/k-means.dat => hdfs://10.11.6.91:9000/user/hadoop7/input/k-means.dat 提交作业并运行： spark-submit --master spark://10.11.6.91:7077 --class kmeansRun Kmeans-1.0.jar 2>&1 | grep \"KMEANS finished iteration:\" 查看结果： "},"project/spark-recap.html":{"url":"project/spark-recap.html","title":"Spark 编程总结","keywords":"","body":"Transformation 常用函数 map(function) 将每个元素传递到function中，并返回一个新的数据集 flatmap 在flatMap中，我们会传入一个函数，该函数对每个输入都会返回一个集合List（而不是一个元素），然后，flatMap把生成的多个集合“拍扁”成为一个集合。 mapToPair() 返回一个tuple2类型的KeyValue对，类型为JavaPairRDD mapValue 对value进行函数映射 reduceByKey 按照key对value进行操作 join(RDD) 按照key进行join，join之后的RDD的value为tuple2类型 filter() 删选出满足函数的元素，并返回一个新数据集 如，筛选出不等于第一行的所有元素： filter(line -> !line.equals(fisrtLine)) 求平均值 常见算子之一，一般先将RDD map为（key，1）的形式，再使用ReduceByKey合并，效率较高，更多方法参考这里。 Action count() 返回RDD中元素个数 collect() 以数组（arrayList）的形式返回数据集中的所有元素，开销很大 first() 返回数据集中的第一个元素 take(n) 以数组（ArrayList）的形式返回数据集的前n个元素 reduce() reduce((a,b)->a+b)，根据两个参数返回一个值，聚合数据集中的元素 "},"project/docker1.html":{"url":"project/docker1.html","title":"使用 Docker 配置 hadoop/spark","keywords":"","body":"+++ title = \"使用 Docker 配置 hadoop/spark\" slug = \"docker1\" tags = [\"distributed system\",\"project\"] date = \"2018-11-03T13:12:38+08:00\" description = \"\" +++ 分别安装hadoop和spark镜像 安装hadoop镜像 选择的docker镜像地址，这个镜像提供的hadoop版本比较新，且安装的是jdk8，可以支持安装最新版本的spark。 docker pull uhopper/hadoop:2.8.1 安装spark镜像 如果对spark版本要求不是很高，可以直接拉取别人的镜像，若要求新版本，则需要对dockerfile进行配置。 环境准备 下载sequenceiq/spark镜像构建源码 git clone https://github.com/sequenceiq/docker-spark 从Spark官网下载Spark 2.3.2安装包 下载地址：http://spark.apache.org/downloads.html 将下载的文件需要放到docker-spark目录下 查看本地image，确保已经安装了hadoop 进入docker-spark目录，确认所有用于镜像构建的文件已经准备好 修改配置文件 修改Dockerfile为以下内容 FROM sequenceiq/hadoop-docker:2.7.0 MAINTAINER scottdyt #support for Hadoop 2.7.0 #RUN curl -s http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz | tar -xz -C /usr/local/ ADD spark-2.3.2-bin-hadoop2.7.tgz /usr/local/ RUN cd /usr/local && ln -s spark-2.3.2-bin-hadoop2.7 spark ENV SPARK_HOME /usr/local/spark RUN mkdir {% math_inline %}SPARK_HOME/yarn-remote-client ADD yarn-remote-client {% endmath_inline %}SPARK_HOME/yarn-remote-client RUN {% math_inline %}BOOTSTRAP && {% endmath_inline %}HADOOP_PREFIX/bin/hadoop dfsadmin -safemode leave && {% math_inline %}HADOOP_PREFIX/bin/hdfs dfs -put {% endmath_inline %}SPARK_HOME-2.3.2-bin-hadoop2.7/jars /spark && {% math_inline %}HADOOP_PREFIX/bin/hdfs dfs -put {% endmath_inline %}SPARK_HOME-2.3.2-bin-hadoop2.7/examples/jars /spark ENV YARN_CONF_DIR {% math_inline %}HADOOP_PREFIX/etc/hadoop ENV PATH {% endmath_inline %}PATH:{% math_inline %}SPARK_HOME/bin:{% endmath_inline %}HADOOP_PREFIX/bin # update boot script COPY bootstrap.sh /etc/bootstrap.sh RUN chown root.root /etc/bootstrap.sh RUN chmod 700 /etc/bootstrap.sh #install R RUN rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm RUN yum -y install R ENTRYPOINT [\"/etc/bootstrap.sh\"] 修改bootstrap.sh为以下内容 #!/bin/bash : {% math_inline %}{HADOOP_PREFIX:=/usr/local/hadoop} {% endmath_inline %}HADOOP_PREFIX/etc/hadoop/hadoop-env.sh rm /tmp/*.pid # installing libraries if any - (resource urls added comma separated to the ACP system variable) cd {% math_inline %}HADOOP_PREFIX/share/hadoop/common ; for cp in {% endmath_inline %}{ACP//,/ }; do echo == {% math_inline %}cp; curl -LO {% endmath_inline %}cp ; done; cd - # altering the core-site configuration sed s/HOSTNAME/{% math_inline %}HOSTNAME/ /usr/local/hadoop/etc/hadoop/core-site.xml.template > /usr/local/hadoop/etc/hadoop/core-site.xml # setting spark defaults echo spark.yarn.jar hdfs:///spark/* > {% endmath_inline %}SPARK_HOME/conf/spark-defaults.conf cp {% math_inline %}SPARK_HOME/conf/metrics.properties.template {% endmath_inline %}SPARK_HOME/conf/metrics.properties service sshd start {% math_inline %}HADOOP_PREFIX/sbin/start-dfs.sh {% endmath_inline %}HADOOP_PREFIX/sbin/start-yarn.sh CMD={% math_inline %}{1:-\"exit 0\"} if [[ \"{% endmath_inline %}CMD\" == \"-d\" ]]; then service sshd stop /usr/sbin/sshd -D -d else /bin/bash -c \"$*\" fi 构建镜像 docker build --rm -t scottdyt/spark:2.3.2 . 查看镜像 启动一个spark2.3.1容器 docker run -it -p 8088:8088 -p 8042:8042 -p 4040:4040 -h sandbox scottdyt/spark:2.3.2 bash 启动成功： 安装spark-hadoop镜像 如果想偷懒一点，直接安装装好spark和hadoop的镜像，镜像地址在这里。 或者直接在终端输入： docker pull uhopper/hadoop-spark:2.1.2_2.8.1 安装完成： 参考 https://blog.csdn.net/farawayzheng_necas/article/details/54341036 "},"project/docker2.html":{"url":"project/docker2.html","title":"使用 docker 搭建 spark(2.3.1) 集群","keywords":"","body":"创建Spark集群 首先在命令行下载该项目： git clone https://github.com/gettyimages/docker-spark.git 在该目录下，输入compose up: 等待安装，最后会提示Worker和master都准备好了： 在浏览器中输入localhost:8080，出现如下界面，说明配置成功： 我们可以使用docker ps -a命令查看当前运行的容器： 集群使用与作业提交 集群使用 首先进入master的容器： docker exec -it docker-spark_master_1 /bin/bash 注意，使用exec命令进入容器时，在命令行输入exit不会退出容器，使用attach命令再次进入 查看Spark和Hadoop的版本： hadoop version spark shell 使用:quit退出spark-shell。 同样也可以查看python的版本，为3.5.3，已经很新了。 作业提交 在配置docker compose时，我们已经将本地文件./data挂载到容器中的/tmp/data下，因此，我们可以先在本地文件中放入需要文件，这里我放入了kmeans的文件： 在master节点中查看该文件： 这已经映射到了hdfs上，我们可以使用hdfs dfs -cat命令查看： 这样，我们就可以使用spark-submit运行我们的程序 在浏览器中查看运行的程序： "},"project/zookeeper.html":{"url":"project/zookeeper.html","title":"ZooKeeper配置及简单使用","keywords":"","body":"Zookeeper配置 下载zookeeper 首先在官网下载zookeeper： wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz 解压： sudo tar xzvf zookeeper-3.4.13.tar.gz -C /usr/local 设置权限： chown -R hadoop:hadoop zookeeper 配置环境变量 export PATH=$PATH:/usr/local/zookeeper/bin 修改配置文件 重命名： sudo mv zookeeper-3.4.13/ zookeeper mv zoo_sample.cfg zoo.cfg 修改zoo.cfg： 可以改conf文件，创建多个端口-> 多个server： 启动zookeeper zkServer.sh start 验证是否启动 telnet localhost 2181 输入start，若看到Zookeeper version则说明启动成功 输入jps看是否启动： 关闭zookeeper zkServer.sh stop Zookeeper命令行 参考这里。 进入命令行工具： zkCli.sh -server 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容： 下面我们通过 set 命令来对 zk 所关联的字符串进行设置： 下面我们将刚才创建的 znode 删除： delete /zk 删除节点： rmr /zk 编写Client程序 创建maven项目 修改pox.xml文件： 参考官方配置 修改为： 4.0.0 Dase 1 1.0-SNAPSHOT org.apache.zookeeper zookeeper 3.4.13 junit junit 4.12 test 下载JUnitGenerator V2.0 在IDEA中，JUnit已经默认下载好，我们需要添加JUnitGenerator V2.0插件： 编写程序 import java.util.List; import org.apache.zookeeper.*; import org.apache.zookeeper.ZooDefs.Ids; import org.apache.zookeeper.data.Stat; public class Simple { private static final String connectString = \"localhost:2181\"; private static final int sessionTimeout = 2000; private ZooKeeper zkClient = null; public void init() throws Exception { zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） System.out.println(event.getType() + \"---\" + event.getPath()); try { zkClient.getChildren(\"/\", true); } catch (Exception e) { } } }); } /** * 数据的增删改查 * * @throws InterruptedException * @throws KeeperException */ // 创建数据节点到zk中 public void Create() throws KeeperException, InterruptedException { // 参数1：要创建的节点的路径 参数2：节点大数据 参数3：节点的权限 参数4：节点的类型 String nodeCreated = zkClient.create(\"/scott\", \"hellozk\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); //上传的数据可以是任何类型，但都要转成byte[] } //判断znode是否存在 public void Exist() throws Exception{ Stat stat = zkClient.exists(\"/scott\", false); System.out.println(stat==null?\"not exist\":\"exist\"); } // 获取子节点 public void getChildren() throws Exception { List children = zkClient.getChildren(\"/\", true); for (String child : children) { System.out.println(child); } Thread.sleep(Long.MAX_VALUE); } //获取znode的数据 public void getData() throws Exception { byte[] data = zkClient.getData(\"/scott\", false, null); System.out.println(new String(data)); } //删除znode public void deleteZnode() throws Exception { //参数2：指定要删除的版本，-1表示删除所有版本 zkClient.delete(\"/eclipse\", -1); } //删除znode public void setData() throws Exception { zkClient.setData(\"/scott\", \"imissyou angelababy\".getBytes(), -1); byte[] data = zkClient.getData(\"/scott\", false, null); System.out.println(new String(data)); } } Junit 使用Junit进行单元测试，这里首先需要创建测试类： 使用快捷键alt+insert： 会自动在test中创建同名文件： 修改该文件： package test; import org.apache.zookeeper.*; import org.apache.zookeeper.data.Stat; import org.junit.Test; import org.junit.Before; import org.junit.After; import java.util.List; /** * Simple Tester. * * @author * @since Nov 11, 2018 * @version 1.0 */ public class SimpleTest { private static final String connectString = \"localhost:2181\"; private static final int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void testInit() throws Exception { zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） System.out.println(event.getType() + \"---\" + event.getPath()); try { zkClient.getChildren(\"/\", true); } catch (Exception e) { } } }); String nodeCreated = zkClient.create(\"/scott\", \"hellozk\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); } /** * * Method: Exist() * */ @Test public void testExist() throws Exception { Stat stat = zkClient.exists(\"/scott\", false); System.out.println(stat==null?\"not exist\":\"exist\"); } /** * * Method: getChildren() * */ @Test public void testGetChildren() throws Exception { List children = zkClient.getChildren(\"/\", true); for (String child : children) { System.out.println(child); } // Thread.sleep(Long.MAX_VALUE); } /** * * Method: getData() * */ @Test public void testGetData() throws Exception { byte[] data = zkClient.getData(\"/scott\", false, null); System.out.println(new String(data)); } /** * * Method: deleteZnode() * */ @Test public void testDeleteZnode() throws Exception { zkClient.delete(\"/scott\", -1); } /** * * Method: setData() * */ @Test public void testSetData() throws Exception { zkClient.setData(\"/scott\", \"imissyou angelababy\".getBytes(), -1); byte[] data = zkClient.getData(\"/scott\", false, null); System.out.println(new String(data)); } } 其中，@Before表示在测试前运行，注意，JUnit不能指定@test运行顺序，如果非要指定，需要对函数名进行重新命名，具体参考这里 测试 对每个函数进行测试： 如：对Exist函数进行测试： 直到每个函数通过为止 体会分布式共享锁的实现 编写程序 在src/main/java下创建DistributedClientLock： import java.util.Collections; import java.util.List; import java.util.Random; import org.apache.zookeeper.CreateMode; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.Watcher.Event.EventType; import org.apache.zookeeper.ZooDefs.Ids; import org.apache.zookeeper.ZooKeeper; public class DistributedClientLock { // 会话超时 private static final int SESSION_TIMEOUT = 2000; // zookeeper集群地址 private String hosts = \"localhost:2181\"; private String groupNode = \"locks\"; private String subNode = \"sub\"; private boolean haveLock = false; private ZooKeeper zk; // 记录自己创建的子节点路径 private volatile String thisPath; /** * 连接zookeeper */ private void connectZookeeper() throws Exception { zk = new ZooKeeper(hosts, SESSION_TIMEOUT, new Watcher() { public void process(WatchedEvent event) { try { // 判断事件类型，此处只处理子节点变化事件 if (event.getType() == EventType.NodeChildrenChanged && event.getPath().equals(\"/\" + groupNode)) { //获取子节点，并对父节点进行监听 List childrenNodes = zk.getChildren(\"/\" + groupNode, true); String thisNode = thisPath.substring((\"/\" + groupNode + \"/\").length()); // 去比较是否自己是最小id Collections.sort(childrenNodes); if (childrenNodes.indexOf(thisNode) == 0) { //访问共享资源处理业务，并且在处理完成之后删除锁 doSomething(); //重新注册一把新的锁 thisPath = zk.create(\"/\" + groupNode + \"/\" + subNode, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); } } } catch (Exception e) { e.printStackTrace(); } } }); // 1、程序一进来就先注册一把锁到zk上 thisPath = zk.create(\"/\" + groupNode + \"/\" + subNode, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // wait一小会，便于观察 Thread.sleep(new Random().nextInt(1000)); // 从zk的锁父目录下，获取所有子节点，并且注册对父节点的监听 List childrenNodes = zk.getChildren(\"/\" + groupNode, true); //如果争抢资源的程序就只有自己，则可以直接去访问共享资源 if (childrenNodes.size() == 1) { doSomething(); thisPath = zk.create(\"/\" + groupNode + \"/\" + subNode, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); } } /** * 处理业务逻辑，并且在最后释放锁 */ private void doSomething() throws Exception { try { System.out.println(\"gain lock: \" + thisPath); Thread.sleep(2000); // do something } finally { System.out.println(\"finished: \" + thisPath); //释放锁 zk.delete(this.thisPath, -1); } } public static void main(String[] args) throws Exception { DistributedClientLock dl = new DistributedClientLock(); dl.connectZookeeper(); Thread.sleep(Long.MAX_VALUE); } } 运行 首先需要在命令行中创建Znode： create /locks locks 然后运行程序，程序会拿到锁、释放锁不停交替运行： 参考 JUnit 测试顺序 Zookeeper简单使用 maven设置 "},"project/yarn.html":{"url":"project/yarn.html","title":"Yarn框架下的系统部署","keywords":"","body":"使用Yarn本地部署Hadoop 修改配置文件 首先修改mapred-sit.xml文件： cd /usr/local/hadoop/etc/hadoop mv mapred-site.xml.template mapred-site.xml 修改为： mapreduce.framework.name yarn 修改配置文件yarn-site.xml： yarn.nodemanager.aux-services mapreduce_shuffle 重新启动 start-dfs.sh 和 start-yarn.sh命令启动hadoop和yarn： 开启历史服务器，才能在Web中查看任务运行情况： mr-jobhistory-daemon.sh start historyserver 再查看进程，多出NodeManager和ResourceManager两个进程： 如果以后在伪分布式模式下启动Hadoop的时候不想启动Yarn，务必把配置文件mapred-site.xml重命名为mapred-site.xml.template 使用Yarn本地部署Spark 修改配置文件 cd /usr/local/spark/conf vim spark-env.sh 添加： export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop 这样Spark就可以跑在YARN上了，也没必要启动spark的master和slaves服务，因为是靠yarn进行任务调度，所以直接提交任务即可： spark-shell --master yarn-client 提示ERROR，这是由于JAVA8的问题，具体参考这里。 首先需要关闭hadoop和yarn： stop-dfs.sh stop-yarn.sh 在/usr/local/hadoop/etc/hadoop/目录下修改yarn-site.xml文件： yarn.nodemanager.pmem-check-enabled false yarn.nodemanager.vmem-check-enabled false 再次启动，成功： 在shell中运行例子： val textFile = sc.textFile(\"hdfs://localhost:9000/user/hadoop/input/k-means.dat\") textFile.first() 成功！ 使用yarn查看作业 在浏览器中查看 在浏览器中输入lcoalhost:8088，在yarn中查看任务： 会发现spark的任务会提交到yarn，而无需启动master和slave节点。 使用hadoop提交 cd /usr/local/hadoop hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output/yarn ‘dfs[a-z.]+’ 作业运行成功，在yarn中查看任务： 但是，无论该任务是否成功，在yarn中查看，只能知道这个container是否运行完，因此，需要通过logs进行查看任务的具体运行情况。 使用spark提交 只需要在master节点中添加master即可，例如： spark-submit --class StuScore --master yarn StuScore-1.0.jar 注意：如果是自己编写的程序，不要在程序中将sparkcontext指定为local 查看yarn logs 在yarn-env.sh中查看log的目录： 默认目录在/usr/local/hadoop/logs里面： 可以发现application的log都存放在里面，现在可以根据web上展示的applicationID选择需要查看的log 关于yarn的log，可参考这里。 "},"project/storm-installation.html":{"url":"project/storm-installation.html","title":"Storm部署与运行","keywords":"","body":"环境配置 Strom 下载 首先从官网下载Strom压缩包，这里以最新的Strom1.2.2作为演示。 解压到/usr/local： sudo tar xzvf apache-storm-1.2.2.tar.gz -C /usr/local 查看解压文件： 需要重命名： sudo mv /usr/local/apache-storm-1.2.2/ /usr/local/storm 修改拥有者： sudo chown -R hadoop:hadoop storm 添加到环境变量 vim ~/.bashrc export STROM=/usr/local/storm source ~/.bashrc 修改配置文件 进入软件目录，修改文件storm.yaml cd /usr/local/storm/ sudo vim conf/storm.yaml 修改为本地server： storm.zookeeper.servers: - \"localhost\" nimbus.seeds: [\"localhost\"] supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 storm.zookeeper.port: 2181 storm.local.dir: \"/usr/local/storm/storm-local\" storm.health.check.dir: \"healthchecks\" storm.health.check.timeout.ms: 5000 Python 检查是否有高于2.6版本的python： 启动storm 修改执行脚本 参考这里修改脚本，主要是修改python的路径 先查找自己的python路径： ls /usr/bin/python* 找到执行脚本，在./bin/storm中，根据之前的python路径修改 启动 执行命令： nohup storm nimbus >/dev/null 2>&1 & nohup storm supervisor >/dev/null 2>&1 & nohup storm ui >/dev/null 2>&1 & 这里的2>&1表示就是把所有标准输出（&1）和标准出错（2）都扔到垃圾桶里面，最后的&表示后台执行 这里可能需要几分钟才能完成启动，使用jps命令查看可以看到nimbus, supervisor, core： 在浏览器中localhost:8080 上查看集群情况： 成功！ 测试运行 示例 /usr/local/storm/example目录下的storm-starter里有很多Storm的项目，比如DRPC、Word Count。 在该目录路径下使用maven来打包Stormjar包 首先找到该目录： 我们发现该目录是用maven进行打包的： 因此，我们可以使用IDEA来打包，也可以直接使用mvn： mvn clean install -Dmaven.test.skip=true mvn package Stream Join的简单实例 WordCount SentenceSpout.java 该文件为模拟外部输入 import java.util.Map; import java.util.UUID; import org.apache.storm.spout.SpoutOutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichSpout; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Values; import org.apache.storm.utils.Utils; public class SentenceSpout extends BaseRichSpout { private SpoutOutputCollector spoutOutputCollector; private String[] sentences = {\"the cow jumped over the moon\", \"an apple a day keeps the doctor away\", \"four score and seven years ago\", \"snow white and the seven dwarfs\", \"i am at two with nature\"}; public void open(Map map, TopologyContext topologycontext, SpoutOutputCollector spoutoutputcollector) { this.spoutOutputCollector = spoutoutputcollector; } public void nextTuple() { for (String sentence : sentences) { Values values = new Values(sentence); UUID msgId = UUID.randomUUID(); this.spoutOutputCollector.emit(values, msgId); } Utils.sleep(1000); } public void declareOutputFields(OutputFieldsDeclarer outputfieldsdeclarer) { outputfieldsdeclarer.declare(new Fields(\"sentence\")); } } SplitSentenceBolt.java 分割字符的bolt import java.util.Map; import org.apache.storm.task.OutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichBolt; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Tuple; import org.apache.storm.tuple.Values; public class SplitSentenceBolt extends BaseRichBolt { private OutputCollector outputCollector; public void execute(Tuple tuple) { String sentence = tuple.getStringByField(\"sentence\"); String[] words = sentence.split(\" \"); for (String word : words) { this.outputCollector.emit(new Values(word)); } } public void prepare(Map map, TopologyContext topologycontext, OutputCollector outputcollector) { this.outputCollector = outputcollector; } public void declareOutputFields(OutputFieldsDeclarer outputfieldsdeclarer) { outputfieldsdeclarer.declare(new Fields(\"word\")); } } WordCountBolt.java 统计次数的bolt import java.util.HashMap; import java.util.Map; import org.apache.storm.task.OutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichBolt; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Tuple; import org.apache.storm.tuple.Values; public class WordCountBolt extends BaseRichBolt { private OutputCollector outputCollector; private HashMap counts = null; public void prepare(Map map, TopologyContext topologycontext, OutputCollector outputcollector) { this.outputCollector = outputcollector; this.counts = new HashMap(); } public void execute(Tuple tuple) { String word = tuple.getStringByField(\"word\"); Integer count = counts.get(word); if (count == null) { count = 0; } count++; this.counts.put(word, count); this.outputCollector.emit(new Values(word, count)); this.outputCollector.ack(tuple); System.out.println(word + \": \" + count); } public void declareOutputFields(OutputFieldsDeclarer outputfieldsdeclarer) { outputfieldsdeclarer.declare(new Fields(\"word\", \"count\")); } } WordCountTopology.java 主函数 import org.apache.storm.Config; import org.apache.storm.LocalCluster; import org.apache.storm.topology.TopologyBuilder; import org.apache.storm.tuple.Fields; public class WordCountTopology { public static void main(String[] args) throws Exception { SentenceSpout sentenceSpout = new SentenceSpout(); SplitSentenceBolt splitSentenceBolt = new SplitSentenceBolt(); WordCountBolt wordCountBolt = new WordCountBolt(); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(\"sentenceSpout-1\", sentenceSpout); builder.setBolt(\"splitSentenceBolt-1\", splitSentenceBolt).shuffleGrouping(\"sentenceSpout-1\"); builder.setBolt(\"wordCountBolt-1\", wordCountBolt).fieldsGrouping(\"splitSentenceBolt-1\", new Fields(\"word\")); Config config = new Config(); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"wordCountTopology-1\", config, builder.createTopology()); Thread.sleep(999999999); cluster.shutdown(); } } 运行 使用已经编译好的WordCountjar包，提交任务： storm jar wordcount.jar WordCountTopology wc 这里WordCountTopoloy为主class，wc为任务别名 可以看见，WordCount的统计是不断变化的： Stream Join SimpleJoinBolt.java 这里主要处理join的过程 import org.apache.storm.Config; import org.apache.storm.generated.GlobalStreamId; import org.apache.storm.task.OutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichBolt; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Tuple; import org.apache.storm.utils.TimeCacheMap; import java.util.ArrayList; import java.util.HashMap; import java.util.HashSet; import java.util.List; import java.util.Map; import java.util.Set; public class SimpleJoinBolt extends BaseRichBolt { private OutputCollector _collector; private Fields _outFields; private Fields _idFields; int _numSources; Map _fieldLocations; //在内存中保留近期活跃的对象 //由于bolt在接收两个数据源的流数据时，同一id两个数据流很可能不会再一个时间点同时发出，因此需要对数据流先进行缓存，直到所有 //id相同的数据源都被后被聚合处理，做完聚合处理后再将对应的tuple信息从缓存中删除。在 TimeCacheMap,Map> _pending; //传进的Fields是聚合后将被输出的字段 public SimpleJoinBolt(Fields outFields){ this._outFields=outFields; } public void execute(Tuple tuple) { // TODO Auto-generated method stub //从tuple中获取_idFields字段，如果不存在于等待被处理的队列_pending中，则加入一行 List id=tuple.select(_idFields); GlobalStreamId streamId=new GlobalStreamId(tuple.getSourceComponent(),tuple.getSourceStreamId()); //打印当前处理元组的来源Spout System.out.println(\"元组来源：\"+tuple.getSourceComponent()); //打印当前元组 System.out.println(\"接收的元组：\"+tuple.getFields().get(0)+\" = \"+tuple.getValues().get(0)+\" , \"+tuple.getFields().get(1)+\" = \"+tuple.getValues().get(1)); //如果当前pending中还不存在join key为此id的元组，则将该条记录加入 if(!_pending.containsKey(id)){ _pending.put(id, new HashMap()); } //从_pending队列中获取当前GlobalStreamId对应的HashMap对象 Map parts=_pending.get(id); //如果streamId已经包含其中，则抛出异常，接收到同一个spout中的两条一样id的tuple，否则将该streamId加入parts中 if(parts.containsKey(streamId)){ throw new RuntimeException(\"Received same side of single join twice\"); } parts.put(streamId, tuple); //如果parts中已经包含了聚合数据源的个数，则从_pending队列中移除这条记录 if(parts.size()==_numSources){ _pending.remove(id); List joinResult=new ArrayList(); for(String outField:_outFields){ GlobalStreamId loc=_fieldLocations.get(outField); joinResult.add(parts.get(loc).getValueByField(outField)); } //输出聚合结果 System.out.print(\"两条关系流中id值为\"+id.get(0)+\"的元组均已收到，聚合结果为：\"); for(Object obj:joinResult){ System.out.print(obj+\" \"); } System.out.println(); //多锚定 _collector.emit(new ArrayList(parts.values()),joinResult); for (Tuple part : parts.values()) { _collector.ack(part); } }else{ System.out.println(\"只从一个关系流中收取到id值为\"+id+\"的元组，不可进行join操作\"); } } public void prepare(Map conf, TopologyContext context, OutputCollector collector) { // TODO Auto-generated method stub _fieldLocations = new HashMap(); this._collector=collector; //创建TimeCacheMap对象，设置超时回调接口，用于tuple处理失败时fail消息 int timeout=((Number)conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS)).intValue(); _pending=new TimeCacheMap,Map>(timeout,new ExpireCallback()); //记录数据源的数据个数 _numSources=context.getThisSources().size(); Set idFields=null; //遍历TopologyContext中不同的数据源:genderSpout和ageSpout System.out.println(context.getThisSources().keySet()); for(GlobalStreamId source:context.getThisSources().keySet()){ //得到公共的Fields字段id,保存到_idFields中 Fields fields=context.getComponentOutputFields(source.get_componentId(),source.get_streamId()); //fields:[id,gender],[id,age] Set setFields=new HashSet(fields.toList()); if(idFields==null){ idFields=setFields; }else{ //求交集 idFields.retainAll(setFields); System.out.println(idFields); } //同时将_outFields中字段所在数据源记录下来，保存到一张HashMap _fieldLocations中，以便聚合后获取对应的字段值 for(String outfield:_outFields){ for(String sourcefield:fields){ if(outfield.equals(sourcefield)){ _fieldLocations.put(outfield, source); } } } //打印结果:gender=GlobalStreamId(componentId=gender-spout,streamId=default) //age=GlobalStreamId(componentId=age-spout,streamId=default) System.out.println(_fieldLocations); } _idFields=new Fields(new ArrayList(idFields)); if(_fieldLocations.size()!=_outFields.size()){ throw new RuntimeException(\"Cannot find all outfields among sources\"); } } public void declareOutputFields(OutputFieldsDeclarer declarer) { // TODO Auto-generated method stub declarer.declare(_outFields); } private class ExpireCallback implements TimeCacheMap.ExpiredCallback, Map>{ public void expire(List key, Map tuples) { // TODO Auto-generated method stub for(Tuple tuple:tuples.values()){ _collector.fail(tuple); } } } } SingleJoinExample.java 这里处理两个spout和setBolt过程 import org.apache.storm.Config; import org.apache.storm.LocalCluster; import org.apache.storm.testing.FeederSpout; import org.apache.storm.topology.TopologyBuilder; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Values; import org.apache.storm.utils.Utils; public class SingleJoinExample { public static void main(String[] args) { FeederSpout genderSpout = new FeederSpout(new Fields(\"id\", \"gender\")); FeederSpout ageSpout = new FeederSpout(new Fields(\"id\", \"age\")); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(\"gender\", genderSpout); builder.setSpout(\"age\", ageSpout); builder.setBolt(\"join\", new SimpleJoinBolt(new Fields(\"gender\", \"age\"))) .fieldsGrouping(\"gender\", new Fields(\"id\")) .fieldsGrouping(\"age\", new Fields(\"id\")); Config conf = new Config(); conf.setMaxTaskParallelism(3); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"join-bolt\", conf, builder.createTopology()); for (int i = 0; i = 0; i--) { ageSpout.feed(new Values(i, i + 20)); } Utils.sleep(2000); cluster.shutdown(); } } 运行 首先生成JAR包，然后执行： storm jar ds.jar SingleJoinExample join 运行结果为： 可以看见，当接收到的元祖没有匹配的id时，会等待另一个元祖到来，然后再聚合。 Reference Async-loop-died Storm处理Stream Join的简单实例 "},"project/storm-coding.html":{"url":"project/storm-coding.html","title":"Storm编程练习","keywords":"","body":"+++ title = \"Storm 编程练习\" slug = \"storm coding\" tags = [\"distributed system\",\"project\"] date = \"2018-11-28T19:51:51+08:00\" description = \"\" +++ WordSort 要求 将之前的WordCount改为WordSort排序 思路 同样，使用一个list来记录当前位置，同时使用二分查找找到当前位置并插入 代码实现 SentenceSpout public class SentenceSpout extends BaseRichSpout { private SpoutOutputCollector spoutOutputCollector; private String[] sentences = {\"the cow jumped over the moon\", \"an apple a day keeps the doctor away\", \"four score and seven years ago\", \"snow white and the seven dwarfs\", \"i am at two with nature\"}; public void open(Map map, TopologyContext topologycontext, SpoutOutputCollector spoutoutputcollector) { this.spoutOutputCollector = spoutoutputcollector; } public void nextTuple() { for (String sentence : sentences) { Values values = new Values(sentence); UUID msgId = UUID.randomUUID(); this.spoutOutputCollector.emit(values, msgId); } Utils.sleep(1000); } public void declareOutputFields(OutputFieldsDeclarer outputfieldsdeclarer) { outputfieldsdeclarer.declare(new Fields(\"sentence\")); } } SplitSentenceBolt public class SplitSentenceBolt extends BaseBasicBolt { public void execute(Tuple tuple, BasicOutputCollector collector) { String sentence = tuple.getStringByField(\"sentence\"); String[] words = sentence.split(\" \"); for (String word : words) { collector.emit(new Values(word)); } } public void declareOutputFields(OutputFieldsDeclarer outputfieldsdeclarer) { outputfieldsdeclarer.declare(new Fields(\"word\")); } } WordSortBolt public class WordSortBolt extends BaseBasicBolt { List wordList = new ArrayList(); /** * @param array * @param key * @return */ public int arrayIndexOf(String key) { int min, max, mid; min = 0; max = wordList.size() - 1; while (min > 1; String tmp = wordList.get(mid); if (key.compareTo(tmp) > 0) { min = mid + 1; } else if (key.compareTo(tmp) WordSortTopology public class WordSortTopology { public static void main(String[] args) throws Exception { SentenceSpout sentenceSpout = new SentenceSpout(); SplitSentenceBolt splitSentenceBolt = new SplitSentenceBolt(); WordSortBolt wordSortBolt = new WordSortBolt(); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(\"sentenceSpout-1\", sentenceSpout); builder.setBolt(\"splitSentenceBolt-1\", splitSentenceBolt).shuffleGrouping(\"sentenceSpout-1\"); builder.setBolt(\"wordSortBolt-1\", wordSortBolt).fieldsGrouping(\"splitSentenceBolt-1\", new Fields(\"word\")); Config config = new Config(); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"wordSortTopology-1\", config, builder.createTopology()); Thread.sleep(999999999); cluster.shutdown(); } } 运行 将程序打包成JAR包，并执行 storm jar WordSort.jar WordSortTopology ws 查看结果，不断有新的tuple进来并排序打印： top-k 要求 单词的top-k：求最频繁的k个word 要考虑代码的性能 思路 首先仍然与WordCount一样，需要一个hashmap来存当前word对应的count数量 考虑到性能原因，肯定不能直接对所有word进行排序，因此，只需要维护前k个word即可 需要设置最小count，表示前k个中最小的count数，如果当前有word的count数大于这个数，则将其加入TopK数组，然后剔除最小count的word 代码实现 SentenceSpout、SplitSentenceBolt与之前一样，不再赘述 pair 保存word和count的tuple，方便访问 public class pair { public final String content; public final Integer count; public pair(String content, Integer count) { this.content = content; this.count = count; } public int compareCount(pair other) { return this.count - other.count; } public int compareWord(pair other) { return this.content.compareTo(other.content); } } TopK 使用HashMap来保存当前所有word的count数量，使用TopList保存前k个word public class TopK extends BaseBasicBolt { private HashMap counts; private ArrayList TopList; public int K; public int minCount; TopK(int k) { this.K = k; this.counts = new HashMap<>(); this.TopList = new ArrayList<>(); this.minCount = 0; } public void insertWord(pair word) { int max = TopList.size() - 1; // if the same word, the new.count > old.count for (int i = max; i >= 0; i--) { pair tmp = TopList.get(i); // find the same word,replace older one if (word.compareWord(tmp) == 0) { TopList.set(i, word); return; } if (word.compareCount(tmp) minCount || TopList.size() K) { TopList.remove(TopList.size() - 1); minCount = TopList.get(TopList.size() - 1).count; } for (pair tmpWord : TopList) System.out.println(tmpWord.content + \" \" + tmpWord.count.toString()); collector.emit(new Values(word, count)); } } public void declareOutputFields(OutputFieldsDeclarer outputfieldsdeclarer) { outputfieldsdeclarer.declare(new Fields(\"word\", \"count\")); } } WordTopKTopology public class WordTopKTopology { public static void main(String[] args) throws Exception { SentenceSpout sentenceSpout = new SentenceSpout(); SplitSentenceBolt splitSentenceBolt = new SplitSentenceBolt(); TopK wordTopKBolt = new TopK(5); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(\"sentenceSpout-1\", sentenceSpout); builder.setBolt(\"splitSentenceBolt-1\", splitSentenceBolt).shuffleGrouping(\"sentenceSpout-1\"); builder.setBolt(\"wordTopKBolt-1\", wordTopKBolt).shuffleGrouping(\"splitSentenceBolt-1\"); Config config = new Config(); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"wordTopKTopology-1\", config, builder.createTopology()); Thread.sleep(999999999); cluster.shutdown(); } } 运行 将代码打包成JAR包，并执行： storm jar TopK.jar WordTopKTopology tk 结果为： 可以发现，始终能输出前k个词频最高的词。 "},"project/storm-summary.html":{"url":"project/storm-summary.html","title":"Storm 编程总结","keywords":"","body":"+++ title = \"Storm 编程总结\" slug = \"storm-summary\" tags = [\"distributed system\",\"project\"] date = \"2018-11-27T15:51:51+08:00\" description = \"\" +++ storm concept 基本类 Spout Spouts represent the source of data in Storm. You can write spouts to read data from data sources such as database, distributed file systems, messaging frameworks etc. BaseRichSpout is an important class and all your Java spouts should extend it. main methods open − Provides the spout with an environment to execute. The executors will run this method to initialize the spout. private SpoutOutputCollector spoutOutputCollector; public void open(Map conf, TopologyContext context, SpoutOutputCollector collector){ this.spoutOutputCollector = collector; } nextTuple − Emits the generated data through the collector. public void nextTuple(){ this.spoutOutputCollector.emit(values); // it should sleep for at least one millisecond to reduce load on the processor before returning. Utils.sleep(100); } declareOutputFields − Declares the output schema of the tuple. public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(\"fieldName\")); } other methods close − This method is called when a spout is going to shutdown. ack − Acknowledges that a specific tuple is processed. fail − Specifies that a specific tuple is not processed and not to be reprocessed. ack and fail are only called for reliable spouts. Bolt 主要提供两个类，分别为BaseRichBolt和BaseBasicBolt BaseBasicBolt tuples are automatically anchored and acknowledged BaseRichBolt we have to do that ourselves 参考这里 不考虑overhead，使用BaseBasicBolt即可 BaseRichBolt 主要实现prepare、execute、declareOutputFields三个接口 注意，需要对OutputCollector进行emit和ack操作 public class SplitSentence extends BaseRichBolt { OutputCollector _collector; public void prepare(Map conf, TopologyContext context, OutputCollector collector) { _collector = collector; } public void execute(Tuple tuple) { String sentence = tuple.getString(0); for(String word: sentence.split(\" \")) { _collector.emit(tuple, new Values(word)); } _collector.ack(tuple); } public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(\"word\")); } } BaseBasicBolt A lot of bolts follow a common pattern of reading an input tuple, emitting tuples based on it, and then acking the tuple at the end of the execute method. These bolts fall into the categories of filters and simple functions 简单的函数时使用BaseBasicBolt更简单，会自动acking，例如： public class SplitSentence extends BaseBasicBolt { public void execute(Tuple tuple, BasicOutputCollector collector) { String sentence = tuple.getString(0); for(String word: sentence.split(\" \")) { collector.emit(new Values(word)); } } public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(\"word\")); } } This implementation is simpler than the implementation from before and is semantically identical. Tuples emitted to BasicOutputCollector are automatically anchored to the input tuple, and the input tuple is acked for you automatically when the execute method completes. TopologyBuilder 指定topology的spout和bolt，以及对应的field TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(\"gender\", genderSpout); builder.setSpout(\"age\", ageSpout); builder.setBolt(\"join\", new SimpleJoinBolt(new Fields(\"gender\", \"age\"))) .fieldsGrouping(\"gender\", new Fields(\"id\")) .fieldsGrouping(\"age\", new Fields(\"id\")); stream grouping Shuffle grouping - This type of grouping distributes tuples equally and randomly to all the available bolt tasks. Fields grouping - This type of grouping makes sure that tuples with same field will go to same bolt task. For example, if the stream is grouped by \"word\" field, tuples with same \"word\" value will always go to same bolt task. Partial Key grouping - The stream is partitioned by the fields specified in the grouping, like the Fields grouping, but are load balanced between two downstream bolts, which provides better utilization of resources when the incoming data is skewed. All grouping - In this grouping techniques, keys are not load balanced and all the stream goes to all of the downstream bolt tasks. Global grouping - In this grouping techniques, all the stream goes to any one of the downstream bolt task. This needs to be used with caution as this will serialize the processing of tuples that can result into slow processing of tuples. None grouping - This grouping is just an indicator that you don't care about grouping and would like to go with default. Currently, default grouping is shuffle grouping and may change in future releases so this should be used carefully. Direct grouping - In this grouping, producer of a tuple decides which task of the consumer will receive the emitted tuple. This is only applicable for streams declared as Direct stream. Local or shuffle grouping - Since a worker process can have multiple tasks, this grouping will shuffle to the in-process tasks. In case, a worker process is not configured to run multiple tasks, this will act as normal shuffle grouping. window相关 joinBolt 现在storm也提供了join，但使用不多，具体用法见这里。 官方文档：JoinBolt Storm可靠性 主要参考官网：Guaranteeing Message Processing Storm可靠性总结 OutputCollector Storm为了简化编程，它提供了BaseBasicBolt，这个类提供的collector不是一般的OutputCollector，而是BasicOutputCollector，通过BasciOutputCollector，已经实现了锚定和结果通知。我们只要继承BaseBasicBolt来定义Bolt就可以了。 发送消息的时候，只要collector.emit(newtuple); 只要指定新产生的tuple就以了，无需指定源tuple。 发送结果的时候，如果成功了，可以不需要写任何代码；如果失败了，则抛出一个FailedException异常。 "},"project/sparksteaming.html":{"url":"project/sparksteaming.html","title":"SparkSteaming使用","keywords":"","body":"环境设置 首先确保已经按安装Spark，使用maven构建工程。 在pox.xml中添加： 4.0.0 SparkStreaming NetworkWordCount 1.0-SNAPSHOT 1.8 1.8 jar alimaven http://maven.aliyun.com/nexus/content/groups/public/ org.apache.spark spark-core_2.11 2.3.2 org.apache.spark spark-streaming_2.11 2.4.0 会自动添加依赖 WordCount 编写程序 这里我们监听一个本地的端口（ TCP source），例如9999，将该端口的数据作为WordCount输入，然后使用与spark一样的操作进行处理即可。 需要注意，这里的local[2]表示两个worker线程： import org.apache.spark.SparkConf; import org.apache.spark.streaming.Durations; import org.apache.spark.streaming.api.java.JavaDStream; import org.apache.spark.streaming.api.java.JavaPairDStream; import org.apache.spark.streaming.api.java.JavaReceiverInputDStream; import org.apache.spark.streaming.api.java.JavaStreamingContext; import scala.Tuple2; import java.util.Arrays; public class NetworkWordCount { public static void main(String[] args) throws Exception{ // Create a local StreamingContext with two working thread and batch interval of 1 second SparkConf conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\"); JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(20)); // Create a DStream that will connect to hostname:port, like localhost:9999 JavaReceiverInputDStream lines = jssc.socketTextStream(\"localhost\", 9999); // Split each line into words JavaDStream words = lines.flatMap(x-> Arrays.asList(x.split(\" \")).iterator()); // Count each word in each batch JavaPairDStream pairs = words.mapToPair(s -> new Tuple2<>(s, 1)); JavaPairDStream wordCounts = pairs.reduceByKey((a, b) -> a+b); // Print the first ten elements of each RDD generated in this DStream to the console wordCounts.print(); jssc.start(); // Start the computation jssc.awaitTermination(); // Wait for the computation to terminate } } 运行程序 首先我们需要启动Netcat 9999端口号作为数据源： nc -lk 9999 然后将打包好的JAR包提交到本地集群： spark-submit --class NetworkWordCount NetworkWordCount.jar localhost 9999 在端口中输入数据，则sparkSteaming会统计20s内的词频： "},"project/flink-installation.html":{"url":"project/flink-installation.html","title":"Flink安装及使用","keywords":"","body":"本地部署 安装 在官网安装Flink，并解压到/usr/local/flink sudo tar -zxf flink-1.6.2-bin-hadoop27-scala_2.11.tgz -C /usr/local cd /usr/local 修改文件名字，并设置权限 sudo mv ./flink-*/ ./flink sudo chown -R hadoop:hadoop ./flink 修改配置文件 Flink对于本地模式是开箱即用的，如果要修改Java运行环境，可修改conf/flink-conf.yaml中的env.java.home，设置为本地java的绝对路径 添加环境变量 vim ~/.bashrc export FLNK_HOME=/usr/local/flink export PATH={% math_inline %}FLINK_HOME/bin:{% endmath_inline %}PATH 启动Flink start-cluster.sh 可以通过观察logs目录下的日志来检测系统是否正在运行了 tail log/flink--jobmanager-.log JobManager同时会在8081端口上启动一个web前端，通过http://localhost:8081来访问 可以发现flink已经正常启动 运行示例 使用Maven创建Flink项目，在pom.xml中添加以下依赖： org.apache.flink flink-java 1.6.2 org.apache.flink flink-streaming-java_2.11 1.6.2 org.apache.flink flink-clients_2.11 1.6.2 批处理运行WordCount 官方示例 可以直接在/usr/local/flink/examples/batch中运行WordCount程序，并且这里还有更多示例： 运行： flink run WordCount.jar 代码 WordCountData 提供原始数据 import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; public class WordCountData { public static final String[] WORDS=new String[]{\"To be, or not to be,--that is the question:--\", \"Whether \\'tis nobler in the mind to suffer\", \"The slings and arrows of outrageous fortune\", \"Or to take arms against a sea of troubles,\", \"And by opposing end them?--To die,--to sleep,--\", \"No more; and by a sleep to say we end\", \"The heartache, and the thousand natural shocks\", \"That flesh is heir to,--\\'tis a consummation\", \"Devoutly to be wish\\'d. To die,--to sleep;--\", \"To sleep! perchance to dream:--ay, there\\'s the rub;\", \"For in that sleep of death what dreams may come,\", \"When we have shuffled off this mortal coil,\", \"Must give us pause: there\\'s the respect\", \"That makes calamity of so long life;\", \"For who would bear the whips and scorns of time,\", \"The oppressor\\'s wrong, the proud man\\'s contumely,\", \"The pangs of despis\\'d love, the law\\'s delay,\", \"The insolence of office, and the spurns\", \"That patient merit of the unworthy takes,\", \"When he himself might his quietus make\", \"With a bare bodkin? who would these fardels bear,\", \"To grunt and sweat under a weary life,\", \"But that the dread of something after death,--\", \"The undiscover\\'d country, from whose bourn\", \"No traveller returns,--puzzles the will,\", \"And makes us rather bear those ills we have\", \"Than fly to others that we know not of?\", \"Thus conscience does make cowards of us all;\", \"And thus the native hue of resolution\", \"Is sicklied o\\'er with the pale cast of thought;\", \"And enterprises of great pith and moment,\", \"With this regard, their currents turn awry,\", \"And lose the name of action.--Soft you now!\", \"The fair Ophelia!--Nymph, in thy orisons\", \"Be all my sins remember\\'d.\"}; public WordCountData() { } public static DataSet getDefaultTextLineDataset(ExecutionEnvironment env){ return env.fromElements(WORDS); } } WordCountTokenizer 切分句子 import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.util.Collector; public class WordCountTokenizer implements FlatMapFunction>{ public WordCountTokenizer(){} public void flatMap(String value, Collector> out) throws Exception { String[] tokens = value.toLowerCase().split(\"\\\\W+\"); int len = tokens.length; for(int i = 0; i0){ out.collect(new Tuple2(tmp,Integer.valueOf(1))); } } } } WordCount 主函数 import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.operators.AggregateOperator; import org.apache.flink.api.java.utils.ParameterTool; public class WordCount { public WordCount(){} public static void main(String[] args) throws Exception { ParameterTool params = ParameterTool.fromArgs(args); ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); Object text; //如果没有指定输入路径，则默认使用WordCountData中提供的数据 if(params.has(\"input\")){ text = env.readTextFile(params.get(\"input\")); }else{ System.out.println(\"Executing WordCount example with default input data set.\"); System.out.println(\"Use -- input to specify file input.\"); text = WordCountData.getDefaultTextLineDataset(env); } AggregateOperator counts = ((DataSet)text).flatMap(new WordCountTokenizer()).groupBy(new int[]{0}).sum(1); //如果没有指定输出，则默认打印到控制台 if(params.has(\"output\")){ counts.writeAsCsv(params.get(\"output\"),\"\\n\", \" \"); env.execute(); }else{ System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); counts.print(); } } } 首先打包成JAR包，这里需要使用-c指定main函数： flink run -c WordCount WordCount.jar 流处理运行WordCount 官方示例 可以直接在/usr/local/flink/examples/streaming中运行WordCount程序，并且这里还有更多示例： 代码 SocketWindowWordCount import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.utils.ParameterTool; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import java.sql.Time; import java.util.stream.Collector; public class SocketWindowWordCount { public static void main(String[] args) throws Exception { // the port to connect to final int port; try { final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(\"port\"); } catch (Exception e) { System.err.println(\"No port specified. Please run 'SocketWindowWordCount --port '\"); return; } // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream text = env.socketTextStream(\"localhost\", port, \"\\n\"); // parse the data, group it, window it, and aggregate the counts DataStream windowCounts = text .flatMap(new FlatMapFunction() { @Override public void flatMap(String value, Collector out) { for (String word : value.split(\"\\\\s\")) { out.collect(new WordWithCount(word, 1L)); } } }) .keyBy(\"word\") .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction() { @Override public WordWithCount reduce(WordWithCount a, WordWithCount b) { return new WordWithCount(a.word, a.count + b.count); } }); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(\"Socket Window WordCount\"); } // Data type for words with count public static class WordWithCount { public String word; public long count; public WordWithCount() {} public WordWithCount(String word, long count) { this.word = word; this.count = count; } @Override public String toString() { return word + \" : \" + count; } } } 首先打包成JAR包，然后启动netcat： nc -l 9000 将终端启动netcat作为输入流： 提交Jar包： flink run -c SocketWindowWordCount WordCountSteaming.jar --port 9000 这样终端会一直等待netcat的输入流 在netcat中输入字符流： 可以在WebUI中查看运行结果： 可视化 Flink的Execution Plan可以可视化，更清楚的查看我们的任务逻辑。 对于任意一个JAR包，我们可以执行： flink info -c WordCount WordCount.jar > ./output 这样，就将执行计划的json文件存放在output中。 将该执行任务复制到这个网站： Flink Plan Visualizer，即可查看： "},"project/flink1.html":{"url":"project/flink1.html","title":"Flink编程练习（一）","keywords":"","body":"环境配置 本项目参考这里，setup。 首先确保已经下载好flink依赖，并从Github下载代码。 下载依赖数据，这里依赖的是纽约出租车数据，可以使用命令行下载： wget http://training.data-artisans.com/trainingData/nycTaxiRides.gz wget http://training.data-artisans.com/trainingData/nycTaxiFares.gz 由于本项目使用java编译，而源文件有scala，为了忽略scala的错误，这里需要指定scala的SDK： 项目 数据设置 使用IDEA打开该maven项目，首先需要找到依赖的文件路径： 可以在ExerciseBase类中找到依赖的出租车数据文件路径并修改： 这里使用的是出租车的 event-time Taxi Ride Cleansing 在utils类中，可以找到GeoUtils，这是用来检测该GPS点数据是否在纽约市内，具体实现参考isInNYC(float lon, float lat)函数。 RideCleansingExercise 若直接运行com/dataartisans/flinktraining/exercises/datastream_java/basics/RideCleansingExercise.java，会报错，其错误是在NYCFilter中： 这里的MissingSolutionException未定义，需要自己实现： RideCleansingTest 在RideCleansingTest中定义了该类的测试类，我们可以每个函数逐步运行： 仔细查看，这里测试了两个函数，分别是测试指定GPS点是否在NYC中，每次将运行的结果与真实结果进行比较（assertEquals），并判断是否相等。 如何test 由于该文件由RideCleansingExercise和RideCleansingSolution组成，且exercise是我们需要处理的类，Solution是正确的类，因此我们要对两个类进行比较。 这里使用javaExercise和javaSolution分别指向这两个类的main函数，并执行RunApp方法： 其中，RunApp又调用了execute方法，这里才是真正执行的函数： 在每个测试类中，我们发现其加入source的方法都一样，如果未指定输入源（ExerciseBase.rides），则将所有的数据作为数据源；这里我们已经指定了特定的GPS点作为数据源。 这里会将执行了filter函数的值进行返回，并与grandtruth进行比较。 Revise 知道错误以后，我们就应该修改之前RideCleansingExercise类中的filter方法，其实就是使用isinNYC函数，返回一个bool类型： private static class NYCFilter implements FilterFunction { @Override public boolean filter(TaxiRide taxiRide) throws Exception { return GeoUtils.isInNYC(taxiRide.startLon, taxiRide.startLat) && GeoUtils.isInNYC(taxiRide.endLon, taxiRide.endLat); } } 再次运行该类，可以得到在NYC的所有GPS点： "},"project/flink2.html":{"url":"project/flink2.html","title":"Flink编程练习（二）","keywords":"","body":"Map 班级学生成绩的随机生成 输入：本班同学的学号 输出： 数据准备 首先需要一个stuID.csv文件，每一列为一个学号： 然后将文件放入HDFS中： hdfs dfs put stuID.csv input 编写程序 import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.utils.ParameterTool; import java.util.Random; public class StuScore { private static Random rand = new Random(); public StuScore(){} public static void main(String[] args) throws Exception { ParameterTool params = ParameterTool.fromArgs(args); ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); DataSet text; if(params.has(\"input\")){ text = env.readTextFile(params.get(\"input\")); }else{ System.out.println(\"Please confirm input keywords!\"); return; } DataSet> stuscore = text.map(new MapFunction>() { @Override public Tuple2 map(String s) throws Exception { return new Tuple2<>(s,rand.nextInt(100) +1); } }); //如果没有指定输出，则默认打印到控制台 if(params.has(\"output\")){ stuscore.writeAsCsv(params.get(\"output\"),\"\\n\", \",\"); env.execute(); }else{ System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); stuscore.print(); } } } 注意点 Flink提供了一个ParameterTool用于简化命令行参数的工具 使用匿名函数新建算子是一种很常见的操作 运行 首先确保已经打开了Flink，并在JAR包下执行 flink run -c StuScore StuScore.jar --input /home/hadoop/Documents/distribution/Flink/StuScore/stuID.csv 这里可以用--output指定输出路径，默认为标准输出 查看结果 reduceByKey 问题 求平均成绩：将全班同学每隔5号分为一组，求每组的平均成绩 输入： 输出： 数据准备 首先需要一个score.csv文件，每一列为学号和学生成绩： 编写程序 import org.apache.commons.compress.archivers.dump.DumpArchiveEntry; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.utils.ParameterTool; public class AVGscore { private static Integer groupSize = 5; public static void main(String[] args) throws Exception { ParameterTool params = ParameterTool.fromArgs(args); ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); DataSet> fileDataSet; if (params.has(\"input\")) { fileDataSet = env.readCsvFile(params.get(\"input\")) .types(Integer.class, Double.class); } else { System.out.println(\"Please confirm input keywords!\"); return; } /** * map string to (id, score) and convert to (group_id, (score,1)) * GroupBy and reduce == reduceByKey * and then map to avg score */ DataSet> stuAVGscore = fileDataSet .map(line -> Tuple2.of( (line.f0-1)/ 5, Tuple2.of(line.f1, 1))) .returns(Types.TUPLE(Types.INT, Types.TUPLE(Types.DOUBLE, Types.INT))) .groupBy(0) .reduce( (kv1, kv2) -> Tuple2.of(kv1.f0, Tuple2.of(kv1.f1.f0 + kv2.f1.f0, kv1.f1.f1 + kv2.f1.f1))) .returns(Types.TUPLE(Types.INT, Types.TUPLE(Types.DOUBLE, Types.INT))) .map( line -> Tuple2.of(line.f0, line.f1.f0 / line.f1.f1) ).returns(Types.TUPLE(Types.INT, Types.DOUBLE)); //如果没有指定输出，则默认打印到控制台 if (params.has(\"output\")) { stuAVGscore.writeAsCsv(params.get(\"output\"), \"\\n\", \",\"); env.execute(); } else { System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); stuAVGscore.print(); } } } 注意点 Flink与Spark不同，没有reduceByKey等算子，都是使用group field进行计算，还没找到更好的方法，具体参考这里 在Flink中，使用Lambda表达式不一定是更好的选择，因为对于每个Lambda表达式，Flink都要求显示的指定返回值（returns） 运行 运行程序 flink run -c AVGscore AVGscore.jar --input /home/hadoop/Documents/distribution/Flink/AVGscore/score.csv 查看结果： Natural join 数据准备 有两个文件 person.txt 1 Aaron 210000 2 Abbott 214000 3 Abel 221000 4 Abner 215000 5 Abraham 226000 6 Adair 225300 7 Adam 223800 8 Addison 224000 9 Adolph 223001 address.txt 210000 Nanjing 214000 Wuxi 221000 Xuzhou 213000 Changzhou 要求以code为连接属性，匹配出person中每个人所在的位置信息；每条记录各个字段之间以空格为分隔符。 编写程序 import org.apache.flink.api.common.functions.FlatJoinFunction; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.api.java.tuple.Tuple4; import org.apache.flink.api.java.utils.ParameterTool; import org.apache.flink.util.Collector; import scala.Int; import java.lang.reflect.Type; public class NaturalJoin { public static void main(String args[]) throws Exception{ ParameterTool params = ParameterTool.fromArgs(args); ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); // code, city DataSet> addDataSet; // id ,name, code DataSet> personDataSet; if (params.has(\"addinput\")) { addDataSet = env.readCsvFile(params.get(\"addinput\")) .fieldDelimiter(\" \") .ignoreInvalidLines() .types(Integer.class, String.class); } else { System.out.println(\"Please confirm input keywords!\"); return; } if (params.has(\"personinput\")) { personDataSet = env.readCsvFile(params.get(\"personinput\")) .fieldDelimiter(\" \") .ignoreInvalidLines() .types(Integer.class, String.class, Integer.class); } else { System.out.println(\"Please confirm input keywords!\"); return; } DataSet> result = personDataSet.join(addDataSet) .where(2) .equalTo(0) .with( (x, y) -> Tuple4.of(x.f0, x.f1, x.f2, y.f1) ).returns(Types.TUPLE(Types.INT,Types.STRING,Types.INT,Types.STRING)); personDataSet.print(); //如果没有指定输出，则默认打印到控制台 if (params.has(\"output\")) { result.writeAsCsv(params.get(\"output\"), \"\\n\", \",\"); env.execute(); } else { System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); result.print(); } } } 注意点 这里需要格外注意的是，本文件存在缺失值，这样会导致readCsvFile失效，因此需要忽略有缺失值的列 flink关于DataSet join 的介绍 运行 本地运行 运行程序并查看结果 flink run -c NaturalJoin NaturalJoin.jar --addinput /home/hadoop/Documents/distribution/Flink/NaturalJoin/address.txt --personinput /home/hadoop/Documents/distribution/Flink/NaturalJoin/person.txt Kmeans 数据准备 输入数据（k-means.dat）： 4,400 96,826 606,776 474,866 400,768 2,920 356,766 36,687 -26,824 第一行标明K的值和数据个数N, 均为整形, 由\",\"隔开 (如 3,10 表示K=3, N=10)。 之后N行中每行代表一个二维向量, 向量元素均为整形, 由\",\"隔开 (如 1,2 表示向量(1, 2))。 输出: K行, 每行是一个聚类图心的二维向量, 向量元素均为浮点型 (如 1.1,2.3)。 编写程序 point.java 用于自定义point类（POJO对象） import java.io.Serializable; public class Point implements Serializable { public double x,y; public Point() { } public Point(double x, double y) { this.x = x; this.y = y; } public Point add(Point other) { x += other.x; y += other.y; return this; } public Point div(long val) { x /= val; y /= val; return this; } public double euclideanDistance(Point other) { return Math.sqrt((x - other.x) * (x - other.x) + (y - other.y) * (y - other.y)); } public String toString() { return x + \" \" + y; } } kmeansRun.java import org.apache.flink.api.common.functions.*; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.functions.FunctionAnnotation; import org.apache.flink.api.java.operators.IterativeDataSet; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.api.java.utils.ParameterTool; import org.apache.flink.configuration.Configuration; import java.io.BufferedReader; import java.io.FileReader; import java.util.ArrayList; import java.util.Collection; public class Kmeans { public static void main(String[] args) throws Exception { // Checking input parameters final ParameterTool params = ParameterTool.fromArgs(args); // set up execution environment ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(params); // make parameters available in the web interface // get input data: DataSet points = getPoint(params, env); if (points == null) return; DataSet centroids = getCentroid(params, env); // set number of bulk iterations for KMeans algorithm IterativeDataSet loop = centroids.iterate(params.getInt(\"iterations\", 100)); DataSet newCentroids = points // compute closest centroid for each point .map(new SelectNearestCenter()).withBroadcastSet(loop, \"centroids\") // count and sum point coordinates for each centroid .map(new CountAppender()) .groupBy(0).reduce(new CentroidAccumulator()) // compute new centroids from point counts and coordinate sums .map(new CentroidAverager()); // feed new centroids back into next iteration DataSet finalCentroids = loop.closeWith(newCentroids, newCentroids.filter(new thresholdFilter()).withBroadcastSet(loop,\"centroids\")); // DataSet> clusteredPoints = points // // assign points to final clusters // .map(new SelectNearestCenter()).withBroadcastSet(finalCentroids, \"centroids\"); // emit result if (params.has(\"output\")) { finalCentroids.writeAsCsv(params.get(\"output\"), \"\\n\", \" \"); env.execute(); } else { System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); finalCentroids.print(); } } private static DataSet getPoint(ParameterTool params, ExecutionEnvironment env) { DataSet points; if (params.has(\"input\")) { // read points from CSV file points = env.readCsvFile(params.get(\"input\")) .ignoreFirstLine() .pojoType(Point.class, \"x\", \"y\"); } else { System.out.println(\"Use --input to specify file input.\"); return null; } return points; } private static DataSet getCentroid(ParameterTool params, ExecutionEnvironment env) throws Exception { ArrayList centroidArrayList = new ArrayList<>(); BufferedReader br = new BufferedReader(new FileReader(params.get(\"input\"))); String text = br.readLine(); int k = Integer.parseInt(text.split(\",\")[0]); while (k != 0) { text = br.readLine(); double x = Double.parseDouble(text.split(\",\")[0]); double y = Double.parseDouble(text.split(\",\")[1]); centroidArrayList.add(new Centroid(k, x, y)); k--; } DataSet centroids = env.fromCollection(centroidArrayList); return centroids; } /** * Determines the closest cluster center for a data point. */ @FunctionAnnotation.ForwardedFields(\"*->1\") public static final class SelectNearestCenter extends RichMapFunction> { private Collection centroids; /** * Reads the centroid values from a broadcast variable into a collection. */ @Override public void open(Configuration parameters) throws Exception { this.centroids = getRuntimeContext().getBroadcastVariable(\"centroids\"); } @Override public Tuple2 map(Point p) throws Exception { double minDistance = Double.MAX_VALUE; int closestCentroidId = -1; // check all cluster centers for (Centroid centroid : centroids) { // compute distance double distance = p.euclideanDistance(centroid); // update nearest cluster if necessary if (distance (closestCentroidId, p); } } /** * Appends a count variable to the tuple. */ @FunctionAnnotation.ForwardedFields(\"f0;f1\") public static final class CountAppender implements MapFunction, Tuple3> { @Override public Tuple3 map(Tuple2 t) { return new Tuple3<>(t.f0, t.f1, 1L); } } /** * Sums and counts point coordinates. */ @FunctionAnnotation.ForwardedFields(\"0\") public static final class CentroidAccumulator implements ReduceFunction> { @Override public Tuple3 reduce(Tuple3 val1, Tuple3 val2) { return new Tuple3<>(val1.f0, val1.f1.add(val2.f1), val1.f2 + val2.f2); } } /** * Computes new centroid from coordinate sum and count of points. */ @FunctionAnnotation.ForwardedFields(\"0->id\") public static final class CentroidAverager implements MapFunction, Centroid> { @Override public Centroid map(Tuple3 value) { // id, X/num Y/num return new Centroid(value.f0, value.f1.div(value.f2)); } } /** * Filter that filters vertices where the centorid difference is below a threshold. */ public static final class thresholdFilter extends RichFilterFunction { private Collection centroids; private double threshold = 1e-5; /** * Reads the centroid values from a broadcast variable into a collection. */ @Override public void open(Configuration parameters) throws Exception { this.centroids = getRuntimeContext().getBroadcastVariable(\"centroids\"); } @Override public boolean filter(Centroid centroid) { for (Centroid oldcentroid : centroids) { if (centroid.id == oldcentroid.id) { // compute distance double distance = centroid.euclideanDistance(oldcentroid); return (distance > this.threshold); } } return true; } } } 注意事项 使用Distributed cache 在transform中，使用ForwardedFields注解来帮助flink优化 这里使用了迭代（iteration），比较复杂，若需要进行比较退出循环，则需要在closeWith中加入第二个Dataset，若其DataSet是空的，则退出循环 运行 本地运行 运行程序 flink run -c Kmeans Kmeans.jar --input /home/hadoop/Documents/distribution/Flink/kmeans/k-means.dat 查看结果： "},"project/graph-in-hadoop.html":{"url":"project/graph-in-hadoop.html","title":"常用图算法实现--Hadoop","keywords":"","body":"PageRank 数据准备 边： 1 2 1 15 2 3 2 4 2 5 2 6 2 7 3 13 4 2 5 11 5 12 6 1 6 7 6 8 7 1 7 8 8 1 8 9 8 10 9 14 9 1 10 1 10 13 11 12 11 1 12 1 13 14 14 12 15 1 网页： 1 2 2 5 3 1 4 1 5 2 6 3 7 2 8 3 9 2 10 2 11 2 12 1 13 1 14 1 15 1 将这两个文件放入HDFS： hdfs dfs -mkdir input/PageRank hdfs dfs -put links.txt input/PageRank hdfs dfs -put pagesHadoop.txt input/PageRank 编写程序 PageRank import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.util.GenericOptionsParser; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.URISyntaxException; import static java.lang.StrictMath.abs; public class PageRank { private static final String CACHED_PATH = \"output/cache\"; private static final String ACTUAL_PATH = \"output/Graph/HadoopPageRank\"; public static final int maxIterations = 500; public static final double threshold = 0.0001; public static final double dumping = 0.85; public static int pageNum = 0; public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException, URISyntaxException { Configuration conf = new Configuration(); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length != 3) { System.err.println(\"Usage: PageRank \"); System.exit(2); } int code = 0; Path PagePath = new Path(otherArgs[0]); Path LinksPath = new Path(otherArgs[1]); pageNum = Integer.parseInt(otherArgs[2]); conf.set(\"pageNum\", pageNum + \"\"); conf.set(\"dumping\", dumping + \"\"); Path cachePath = new Path(CACHED_PATH); Path actualPath = new Path(ACTUAL_PATH); // Delete output if exists FileSystem hdfs = FileSystem.get(conf); if (hdfs.exists(actualPath)) hdfs.delete(actualPath, true); // recursive delete // prepare original rank for (int i = 1; i > \" + counter + \" || rank change: \" + changed); } System.exit(code); } public static void writeFileByline(String dst, String contents) throws IOException { Configuration conf = new Configuration(); Path dstPath = new Path(dst); FileSystem fs = dstPath.getFileSystem(conf); FSDataOutputStream outputStream = null; if (!fs.exists(dstPath)) { outputStream = fs.create(dstPath); } else { outputStream = fs.append(dstPath); } contents = contents + \"\\n\"; outputStream.write(contents.getBytes(\"utf-8\")); outputStream.close(); } } PageRankMapper import org.apache.commons.lang.StringUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import java.io.BufferedReader; import java.io.FileReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.*; public class PageRankMapper extends Mapper { Map rank = new HashMap<>(); Map pages = new HashMap<>(); /** * reading the rank from the distributed cache */ public void setup(Context context) throws IOException, InterruptedException { String lineString = null; // read rank file Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); FSDataInputStream hdfsInStream = fs.open(new Path(\"output/cache/part-r-00000\")); InputStreamReader isr = new InputStreamReader(hdfsInStream, \"utf-8\"); BufferedReader br = new BufferedReader(isr); while ((lineString = br.readLine()) != null) { String[] keyValue = StringUtils.split(lineString, \" \"); rank.put(Integer.parseInt(keyValue[0]), Double.parseDouble(keyValue[1])); } br.close(); // read pages file String PagesFiles = context.getLocalCacheFiles()[0].getName(); br = new BufferedReader(new FileReader(PagesFiles)); while ((lineString = br.readLine()) != null) { String[] keyValue = StringUtils.split(lineString, \" \"); pages.put(Integer.parseInt(keyValue[0]), Integer.parseInt(keyValue[1])); } br.close(); } public void map(Text from, Text to, Context context) throws IOException, InterruptedException { int fromPoint = Integer.parseInt(from.toString()); int toPoint = Integer.parseInt(to.toString()); double newRank = rank.get(fromPoint) * (1.0 / pages.get(fromPoint)); context.write(new IntWritable(toPoint), new DoubleWritable(newRank)); } } PageRankReducer import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.Reducer; import java.io.IOException; public class PageRankReducer extends Reducer { public void reduce(IntWritable key, Iterable values, Context context) throws IOException, InterruptedException { Configuration conf = context.getConfiguration(); int pageNum = Integer.parseInt(conf.get(\"pageNum\")); double dumping = Double.parseDouble(conf.get(\"dumping\")); double rank = 0.0; for (DoubleWritable value : values) rank += value.get(); rank = (1 - dumping) * (1.0/pageNum) + dumping * rank; context.write(key, new DoubleWritable(rank)); } } 思路： 首先指定KeyValueTextInputFormat，并指定page个数（在Hadoop中不太好直接求） 将每个顶点的出度文件pagesHadoop作为distributionCache，并首先将初始rank值写入cache文件中 每次读cache文件中的rank值，再进行计算，写入目标文件中，前后的rank值进行比较，若不满足阈值，将更新后的rank值写入cache中继续进行迭代 运行 hadoop jar PageRank.jar input/PageRank/pagesHadoop.txt input/PageRank/links.txt 15 可以发现，Hadoop执行循环操作，比spark、flink慢很多 查看结果： hdfs dfs -cat output/Graph/HadoopPageRank/* ConnectedComponents 数据准备 提供基本数据集，与PageRank一样，指定顶点和边 vertices.txt 准备一些顶点，例如1-16 edges.txt 准备一些连接边： 1 2 2 3 2 4 3 5 6 7 8 9 8 10 5 11 11 12 10 13 9 14 13 14 1 15 16 1 放入HDFS： hdfs dfs -mkdir input/ConnectedComponents hdfs dfs -put edges.txt input/ConnectedComponents 编写程序 ConnectedComponents import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.util.GenericOptionsParser; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.URISyntaxException; public class ConnectedComponents { private static final String CACHED_PATH = \"output/cache\"; private static final String ACTUAL_PATH = \"output/Graph/HadoopConnectedComponents\"; public static final int maxIterations = 100; public static int verticesNum = 0; public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException, URISyntaxException { Configuration conf = new Configuration(); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length != 2) { System.err.println(\"Usage: PageRank \"); System.exit(2); } int code = 0; Path EdgesPath = new Path(otherArgs[0]); verticesNum = Integer.parseInt(otherArgs[1]); conf.set(\"verticesNum\", verticesNum + \"\"); Path cachePath = new Path(CACHED_PATH); Path actualPath = new Path(ACTUAL_PATH); // Delete output if exists FileSystem hdfs = FileSystem.get(conf); if (hdfs.exists(actualPath)) hdfs.delete(actualPath, true); // recursive delete // prepare original ConnectedComponents for (int i = 1; i > \" + counter + \" || component change: \" + changed); } System.exit(code); } public static void writeFileByline(String dst, String contents) throws IOException { Configuration conf = new Configuration(); Path dstPath = new Path(dst); FileSystem fs = dstPath.getFileSystem(conf); FSDataOutputStream outputStream = null; if (!fs.exists(dstPath)) { outputStream = fs.create(dstPath); } else { outputStream = fs.append(dstPath); } contents = contents + \"\\n\"; outputStream.write(contents.getBytes(\"utf-8\")); outputStream.close(); } } ConnectedComponentsMapper import org.apache.commons.lang.StringUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.*; public class ConnectedComponentsMapper extends Mapper { Map components = new HashMap<>(); /** * reading the rank from the distributed cache */ public void setup(Context context) throws IOException, InterruptedException { String lineString = null; // read rank file Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); FSDataInputStream hdfsInStream = fs.open(new Path(\"output/cache/part-r-00000\")); InputStreamReader isr = new InputStreamReader(hdfsInStream, \"utf-8\"); BufferedReader br = new BufferedReader(isr); while ((lineString = br.readLine()) != null) { String[] keyValue = StringUtils.split(lineString, \" \"); components.put(Integer.parseInt(keyValue[0]), Integer.parseInt(keyValue[1])); } br.close(); } public void map(Text from, Text to, Context context) throws IOException, InterruptedException { int fromPoint = Integer.parseInt(from.toString()); int toPoint = Integer.parseInt(to.toString()); context.write(new IntWritable(toPoint), new IntWritable(components.get(fromPoint))); context.write(new IntWritable(fromPoint), new IntWritable(components.get(fromPoint))); } } ConnectedComponentsReduer import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.Reducer; import java.io.IOException; public class ConnectedComponentsReduer extends Reducer { public void reduce(IntWritable key, Iterable values, Context context) throws IOException, InterruptedException { Configuration conf = context.getConfiguration(); int component = Integer.parseInt(conf.get(\"verticesNum\")); for (IntWritable value : values) { if (value.get() 思路： 与PageRank一样，需要准备cache文件作为初始化连通分量，每次得到新的结果与cache文件进行比较，如果有更新则继续迭代 在map中，为了保证每个点都会出现在reduce中，将from点和to点都输入到reduce中 运行 hadoop jar ConnectedComponents.jar input/ConnectedComponents/edges.txt 16 迭代了6次： hdfs dfs -cat output/Graph/HadoopConnectedComponents/* 最后结果为： SingleSourceShortestPaths 数据准备 首先我们需要准备边和点 边： 1 2 12.0 1 3 13.0 2 3 23.0 3 4 34.0 3 5 35.0 4 5 45.0 5 1 51.0 放入HDFS： hdfs dfs -mkdir input/SingleSourceShortestPaths hdfs dfs -put edges.txt input/SingleSourceShortestPaths 编写程序 SingleSourceShortestPaths import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.util.GenericOptionsParser; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.URISyntaxException; import static java.lang.StrictMath.abs; public class SingleSourceShortestPaths { private static final String CACHED_PATH = \"output/cache\"; private static final String ACTUAL_PATH = \"output/Graph/HadoopSingleSourceShortestPaths\"; public static final int maxIterations = 100; private static final double EPSILON = 0.0001; public static int sourcePoint = 1; public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException, URISyntaxException { Configuration conf = new Configuration(); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if (otherArgs.length != 2) { System.err.println(\"Usage: PageRank \"); System.exit(2); } int code = 0; Path EdgesPath = new Path(otherArgs[0]); int verticesNum = Integer.parseInt(otherArgs[1]); conf.set(\"verticesNum\", verticesNum + \"\"); Path cachePath = new Path(CACHED_PATH); Path actualPath = new Path(ACTUAL_PATH); // Delete output if exists FileSystem hdfs = FileSystem.get(conf); if (hdfs.exists(actualPath)) hdfs.delete(actualPath, true); // recursive delete // prepare original distance for (int i = 1; i > \" + counter + \" || distance change: \" + changed); } System.exit(code); } public static void writeFileByline(String dst, String contents) throws IOException { Configuration conf = new Configuration(); Path dstPath = new Path(dst); FileSystem fs = dstPath.getFileSystem(conf); FSDataOutputStream outputStream = null; if (!fs.exists(dstPath)) { outputStream = fs.create(dstPath); } else { outputStream = fs.append(dstPath); } contents = contents + \"\\n\"; outputStream.write(contents.getBytes(\"utf-8\")); outputStream.close(); } } SingleSourceShortestPathsMapper import org.apache.commons.lang.StringUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.*; public class SingleSourceShortestPathsMapper extends Mapper { Map PointDistance = new HashMap<>(); /** * reading the rank from the distributed cache */ public void setup(Context context) throws IOException, InterruptedException { String lineString = null; // read rank file Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); FSDataInputStream hdfsInStream = fs.open(new Path(\"output/cache/part-r-00000\")); InputStreamReader isr = new InputStreamReader(hdfsInStream, \"utf-8\"); BufferedReader br = new BufferedReader(isr); while ((lineString = br.readLine()) != null) { String[] keyValue = StringUtils.split(lineString, \" \"); PointDistance.put(Integer.parseInt(keyValue[0]), Double.parseDouble(keyValue[1])); } br.close(); } public void map(Object object, Text line, Context context) throws IOException, InterruptedException { String[] lineData = line.toString().split(\" \"); int fromPoint = Integer.parseInt(lineData[0]); int toPoint = Integer.parseInt(lineData[1]); double distance = Double.parseDouble(lineData[2]); if (distance SingleSourceShortestPathsReducer import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.mapreduce.Reducer; import java.io.IOException; public class SingleSourceShortestPathsReducer extends Reducer { public void reduce(IntWritable key, Iterable values, Context context) throws IOException, InterruptedException { double dis = Double.POSITIVE_INFINITY; for (DoubleWritable value : values) { if (value.get() 思想： 主要想法和之前一样，不再赘述 需要注意的是，每次map需要把前一次的结果也发给reduce进行比较，不然reduce出来的点个数会变少（例如原点就不会有） 运行 hadoop jar SingleSourceShortestPaths.jar input/SingleSourceShortestPaths/edges.txt 5 一共迭代了4次： 查看结果 hdfs dfs -cat output/Graph/HadoopSingleSourceShortestPaths/* "},"project/graph-in-spark.html":{"url":"project/graph-in-spark.html","title":"常用图算法实现--Spark","keywords":"","body":"PageRank 数据准备 边： 1 2 1 15 2 3 2 4 2 5 2 6 2 7 3 13 4 2 5 11 5 12 6 1 6 7 6 8 7 1 7 8 8 1 8 9 8 10 9 14 9 1 10 1 10 13 11 12 11 1 12 1 13 14 14 12 15 1 网页： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 将这两个文件放入HDFS： hdfs dfs -mkdir input/PageRank hdfs dfs -put links.txt input/PageRank hdfs dfs -put pages.txt input/PageRank 编写程序 import org.apache.spark.SparkConf; import org.apache.spark.api.java.*; import org.apache.spark.api.java.function.Function; import org.apache.spark.api.java.function.PairFunction; import scala.Tuple2; import static java.lang.Math.abs; public class PageRank { private static int MaxIteration = 100; private static final double DAMPENING_FACTOR = 0.85; private static final double EPSILON = 0.0001; public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(\"PageRank\"); JavaSparkContext sc = new JavaSparkContext(conf); sc.setLogLevel(\"WARN\"); String linksFile = \"hdfs:///user/hadoop/input/PageRank/links.txt\"; String pagesFile = \"hdfs:///user/hadoop/input/PageRank/pages.txt\"; String rankFile = \"hdfs:///user/hadoop/output/Graph/SparkPageRank\"; /** * neighborRDD: (from, s) * linksRDD: tuple (from, [to,1/m]) * pageRDD: vertex * pageRankRDD: (point, 1/n) */ JavaPairRDD neighborRDD = sc.textFile(linksFile) .mapToPair( line -> new Tuple2<>( Integer.parseInt(line.split(\" \")[0]), 1)) .reduceByKey((x, y) -> x + y); JavaPairRDD> linksRDD = sc.textFile(linksFile) .mapToPair( line -> new Tuple2<>( Integer.parseInt(line.split(\" \")[0]), Integer.parseInt(line.split(\" \")[1]) )) .join(neighborRDD); JavaRDD pagesRDD = sc.textFile(pagesFile).map(line -> Integer.parseInt(line)); long pageCount = pagesRDD.count(); JavaPairRDD pageRankRDD = pagesRDD.mapToPair( vertex -> new Tuple2<>(vertex, 1.0 / pageCount) ); int count = 0; while (count NewPageRankRDD = linksRDD.join(pageRankRDD) .mapToPair( new PairFunction, Double>>, Integer, Double>() { @Override public Tuple2 call(Tuple2, Double>> ans) throws Exception { // // [ toNode, fraction * rank] return new Tuple2<>(ans._2._1._1, ans._2._2/ans._2._1._2); } }) .reduceByKey((v1, v2) -> v1 + v2) .mapValues( new Function() { double dampening = DAMPENING_FACTOR; double randomJump = (1 - DAMPENING_FACTOR) / pageCount; @Override public Double call(Double value) throws Exception { value = value * dampening + randomJump; return value; } } ); count++; JavaPairRDD> compare = pageRankRDD.join(NewPageRankRDD).filter(each -> abs(each._2._1 - each._2._2) > EPSILON); if (compare.isEmpty() || count > MaxIteration) break; pageRankRDD = NewPageRankRDD; } pageRankRDD.saveAsTextFile(rankFile); } } 思路： 全部使用Lambda表达式进行，首先需要找到所有的边的条数，初始化Rank值 然后使用Join进行合并，并计算下一轮Rank 使用DAMPENING_FACTOR进行随机跳转 运行 spark-submit --class PageRank PageRank-1.0.jar hdfs dfs -cat output/Graph/SparkPageRank/* 结果为： ConnectedComponents 数据准备 提供基本数据集，与PageRank一样，指定顶点和边 vertices.txt 准备一些顶点，例如1-16 edges.txt 准备一些连接边： 1 2 2 3 2 4 3 5 6 7 8 9 8 10 5 11 11 12 10 13 9 14 13 14 1 15 16 1 将这两个文件放入HDFS： hdfs dfs -mkdir input/ConnectedComponents hdfs dfs -put edges.txt input/ConnectedComponents hdfs dfs -put vertices.txt input/ConnectedComponents 编写程序 import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaSparkContext; import scala.Tuple2; import static java.lang.StrictMath.min; public class ConnectedComponents { public static int MaxIteration = 100; public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(\"ConnectedComponents\"); JavaSparkContext sc = new JavaSparkContext(conf); sc.setLogLevel(\"WARN\"); String edgesFile = \"hdfs:///user/hadoop/input/ConnectedComponents/edges.txt\"; String verticesFile = \"hdfs:///user/hadoop/input/ConnectedComponents/vertices.txt\"; String outFile = \"hdfs:///user/hadoop/output/Graph/SparkConnectedComponents\"; /** * edgesRDD: [x,y] * componentsRDD: [x,x] init */ JavaPairRDD edgesRDD = sc.textFile(edgesFile) .mapToPair( line -> new Tuple2<>( Integer.parseInt(line.split(\" \")[0]), Integer.parseInt(line.split(\" \")[1]) ) ); JavaPairRDD componentsRDD = sc.textFile(verticesFile) .mapToPair( line -> new Tuple2<>(Integer.parseInt(line), Integer.parseInt(line)) ); int count = 0; while (count newcomponentsRDD = componentsRDD.join(edgesRDD) .mapToPair( x -> new Tuple2<>(x._2._2, x._2._1) ) .reduceByKey( (v1, v2) -> min(v1, v2) ); JavaPairRDD> filterRDD = newcomponentsRDD.join(componentsRDD) .filter( each -> each._2._1 min(v._1, v._2.orElse(v._1)) ); count++; } componentsRDD.saveAsTextFile(outFile); } } 思路： 首先需要将每个点映射成自己的强连通分支 每次迭代，更新与自己相连的点的强连通分支，取最小值 使用左连接更新原始的强连通分支 运行 spark-submit --class ConnectedComponents ConnectedComponents-1.0.jar hdfs dfs -cat output/Graph/SparkConnectedComponents/* 查看结果： SingleSourceShortestPaths 数据准备 首先我们需要准备边和点 边： 1 2 12.0 1 3 13.0 2 3 23.0 3 4 34.0 3 5 35.0 4 5 45.0 5 1 51.0 点： 1 2 3 4 5 将这两个文件放入HDFS： hdfs dfs -mkdir input/SingleSourceShortestPaths hdfs dfs -put edges.txt input/SingleSourceShortestPaths hdfs dfs -put vertices.txt input/SingleSourceShortestPaths 编写程序 import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaSparkContext; import scala.Tuple2; import javax.validation.constraints.Max; import static java.lang.StrictMath.min; public class SingleSourceShortestPaths { public static int sourceVerticeID = 1; public static int MaxIteration = 100; public static void main(String[] args) throws Exception { SparkConf conf = new SparkConf().setAppName(\"ConnectedComponents\"); JavaSparkContext sc = new JavaSparkContext(conf); sc.setLogLevel(\"WARN\"); String edgesFile = \"hdfs:///user/hadoop/input/SingleSourceShortestPaths/edges.txt\"; String verticesFile = \"hdfs:///user/hadoop/input/SingleSourceShortestPaths/vertices.txt\"; String outFile = \"hdfs:///user/hadoop/output/Graph/SparkSingleSourceShortestPaths\"; /** * edgesRDD: [from, to, dis ] * verticesRDD: [vertice, dis] */ JavaPairRDD> edgesRDD = sc.textFile(edgesFile) .mapToPair( line -> { int from = Integer.parseInt(line.split(\" \")[0]); int to = Integer.parseInt(line.split(\" \")[1]); double dis = Double.parseDouble(line.split(\" \")[2]); return new Tuple2<>(from, new Tuple2<>(to, dis)); } ); JavaPairRDD verticesRDD = sc.textFile(verticesFile) .mapToPair( line -> { int vertice = Integer.parseInt(line); if (vertice == sourceVerticeID) return new Tuple2<>(vertice, 0.0); return new Tuple2<>(vertice, Double.POSITIVE_INFINITY); } ); int count = 0; while (count newVerticesRDD = verticesRDD .join(edgesRDD) .mapToPair( line -> { if (line._2._1 != Double.POSITIVE_INFINITY) return new Tuple2<>(line._2._2._1, line._2._1 + line._2._2._2); return new Tuple2<>(line._2._2._1, Double.POSITIVE_INFINITY); } ).reduceByKey( (v1, v2) -> min(v1, v2)); JavaPairRDD> filterRDD = newVerticesRDD.join(verticesRDD) .filter( each -> each._2._1 min(v._1, v._2.orElse(v._1))); } verticesRDD.saveAsTextFile(outFile); } } 思路： 首先需要初始化每个顶点的距离，将原始点设置为0，其余设置为无穷 每次迭代得到新的顶点距离，并使用reduceByKey最小化，比较是否更新 然后将更新得到的顶点距离加入原始RDD中 运行 spark-submit --class SingleSourceShortestPaths SingleSourceShortestPaths-1.0.jar hdfs dfs -cat output/Graph/SparkSingleSourceShortestPaths/* 查看结果： "},"project/graph-in-flink.html":{"url":"project/graph-in-flink.html","title":"常用图算法实现--Flink","keywords":"","body":"PageRank 主要参考官网的example 算法流程 每次计算当前每个网页的转移概率，计算下一时刻到达每个网页的概率并加入随机跳转 数据准备 pages.txt 准备一些顶点，例如1-15 links.txt 准备一些连接边（也就是链接数）： 1 2 1 15 2 3 2 4 2 5 2 6 2 7 3 13 4 2 5 11 5 12 6 1 6 7 6 8 7 1 7 8 8 1 8 9 8 10 PageRank.java @SuppressWarnings(\"serial\") public class PageRank { private static final double DAMPENING_FACTOR = 0.85; private static final double EPSILON = 0.0001; // ************************************************************************* // PROGRAM // ************************************************************************* public static void main(String[] args) throws Exception { ParameterTool params = ParameterTool.fromArgs(args); final int numPages = params.getInt(\"numPages\", PageRankData.getNumberOfPages()); final int maxIterations = params.getInt(\"iterations\", 10); // set up execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // make the parameters available to the web ui env.getConfig().setGlobalJobParameters(params); // get input data DataSet pagesInput = getPagesDataSet(env, params); DataSet> linksInput = getLinksDataSet(env, params); // assign initial rank to pages pi = ([1,1/n] ,... [n,1/n]) DataSet> pagesWithRanks = pagesInput. map(new RankAssigner((1.0d / numPages))); // build adjacency list from link input (1,[2,3,5])... DataSet> adjacencyListInput = linksInput.groupBy(0).reduceGroup(new BuildOutgoingEdgeList()); // set iterative data set IterativeDataSet> iteration = pagesWithRanks.iterate(maxIterations); DataSet> newRanks = iteration // join pages with outgoing edges and distribute rank [1,1/n] join 1,[1,3,5] => [1,1/3n],[3,1/3n],[5,1/3n] .join(adjacencyListInput).where(0).equalTo(0).flatMap(new JoinVertexWithEdgesMatch()) // collect and sum ranks .groupBy(0).aggregate(SUM, 1) // apply dampening factor choosing stay or leave .map(new Dampener(DAMPENING_FACTOR, numPages)); DataSet> finalPageRanks = iteration.closeWith( newRanks, newRanks.join(iteration).where(0).equalTo(0) // termination condition .filter(new EpsilonFilter())); // emit result if (params.has(\"output\")) { finalPageRanks.writeAsCsv(params.get(\"output\"), \"\\n\", \" \"); // execute program env.execute(\"Basic Page Rank Example\"); } else { System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); finalPageRanks.print(); } } // ************************************************************************* // USER FUNCTIONS // ************************************************************************* /** * A map function that assigns an initial rank to all pages. */ public static final class RankAssigner implements MapFunction> { Tuple2 outPageWithRank; public RankAssigner(double rank) { this.outPageWithRank = new Tuple2(-1L, rank); } @Override public Tuple2 map(Long page) { outPageWithRank.f0 = page; return outPageWithRank; } } /** * A reduce function that takes a sequence of edges and builds the adjacency list for the vertex where the edges * originate. Run as a pre-processing step. */ @ForwardedFields(\"0\") public static final class BuildOutgoingEdgeList implements GroupReduceFunction, Tuple2> { private final ArrayList neighbors = new ArrayList(); @Override public void reduce(Iterable> values, Collector> out) { neighbors.clear(); Long id = 0L; for (Tuple2 n : values) { id = n.f0; neighbors.add(n.f1); } out.collect(new Tuple2(id, neighbors.toArray(new Long[neighbors.size()]))); } } /** * Join function that distributes a fraction of a vertex's rank to all neighbors. */ public static final class JoinVertexWithEdgesMatch implements FlatMapFunction, Tuple2>, Tuple2> { @Override public void flatMap(Tuple2, Tuple2> value, Collector> out){ Long[] neighbors = value.f1.f1; double rank = value.f0.f1; double rankToDistribute = rank / ((double) neighbors.length); for (Long neighbor: neighbors) { out.collect(new Tuple2(neighbor, rankToDistribute)); } } } /** * The function that applies the page rank dampening formula. */ @ForwardedFields(\"0\") public static final class Dampener implements MapFunction, Tuple2> { private final double dampening; private final double randomJump; public Dampener(double dampening, double numVertices) { this.dampening = dampening; this.randomJump = (1 - dampening) / numVertices; } @Override public Tuple2 map(Tuple2 value) { value.f1 = (value.f1 * dampening) + randomJump; return value; } } /** * Filter that filters vertices where the rank difference is below a threshold. */ public static final class EpsilonFilter implements FilterFunction, Tuple2>> { @Override public boolean filter(Tuple2, Tuple2> value) { return Math.abs(value.f0.f1 - value.f1.f1) > EPSILON; } } // ************************************************************************* // UTIL METHODS // ************************************************************************* private static DataSet getPagesDataSet(ExecutionEnvironment env, ParameterTool params) { if (params.has(\"pages\")) { return env.readCsvFile(params.get(\"pages\")) .fieldDelimiter(\" \") .lineDelimiter(\"\\n\") .types(Long.class) .map(new MapFunction, Long>() { @Override public Long map(Tuple1 v) { return v.f0; } }); } else { System.out.println(\"Executing PageRank example with default pages data set.\"); System.out.println(\"Use --pages to specify file input.\"); return PageRankData.getDefaultPagesDataSet(env); } } private static DataSet> getLinksDataSet(ExecutionEnvironment env, ParameterTool params) { if (params.has(\"links\")) { return env.readCsvFile(params.get(\"links\")) .fieldDelimiter(\" \") .lineDelimiter(\"\\n\") .types(Long.class, Long.class); } else { System.out.println(\"Executing PageRank example with default links data set.\"); System.out.println(\"Use --links to specify file input.\"); return PageRankData.getDefaultEdgeDataSet(env); } } } 注意点 处理逻辑为：首先将输入数据转为邻接链表，然后迭代计算每一次的Rank，再加上每一次dampening（可能停留，可能随机），得到下一次的Rank 最后在closeWith中与前一次的Rank值进行对比，小于阈值则退出循环 运行 打包成Jar包，并执行： flink run -c PageRank PageRank.jar --links /home/hadoop/Documents/distribution/Flink/PageRank/links.txt --pages /home/hadoop/Documents/distribution/Flink/PageRank/pages.txt 结果为： ConnectedComponents 数据准备 提供基本数据集，与PageRank一样，指定顶点和边 vertices.txt 准备一些顶点，例如1-16 edges.txt 准备一些连接边： 1 2 2 3 2 4 3 5 6 7 8 9 8 10 5 11 11 12 10 13 9 14 13 14 1 15 16 1 ConnectedComponents.java import org.apache.flink.api.common.functions.FlatJoinFunction; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.common.functions.JoinFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.aggregation.Aggregations; import org.apache.flink.api.java.functions.FunctionAnnotation.ForwardedFields; import org.apache.flink.api.java.functions.FunctionAnnotation.ForwardedFieldsFirst; import org.apache.flink.api.java.functions.FunctionAnnotation.ForwardedFieldsSecond; import org.apache.flink.api.java.operators.DeltaIteration; import org.apache.flink.api.java.tuple.Tuple1; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.utils.ParameterTool; import org.apache.flink.util.Collector; @SuppressWarnings(\"serial\") public class ConnectedComponents { // ************************************************************************* // PROGRAM // ************************************************************************* public static void main(String... args) throws Exception { // Checking input parameters final ParameterTool params = ParameterTool.fromArgs(args); // set up execution environment ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); final int maxIterations = params.getInt(\"iterations\", 10); // make parameters available in the web interface env.getConfig().setGlobalJobParameters(params); // read vertex and edge data DataSet vertices = getVertexDataSet(env, params); DataSet> edges = getEdgeDataSet(env, params).flatMap(new UndirectEdge()); // assign the initial components (equal to the vertex id) [1,1],[2,2] DataSet> verticesWithInitialId = vertices.map(new DuplicateValue()); // open a delta iteration DeltaIteration, Tuple2> iteration = verticesWithInitialId.iterateDelta(verticesWithInitialId, maxIterations, 0); // apply the step logic: join with the edges, select the minimum neighbor, update if the component of the candidate is smaller DataSet> changes = iteration.getWorkset().join(edges).where(0).equalTo(0).with(new NeighborWithComponentIDJoin()) .groupBy(0).aggregate(Aggregations.MIN, 1) .join(iteration.getSolutionSet()).where(0).equalTo(0) .with(new ComponentIdFilter()); // close the delta iteration (delta and new workset are identical) DataSet> result = iteration.closeWith(changes, changes); // emit result if (params.has(\"output\")) { result.writeAsCsv(params.get(\"output\"), \"\\n\", \" \"); // execute program env.execute(\"Connected Components Example\"); } else { System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); result.print(); } } // ************************************************************************* // USER FUNCTIONS // ************************************************************************* /** * Function that turns a value into a 2-tuple where both fields are that value. */ @ForwardedFields(\"*->f0\") public static final class DuplicateValue implements MapFunction> { @Override public Tuple2 map(T vertex) { return new Tuple2(vertex, vertex); } } /** * Undirected edges by emitting for each input edge the input edges itself and an inverted version. */ public static final class UndirectEdge implements FlatMapFunction, Tuple2> { Tuple2 invertedEdge = new Tuple2(); @Override public void flatMap(Tuple2 edge, Collector> out) { invertedEdge.f0 = edge.f1; invertedEdge.f1 = edge.f0; out.collect(edge); out.collect(invertedEdge); } } /** * UDF that joins a (Vertex-ID, Component-ID) pair that represents the current component that * a vertex is associated with, with a (Source-Vertex-ID, Target-VertexID) edge. The function * produces a (Target-vertex-ID, Component-ID) pair. */ @ForwardedFieldsFirst(\"f1->f1\") @ForwardedFieldsSecond(\"f1->f0\") public static final class NeighborWithComponentIDJoin implements JoinFunction, Tuple2, Tuple2> { @Override public Tuple2 join(Tuple2 vertexWithComponent, Tuple2 edge) { return new Tuple2(edge.f1, vertexWithComponent.f1); } } /** * Emit the candidate (Vertex-ID, Component-ID) pair if and only if the * candidate component ID is less than the vertex's current component ID. */ @ForwardedFieldsFirst(\"*\") public static final class ComponentIdFilter implements FlatJoinFunction, Tuple2, Tuple2> { @Override public void join(Tuple2 candidate, Tuple2 old, Collector> out) { if (candidate.f1 getVertexDataSet(ExecutionEnvironment env, ParameterTool params) { if (params.has(\"vertices\")) { return env.readCsvFile(params.get(\"vertices\")).types(Long.class).map( new MapFunction, Long>() { public Long map(Tuple1 value) { return value.f0; } }); } else { System.out.println(\"Executing Connected Components example with default vertices data set.\"); System.out.println(\"Use --vertices to specify file input.\"); return ConnectedComponentsData.getDefaultVertexDataSet(env); } } private static DataSet> getEdgeDataSet(ExecutionEnvironment env, ParameterTool params) { if (params.has(\"edges\")) { return env.readCsvFile(params.get(\"edges\")).fieldDelimiter(\" \").types(Long.class, Long.class); } else { System.out.println(\"Executing Connected Components example with default edges data set.\"); System.out.println(\"Use --edges to specify file input.\"); return ConnectedComponentsData.getDefaultEdgeDataSet(env); } } } 注意点 首先将每个点映射成（id，id），表示初始化每个点都是自己的连通分量。 对当前的连通分量与边进行join，得到(Target-vertex-ID, Component-ID)的pair，并保留最小的ID作为当前的连通分量。 在DeltaIteration中，将WorkSet计算得到的新的强连通分量与SolutionSet进行比较，得到changes，若changes存在（不为空），则继续迭代，同时，将changes传给SolutionSet和WorkSet。 运行 flink run -c ConnectedComponents ConnectedComponents.jar --edges /home/hadoop/Documents/distribution/Flink/ConnectedComponents/edges.txt --vertices /home/hadoop/Documents/distribution/Flink/ConnectedComponents/vertices.txt SingleSourceShortestPaths 数据准备 首先我们需要准备边和点 边： 1 2 12.0 1 3 13.0 2 3 23.0 3 4 34.0 3 5 35.0 4 5 45.0 5 1 51.0 点： 1 2 3 4 5 SingleSourceShortestPaths.java import org.apache.flink.api.common.functions.FlatJoinFunction; import org.apache.flink.api.common.functions.JoinFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.aggregation.Aggregations; import org.apache.flink.api.java.functions.FunctionAnnotation; import org.apache.flink.api.java.operators.DeltaIteration; import org.apache.flink.api.java.tuple.Tuple1; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.api.java.utils.ParameterTool; import org.apache.flink.util.Collector; @SuppressWarnings(\"serial\") public class SingleSourceShortestPaths { public static int sourceVerticeID = 1; public static void main(String[] args) throws Exception { final ParameterTool params = ParameterTool.fromArgs(args); ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet> edges = getEdgesDataSet(params, env); DataSet> vertices = getVerticesDataSet(params, env); DeltaIteration, Tuple2> iteration = vertices .iterateDelta(vertices, 100,0); DataSet> NewSolutionSet = iteration.getWorkset() .join(edges).where(0).equalTo(0) .with(new FindDistance()) .groupBy(0).aggregate(Aggregations.MIN, 1) .join(iteration.getSolutionSet()).where(0).equalTo(0) .with(new DistanceFilter()); // close the delta iteration (changes are empty) DataSet> result = iteration.closeWith(NewSolutionSet, NewSolutionSet); // emit result if (params.has(\"output\")) { result.writeAsCsv(params.get(\"output\"), \"\\n\", \" \"); // execute program env.execute(\"Connected Components Example\"); } else { System.out.println(\"Printing result to stdout. Use --output to specify output path.\"); result.print(); } } public static final class DistanceFilter implements FlatJoinFunction, Tuple2, Tuple2> { @Override public void join(Tuple2 candidate, Tuple2 old, Collector> out) throws Exception { if (candidate.f1 f0\") public static final class FindDistance implements JoinFunction, Tuple3, Tuple2> { @Override public Tuple2 join(Tuple2 vertices, Tuple3 edges) throws Exception { return Tuple2.of(edges.f1, vertices.f1 > getEdgesDataSet(ParameterTool params, ExecutionEnvironment env) { if (params.has(\"edges\")) { return env.readCsvFile(params.get(\"edges\")) .fieldDelimiter(\" \") .types(Integer.class, Integer.class, Double.class); } else { return SingleSourceShortestPathsData.getDefaultEdgeDataSet(env); } } /** * Get Vertices data * * @param params * @param env * @return */ private static DataSet> getVerticesDataSet(ParameterTool params, ExecutionEnvironment env) { DataSet vertices; if (params.has(\"vertices\")) { vertices = env.readCsvFile(params.get(\"vertices\")).types(Integer.class).map( new MapFunction, Integer>() { public Integer map(Tuple1 value) { return value.f0; } }); } else vertices = env.fromElements(1, 2, 3, 4, 5); return vertices.map(new MapFunction>() { @Override public Tuple2 map(Integer integer) throws Exception { if (integer == sourceVerticeID) return Tuple2.of(integer, 0.0); else return Tuple2.of(integer, Double.POSITIVE_INFINITY); } }); } } 注意点： 正确使用DeltaIteration，分清楚SolutionSet和WorkSet，其中，CloseWith的第一个是要merge到SolutionSet，第二个作为WorkSet。 通过比较是否有新的最短路径产生来结束循环 运行 默认数据运行： flink run -c SingleSourceShortestPaths SingleSourceShortestPaths.jar 使用指定参数运行： flink run -c SingleSourceShortestPaths SingleSourceShortestPaths.jar --edges /home/hadoop/Documents/distribution/Flink/SingleSourceShortestPaths/edges.txt --vertices /home/hadoop/Documents/distribution/Flink/SingleSourceShortestPaths/vertices.txt "},"project/giraph.html":{"url":"project/giraph.html","title":"Giraph配置及使用","keywords":"","body":"环境配置 Hadoop配置 这里使用Hadoop2.5.1进行配置，可以参考这里。 查看版本号 运行Hadoop ./hadoop-2.5.1/sbin/start-dfs.sh 查看是否成功 Giraph配置 下载Giraph cd /usr/local sudo git clone https://github.com/apache/giraph.git sudo chown -R hadoop:hadoop giraph 编译 cd giraph mvn -Phadoop_2 -Dhadoop.version=2.5.1 -DskipTests clean package 编译成功： 运行 执行最短路径程序 输入 创建/tmp/tiny_graph.txt，输入: [0,0,[[1,1],[3,3]]] [1,0,[[0,1],[2,2],[3,1]]] [2,0,[[1,2],[4,4]]] [3,0,[[0,3],[1,1],[4,4]]] [4,0,[[3,4],[2,4]]] 每一条线由[source_id,source_value,[[dest_id, edge_value],...]]构成。 并拷贝到HDFS中： cd /usr/local hadoop-2.5.1/bin/hadoop dfs -copyFromLocal /tmp/tiny_graph.txt /tiny_graph.txt 提交任务 /usr/local/hadoop-2.5.1/bin/hadoop jar /usr/local/giraph/giraph-examples/target/giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.5.1-jar-with-dependencies.jar org.apache.giraph.GiraphRunner org.apache.giraph.examples.SimpleShortestPathsComputation -vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat -vip /tiny_graph.txt -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat -op /shortestpaths -w 1 -ca giraph.SplitMasterWorker=false 查看结果 每个点离点1的最短路径： 查看代码例子 package org.apache.giraph.examples; import org.apache.giraph.graph.BasicComputation; import org.apache.giraph.conf.LongConfOption; import org.apache.giraph.edge.Edge; import org.apache.giraph.graph.Vertex; import org.apache.hadoop.io.DoubleWritable; import org.apache.hadoop.io.FloatWritable; import org.apache.hadoop.io.LongWritable; import org.apache.log4j.Logger; import java.io.IOException; /** * Demonstrates the basic Pregel shortest paths implementation. */ @Algorithm( name = \"Shortest paths\", description = \"Finds all shortest paths from a selected vertex\" ) public class SimpleShortestPathsComputation extends BasicComputation { /** The shortest paths id */ public static final LongConfOption SOURCE_ID = new LongConfOption(\"SimpleShortestPathsVertex.sourceId\", 1, \"The shortest paths id\"); /** Class logger */ private static final Logger LOG = Logger.getLogger(SimpleShortestPathsComputation.class); /** * Is this vertex the source id? * * @param vertex Vertex * @return True if the source id */ private boolean isSource(Vertex vertex) { return vertex.getId().get() == SOURCE_ID.get(getConf()); } @Override public void compute( Vertex vertex, Iterable messages) throws IOException { if (getSuperstep() == 0) { vertex.setValue(new DoubleWritable(Double.MAX_VALUE)); } double minDist = isSource(vertex) ? 0d : Double.MAX_VALUE; for (DoubleWritable message : messages) { minDist = Math.min(minDist, message.get()); } if (LOG.isDebugEnabled()) { LOG.debug(\"Vertex \" + vertex.getId() + \" got minDist = \" + minDist + \" vertex value = \" + vertex.getValue()); } if (minDist edge : vertex.getEdges()) { double distance = minDist + edge.getValue().get(); if (LOG.isDebugEnabled()) { LOG.debug(\"Vertex \" + vertex.getId() + \" sent to \" + edge.getTargetVertexId() + \" = \" + distance); } sendMessage(edge.getTargetVertexId(), new DoubleWritable(distance)); } } vertex.voteToHalt(); } } 主要就两个函数： compute 执行实际的计算 sendMessage 发送消息 "},"project/flink-iteration.html":{"url":"project/flink-iteration.html","title":"Flink迭代小记","keywords":"","body":"在Flink中，有三种迭代类型，一种对应DataStream，两种对应DataSet DataStream Iteration 这里的iterationBody接收X的输入，并根据最后的closeWith完成一次迭代，另外的数据操作Z退出迭代。 官方文档参考这里 由于DataStream有可能永远不停止，因此不能设置最大的迭代次数，必须显式的指定哪些流需要迭代，哪些流需要输出。 这时候，一般使用split transformation 或者 filter来实现。 下面定义了一个0-1000的字符流，每次迭代都减1，每个数直到小于等于1时输出（退出循环）： DataStream someIntegers = env.generateSequence(0, 1000); IterativeStream iteration = someIntegers.iterate(); DataStream minusOne = iteration.map(new MapFunction() { @Override public Long map(Long value) throws Exception { return value - 1 ; } }); DataStream stillGreaterThanZero = minusOne.filter(new FilterFunction() { @Override public boolean filter(Long value) throws Exception { return (value > 0); } }); iteration.closeWith(stillGreaterThanZero); DataStream lessThanZero = minusOne.filter(new FilterFunction() { @Override public boolean filter(Long value) throws Exception { return (value 对于迭代体iteration，每次执行map操作生成minusOne，然后将stillGreaterThanZero返回给iteration，同时将lessThanZero输出。 DataSet Bulk Iteration 可以发现与DataStream类似，但必须要迭代结束才能有输出。 同时，除了设置最大迭代次数，在closeWith中还可以添加第二个DataSet，当其为空时，则退出循环。 与流计算的区别： Input会有源源不断的来，且迭代过程中会有数据进入 Output随时都可以输出 DataSet Delta Iteration 由于在图计算中有很多算法在每次迭代中，不是所有TrainData都参与运算，所以定义Delta算子。 workset是每次迭代都需要进行更新的，而solution set不一定更新。 可以通过iteration.getWorkset/getSolutionSet得到对应的DataSet，同时分别进行计算，当nextWorkset为空时，退出循环。 在迭代过程中，WorkSet是直接替代，而deltaSet是merge。 "},"recap/fundamental-of-distributed-system.html":{"url":"recap/fundamental-of-distributed-system.html","title":"分布式系统基础","keywords":"","body":"分布式系统基础 A distributed system is: A collection of independent computers that appears to its users as a single coherent system 目标 使资源易于访问 资源的范围很广泛 打印机、存储、数据、文件等 透明 用户看到的就像是一台机器 开放 对外有统一的接口 可扩展：使得系统匹配资源规模的增长 纵向扩展scale-up:提高单台机器的处理能力 横向扩展scale-out:增加机器的数量 例子 分布式计算系统 科学计算(计算密集型) CPU是计算过程的瓶颈 数据管理(数据密集型) 数据IO是计算过程的瓶颈 分布式信息管理系统 事务处理系统 分布式普适系统 智慧家庭 传感网络 大数据处理 Volume 大量化 数据增长量大 Velocity 快速化 从数据的生成到消耗，时间窗口非常小，生成决策的时间少 Variety 多样化 大部分是非结构化数据 Value 价值化 价值密度低，商业价值高 Veracity 质量 数据的准确性和可信赖度，即数据的质量 大数据产生的原因 存储容量上升 CPU处理性能提高 网络带宽加大 人的参与 分布式数据处理系统 两大核心技术 分布式事务管理 NoSQL/NewSQL 分布式数据处理系统 批处理/流计算 分布式编程模型 分布式并行编程 MPI 容错差/扩展性差/共享式 MapReduce DAG 进程通信 线程 vs 进程 进程 系统进行资源分配和调度的一个独立单位，进程有独立的地址空间 线程 是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位 区别 一个程序至少有一个进程,一个进程至少有一个线程 线程的划分尺度小于进程，使得多线程程序的并发性高 进程在执行过程中拥有独立的内存单元， 而多个线程共享内存 线程不能够独立执行，必须依赖进程 操作系统资源分配的基本单位是进程 进程间通信方式 进程间有不同的用户地址空间 同一台机器/不同机器 通过操作系统内核实现 IPC（Inter-prodcess communication） 控制 Control Flow 信号(Signal) 由内核发送给进程 信号量(Semaphore) 进程间对共享数据的互斥访问 消息(Message)队列 两个不相关进程间，独立于发送进程、接受进程而存在 数据 Data Flow 文件(file) 管道/匿名管道(pipe) 匿名管道只能有一个方向流动，只能用于父子进程 实质上是一个内核缓冲区（独立于文件系统） 有名管道(FIFO) 有名管道的名字存在于文件系统中，内容存放在内存中 共享内存(shared memory) 需要同步机制来得到进程的同步和互斥 套接字(socket) 跨网络通信 远程过程调用 RPC 在形式上像本地函数一样去调用远程的函数 参数传递 传值调用 vs. 传址调用 如何根据函数名称进行函数调用? 反射 根据方法名调用该类 代理 让使用者在不知觉的情况下，代替调用者完成远程访问的功能 传送协议 网络传输 （TCP/IP） 数据表示 不同体系结构的机器之间 序列化和压缩 序列化与反序列化 序列化 把对象状态按照一定的格式转换成有序字节流，以便在网络上传输或者保存在本地文件中 反序列化 从文件中或网络上获得序列化后的对象字节流后，根据字节流中所保存的对象状态及描述信息，重建对象 数据压缩的原理 压缩：通过一些有别于原始编码的特殊编码方式来保存数据，使数据占用的存储空间比较小 减少网络传输的时间 压缩本身需要时间 解压缩：将被压缩的数据从特殊编码方式还原为原始数据的过程 解压需要时间 KEYS 大数据的5个V 进程和线程的区别 "},"recap/batch-system.html":{"url":"recap/batch-system.html","title":"批处理系统","keywords":"","body":"分布式文件系统 文件和目录 文件系统的主要优点是实现按名存取 如何实现？ 文件目录 文件的物理结构 顺序文件/连续存储 索引文件 链接文件 分布式文件系统 实现思路 保证每台机器可以透明的访问其他机器上的文件（P2P结构） 将所有机器的文件系统关联起来，形成一个对外统一的整体（主从结构） 通过一致性哈希/RPC实现 文件访问 主要使用Immutable files，不能修改 备份与一致性 客户端备份 客户端得到文件的副本，修改后返回给服务端 服务端备份 HDFS 假设 硬件的异常比软件的异常更加常见 应用程序关注的是吞吐量，而不是响应时间 存储的数据较大 文件仅允许追加，不允许修改 移动计算比移动数据更划算 采用流式数据访问（数据批处理） 数据块 与操作系统中block的区别 block是操作系统中读取的基本单位（总是一样大的），目的是为了省IO 在HDFS中是为了切分文件（最后的数据库可能较小） 体系架构 NameNode 负责文件系统元数据操作，数据块的复制和定位 元数据镜像文件/日志文件，保存在内存中 处理控制流 SecondNameNode NameNode的备份节点 检查点备份，不是热备份 DataNode 集群中的每个节点一个数据节点 负责数据块的存储，提供实际文件数据，保存在磁盘中 处理数据流 为什么不需要备份DataNode？ 文件访问模型 一次写入多次读写 对于单文件，不支持并发写，只支持并发读（无需文件锁，简单） 修改内容需删除，重新写入 仅允许追加（对文件增加一个block），且同时只能有一个进程操作 数据备份 为什么需要备份： 实现容错，保证数据可靠性 加快数据传输速度 实现负载均衡 容易检查数据错误 写入成功的备份是强一致的 容错机制 DataNode故障 宕机节点上面的所有数据都会被标记为 “不可读” 数据块自动复制到剩余的节点以保证满足备份因子，如默认为3 NameNode故障 根据 SecondaryNameNode 中的 FsImage 和 Editlog 数据进行恢复 MapReduce MPI 是一个信息传递应用程序接口，包括协议和语义说明 已经有MPI成熟的并行计算框架，为什么还需要MapReduce？ 传统并行计算框架容错性差，只能使用昂贵机器 编程学习很难 使用场景为计算密集型 系统架构 Client 提交作业：用户编写的MapReduce程序通过Client提交到JobTracker端 作业监控：用户可通过Client提供的一些接口查看作业运行状态 JobTracker 资源管理：监控TaskTracker与Job的状况。一旦发现失败，就将Task转移到其它节点 作业调度：将Job拆分成Task，跟踪Task的执行进度、资源使用量等信息，由TaskScheduler调度（不是一个单独的进程，是一个模块） TaskTracker 执行操作：接收JobTracker发送过来的命令并执行（如启动新Task、杀死Task等） 划分资源：使用“slot”等量划分本节点上的资源量（CPU、内存等），一个Task 获取到一个slot 后才有机会运行 汇报信息：通过“心跳”将本节点上资源使用情况和任务运行进度汇报给JobTracker 将Jar包发送到TaskTracker，利用反射和代理动态加载代码 Map slot-> Map Task Reduce slot -> Reduce Task Task 任务执行 Map task Reduce task 工作流程 只有mapper完成后Reducer才可以开始：指的是一个节点完成后，才可以和被Reducer读取，并不是所有map完成。 slot 在TaskTracker中，使用Slot划分本节点的资源（CPU，内存），一个Task获取到一个slot后才有机会运行 Map slot -> Map Task Reduce slot -> Reduce Task TaskTracker如何汇报本节点的资源使用情况？ 通过心跳将资源使用和任务运行进度汇报给JobTracker Task如何执行Map和Reduce函数？ Jar包发送到TaskTracker，利用反射和代理机制动态加载代码 数据交换 不同Map、Reduce任务之间不会发生任何信息交换 有多少个Reduce，就有多少个输出 所有的数据交换都是MapReduce框架自身去实现的（shuffle） 如何确定shuffle到哪个reduce？ 通过hash partition split 在InputFormat中将数据划分为多个逻辑单元，而block是物理概念 Hadoop为每个split创建一个map任务，split的多少决定了map任务的数目（框架决定） 如何划分 如果map task的物理机上有需要的split文件，则采取就近原则。 Shuffle过程 分为Map端和Reduce端（阻塞式的，reduce一定在shuffle后结束） Map端 溢写 分区 排序 使其局部有序 使得reduce可以直接归并，提高效率 合并（Combine） combine函数，用户定义，不一定需要 可以数据压缩，提高效率 一定溢写到磁盘（为了容错） 文件归并（merge） 归并到磁盘（为了容错，与溢写同时做） 与合并的区别 归并是将key相同的记录拼接在一起，变为list 合并是按照用户指定的函数进行数据合并 Reduce端 copy Reduce会主动来拉取写到磁盘的数据 将读来的文件进行归并 merge sort Map的输出数据已经是有序的，Merge进行一次合并排序，一般Reduce是一边copy一边sort（磁盘和内存同时使用） 容错 Task容错 Map Task失败 去HDFS重新读入数据 Reduce Task失败 从Map的中间结果中读入数据 因为要写入磁盘，因此没办法实时流计算 JobTracker失败 最严重的失败，Hadoop没有处理其失败的机制，是单点故障 所有任务需要重新运行 缺点 输入输出/shuffle中间结果要落磁盘，磁盘IO开销大 表达能力有限，只有map和reduce函数 延迟高（多个job） 在有依赖关系中，多个job之间的衔接涉及IO开销 无依赖关系中，在前一个job执行完成之前，其他job依然无法开始（不利于并行） 编程 单个mapreduce 组合式mapreduce 隐式依赖描述/显式 显式依赖描述更好，因为可以让系统知道调度信息，可进行优化（短作业优先），且可以避免上个程序运行失败导致的出错 链式mapreduce 只有一个reduce，有多个map（在reduce前后都可以） 迭代mapreduce hadoop streaming 它允许用户使用任何可执行文件或者脚本文件作为Mapper和Reducer Spark 改进 表达能力有限 增加join等更多复杂的函数，可以串联为DAG 编程模型更灵活 磁盘IO开销大(单个job) 非shuffle阶段避免中间结果写磁盘 迭代运算效率更高 延迟高(多个job作为一整个job) 将原来的多个job作为一个job的多个阶段 有依赖关系:各个阶段之间的衔接尽量写内存 无依赖关系:多个阶段可以同时执行 RDD抽象 弹性分布式数据集，这里的弹性主要指容错 是分布式内存中的一个抽象概念，提供了一种高度受限的共享式内存模型 具有可恢复的容错特性 每个RDD可分成多个分区，一个RDD的不同分区可以保存到集群中的不同节点上 每个分区都是一个数据集片段 特性 只读（immutable） 本质上是一个只读的对象集合，不能直接修改，只能基于稳定的物理存储中的数据集创建RDD 支持运算操作 转换（transformation）：描述RDD的转换逻辑 动作（action）：标志转换结束，触发DAG生成 惰性求值：只有遇到action操作时，才会发生真正的计算 RDD Lineage 指DAG的拓扑结构 RDD经过一系列的transformation操作，产生不同的RDD，最后一个RDD经过action进行转换，并输出 保留RDD lineage的信息，为了容错 RDD依赖关系： 宽依赖：存在一个父RDD的一个分区对应一个子RDD的多个分区，容错需要使用整个父RDD，将子RDD重新算一遍 窄依赖：一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区，容错仅需要部分父RDD将丢失部分的子RDD计算一遍 为什么关心依赖关系？ 分析各个RDD的偏序关系生成DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage 具体划分方法 在DAG中进行反向解析，遇到宽依赖就断开 遇到窄依赖就把当前的RDD加入到Stage中 将窄依赖尽量划分在同一个Stage中，可以实现流水线计算 pipeline stage 类型 ShuffleMapStage 不是最终的stage，输出一定经过shuffle过程，并作为后续stage的输入 ResultStage 输出直接产生结果或存储，在一个job最终必定含有该类型 体系结构 driver -> spark master -> spark worker -> executor -> task Master负责 集群资源管理（也可以使用yarn） Worker 运行作业的工作节点，负责执行的进程（executor）和线程（task） 作业与任务 Job -> DAG 包含多个RDD及对应RDD的各种操作 stage -> TaskSet 一个job会分成多组task，每组task被称为stage 是job的基本调度单位，代表了一组关联的，相互没有shuffle依赖关系的任务组成的任务集 RDD -> task 表示运行在executor上的工作单元 运行流程 Driver向集群管理器申请资源 集群管理器启动Executor Driver向Executor发送应用程序代码和文件 Executor上执行Task，运行结束后，执行结果 会返回给Driver，或写到HDFS等 运行特点 Application有专属的Executor进程，并且该进程在Application运行期间一直驻留 Executor进程以多线程的方式运行Task Spark运行过程与资源管理器无关，只要 够获取Executor进程并保持通信即可 Task采用了数据本地性和推测执行等优化机制 推测执行将较慢的任务再次在其它节点启动 executor相比mapreduce的优点： 利用多线程来执行具体任务，减少任务的启动开销 将内存和磁盘共同作为存储设备，减少IO开销 Spark Shuffle 在stage之间是shuffle 在stage内部是流水线pipeline consolidateFiles 若设置为true，则表示将一个核上的多个小文件变为大文件进行传输，提高效率 spark pipeline 可以在内存中，shuffle必须要压磁盘 Spark的主要改进 算子的扩充（join） 避免stage内部的shuffle写磁盘 迭代时，每一次不必写磁盘，只是RDD的转换 容错 Master故障：没有办法（zookeeper） Worker故障（因为Task已经是线程，所以不需要容错处理） Lineage机制 重新计算丢失分区（宽依赖/窄依赖），无需回滚系统 重算过程在不同节点之间并行，只记录粗粒度的操作 RDD存储机制 RDD提供持久化接口 没有标记持久化的RDD会被garbage collection回收 检查点机制 Lineage可能非常长 RDD存储机制主要面向本地磁盘的存储 检查点机制将RDD写入可靠的外部分布式文件系统，例如HDFS 只写入指定的RDD SQL Shark & Spark SQL RDD的局限性 对于对象内部结构，即数据schema不可知 DataFrame 无论读取什么数据，都写成DataSet\\ (泛型) 对于SQL语句，只是一条string语句，在编译时不会报语义错误 DataSet 明确声明类型DataSet\\ 若访问不存在的列，在编译时会报错 KEYS DFS DFS中的block与FS中的block的区别 DFS中为什么需要备份 HDFS中的容错机制 Hadoop 已经有MPI成熟的并行计算框架，为什么还需要MapReduce？ 只有mapper完成后Reducer才可以开始 如何确定数据shuffle到哪个reduce？ 如何划分split？ shuffle在map端和reduce端的操作 合并（conbine）与归并（merge）的区别 reduce输出是否需要排序 为什么mapreduce不能实时流计算？ MapReduce执行过程图 设计理念为计算向数据靠拢 Spark 线程、进程 除了mapreduce，其余都是多线程实现 线程更轻量化 多核CPU可以同时运行多线程（CPU同时只能运行一个进程） 进程容易debug RDD 运算操作 Lineage 宽依赖/窄依赖（有什么用？1.用来划分stage，2.用来容错） 窄依赖 表现为一个父的RDD分区对于于一个子RDD的分区 或多个父的RDD分区对应于一个子的RDD分区 宽依赖 表现为存在一个父RDD的一个分区对应一个子RDD的多个分区 为什么关心依赖关系？ spark体系结构与执行过程图 Spark中的线程是什么，进程是什么？ spark中job/stage/task对应关系（逻辑执行角度/物理执行角度） spark优化机制 spark中的3种容错方式及优缺点 什么时候会将RDD Repartition？ 当Join操作为宽依赖，可以经过partition之后变为窄依赖，pipeline执行 DataFrame和DataSet的区别 "},"recap/management-system.html":{"url":"recap/management-system.html","title":"支持数据管理的底层系统","keywords":"","body":"Zookeeper 背景 在传统的HDFS1.0中，单点故障是使用SecondaryNameNode，主要用途： 冷备份 主要是防止日志文件过大，导致NameNode节点恢复时时间过长 使用Zookeeper做到高可用 NameNode之间的数据同步 NameNode之间的状态感知（一旦Active出现故障，立即切换Standby） 在配置多个NameNode时候，谁会成为Active？ 由于在动态变化，不能写入配置文件中 体系架构 ZooKeeper：轻量级的分布式系统 作用：用于解决分布式应用中通用的协作问题 命名管理 Naming 配置管理 Configuration Management 集群管理 Group Membership （监控节点的状态） 同步管理 Synchronization 角色 server：负责管理元数据 leader：某一个server，保证分布式数据一致性的关键，何时需要选取leader？ 服务器初始化启动 服务器运行期间无法和Leader保持连接 client：需要进行协作的用户（系统） 提供元数据 获取元数据，并执行相关操作 数据模型 Znode 不保存数据，保存元数据或配置信息 可以保存时间戳进行版本控制 类型：常规/临时 标识：sequential flag client 可以在某个Znode上设置一个Watcher，来检测该Znode上的变化 server 当这个Znode中的存储数据修改/子节点目录变化，可以通知设置监控的客户端 Znode 分为四种类型 是否会自动删除 常规/临时（在Session结束后自动删除） 临时节点不能有子节点目录 是否带有顺序号 Znode是有版本的，每个Znode中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据 应用 命名管理 统一命名服务:分布式应用通常需要一套命名规则，既能够产生唯一的名称又便于人识别和记住（如访问分布式中的各个节点） 动态配置管理 如用户命令行修改了默认的参数（master节点IP，worker节点数量，webport） 配置信息存在 ZooKeeper 某个目录节点，所有机器watch该目录节点 一旦发生变化，每台机器收到通知，然后从 ZooKeeper获取新的配置信息应用到系统中 集群管理 动态感知机器增减 创建临时文件，当新建文件时，个数发生变化，减少文件时，由于是临时的，文件数减少 选主 注意这里的选主与zookeeper本身选主（基于一致性协议）的区别 创建一个临时有序的文件，每次选择序号最小的 共享锁 若所有节点都监控一个Znode，则Zookeeper需要发出大量的事件通知 每次lock都需要发出很多通知，但只有一个能拿到锁，代价很大 羊群效应 进行分组处理 对每个需要拿锁的client创建一个临时有序的Znode，每次取序号最小的 若自己是最小的，则拿到锁，否则监控比自己序号小的一个Znode 可以实现负载均衡 队列管理 同步队列 双屏障：允许客户端在计算的开始和结束时同步。当足够的进程加入到双屏障时，进程开始计算。当计算完成后，离开屏障（BSP model中使用）。 使用ready和process节点实现 每个进程到ready中注册，达到要求后到process中建立节点 为什么需要两个节点 若只有ready，迭代一次删除，但循环没有终止，还需要注册 若在做时有新节点加入，则不知道什么时候结束（本来的判断标识为集合为空） YARN 在MapReduce系统中，JobTracker需要进行资源管理和任务调度 存在单点故障的风险 内存开销大 资源分配只考虑MapReduce任务数量，不考虑任务需要的CPU/内存，TaskTracker所在节点容易内存溢出 资源划分不灵活 资源管理不单是MapReduce系统所需要的，而是通用的 体系结构 ResourceManager 处理客户端请求 应用程序管理器：启动/监控ApplicationMaster，监控NodeManager 资源调度器：资源分配与调度 ApplicationMaster 与RM进行交互：为内存应用申请资源并分配给内部任务 任务调度/监控与容错 NodeManager 单个节点上的资源管理，处理来自ApplicationMaster/ResourceManager的命令 只负责与容器有关的事情，不负责具体每个任务自身的状态管理 Container 运行任务 两个主从关系 RM -> NodeManager 资源管理 AM -> Container 计算，任务实际上向AM汇报自己的状态和进度 发展目标 实现集群资源共享和资源弹性收缩 提高集群利用率 不同计算系统可以共享底层存储，避免了数据集跨集群的移动 KEYS Zookeeper 在配置多个NameNode时候，谁会成为Active？ zookeeper的主要作用 在zookeeper中谁时client？ Znode的四种类型 羊群效应 如何实现双屏障，什么系统需要双屏障？BSP Model YARN 体系结构中的两个主从关系 YARN多租户部署 增强集群的资源利益率 减少维护代价 YARN发展目标 "},"recap/streaming-system.html":{"url":"recap/streaming-system.html","title":"流处理系统","keywords":"","body":"流计算系统 需求 静态数据 -> OLAP 流数据 数据快速持续到达 数据量大，但不十分关注存储 注重数据的整体价值，不过分关注个别数据 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序 应用 实时广告更新 实时交通路况 概述 流计算特征 乱序：数据记录的原始顺序和在处理节点上的处理顺序可能不一致（数据传递也可能导致顺序改变） 延迟：数据记录的产生时间和在处理节点上的处理时间可能差别很大 无界：数据记录在计算过程中不断动态到达 基本概念 State（状态） 两种算子 stateless streaming process: filter / map stateful streaming process: wordcount/ groupbykey Processing Element（PEs）：基本的运算单元（函数） 接收并产生新数据 每个数据只能算一遍，不能倒回去 Summary States：由于流计算不能存储所有数据，因此存放一个梗概（一种数据结构） 例如样本/直方图/状态机/平均数/方差 Window（窗口） Range：将多久的数据进行计算 Trigger/Slide：每隔多久激发计算 滑动窗口：Range > Slide 每隔20个记录过去60个 滚动窗口： Range = Slide 每隔20个记录过去20个 跳跃窗口： Range Session Window：由于不知道开始时间和结束时间，很难用批处理的方式解决（在Session结束时断开） 自定义Window操作 Window Assigner（Range）：负责将什么元素分配到不同的Window Trigger（Slide）：触发器 Evictor（可选）：可以在触发器之后，计算之前移除某些数据 Time（时间） Processing Time：在每个PE上处理的Local时间，系统可以把握 Event Time：事件发生时间（更重要），确定时间，系统不能把握 可以无序的到达 在消息传递Kafka中可能导致交错 在不同PE进行shuffle时可能导致交错 如何根据Event time进行window操作呢？ Watermarks Low WaterMakes 表示所有数据都到达的一个时间点，表示系统可以确认该时间之前的数据全部已经收到并完成处理 系统任务lwm之前的数据全部已经收到并完成处理 Wall time：系统的当前当前时间，与Processing Time 是一样的 计算方式（若C是A的上流节点） low watermark of A = min (oldest work of A , low watermark of C ) 局限性 在一定程度上保证数据的完整性，时效性 无法完全避免数据比lwm晚到到达 比如数据在没有进入系统之前就延误了，lwm根本不得而知 不同系统处理方式不一样（超时机制） Ingestion Time a hybrid between processing and event-time 当数据没有Event time时使用，记录进入系统的时间，用来模仿origin time 并行化 几种并行化的方式： 为什么需要并行化 数据的计算（process）跟不上数据的产生 状态（State）太大，不能存在一个节点（graph） 数据已经进行了partition（kafka） Hadoop 与流计算 设计初衷是面向大规模数据批量处理，适合处理静态数据 可以使用小批量处理来降低批处理的时间延迟，但也无法有效处理流数据 如何处理片段之间的依赖关系（window跨batch，如何做？） 性能太差 Storm Storm将流数据Stream描述成一个无限的 Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理 从逻辑上看，Storm的tuple类似于关系数据库中的tuple 这些字段是否需要存储？ 不需要，在开始之前就已经定下来了 系统构成 Spout 是Stream的源头，从外部数据元读取数据，然后封装成tuple形式，发送到stream中 是主动的角色 Bolt 描述stream的转化过程 是一个被动的角色 Topology 描述Spout和Bolts组成的网络 节点：Spout或者Bolt（描述处理逻辑） 边：Bolt订阅哪个Stream（数据流动方向） 系统架构 采用主从方式 Nimbus：主节点运行的后台程序，负责分发代码，分配任务和检测故障 Supervisor：从节点运行的后台程序 负责监听所在机器的工作，决定是否启动或运行Worker进程 Zookeeper 负责Nimbus和Supervisor之间的所有协调工作（没有Zookeeper无法运行） 同时可以保证高可用（Nimbus进程或Supervisor终止，重启时也能读取恢复之前的状态） Worker（进程，与MapReduce更相似） 对Spout或Bolt运行一个或多个executor线程来提供task的运行服务 executor：线程，执行同一个组建的一个或多个task task：实际数据处理（Spout/Blot） Supervisor -> 多个worker（线程） -> 多个executor（线程）执行 task storm与MapReduce/Spark的系统架构比较 工作流程 系统只负责Continous execution： 让系统运行 重新调度失败的worker 将状态（State）交给用户来管理，无论用户将State存在哪（外部数据库/堆栈） 工作流程 客户端Topology生成后，提交给Nimbus Nimbus创建一个configuration（一种作业的调度逻辑，将Topology分片成task），然后原子性的将task和supervisor相关信息写入zookeeper Supervisor会去Zookeeper上认领自己的Task，通知自己的Worker进程进行Task的处理 worker启动，开始运行。 Stream Grouping 每一个Spout和bolt都可以是多个分布式task来执行，Stream Grouping用来决定task如何分发一组Tuple ShuffleGrouping：随机分组，保证每个bolt的task接收tuple的数量大致一致（负载均衡） FieldsGrouping：按照字段分组，将相同字段分到同一个Task中 AllGrouping：广播发送（信息会重复发送） NonGrouping：不分组，当前Task的执行会和它的被订阅者在同一个线程中执行（优化时使用） MessagePassing Tuple处理完即可发送：无阻塞、符合实时计算要求 一次一记录：一次发送一个tuple 与批处理的不同点： MapReduce和Spark的shuffle阶段存在阻塞 数据传输是成块进行的 容错机制 Nimbus：使用zookeeper重启 Supervisor：使用zookeeper重启所有节点（重新调度） Worker：Supervisor重启 重启之后怎么办？使用消息重放和消息应答机制 当每个bolt接收到数据时，会进行应答，回复成功（acknowledge）或者失败（fail） 问题，会重复计算（At least once） Storm改进 制约原生native Storm容错机制的因素 无状态:中间结果无法重放 引入状态 一次一记录传输方式:元数据量太大 小批量传输 使用exactly-once semantic Spark Streaming 使用一次一记录的方式存在的问题： 每个节点都是可修改的状态 对于每个记录，都需要更新状态和发送数据 若节点丢失，状态丢失/需要将状态进行容错 主要思想 将输入数据变为batches batch的大小很关键，太小的话效率不高，太大的话延迟很高 理想的大小：batch刚刚到达，之前的已经处理好了 对每个batch看成是RDD，对每个RDD的操作用来模拟continous execution Micro - Batch 使用processing time来获得Micro - Batch DStream：一系列RDD DStream将输入数据流进行切割（根据用户指定的时间间隔） 维护一系列RDD的信息 大部分操作与RDD类似 体系架构 在Spark Streaming中，会有一个组件Receiver，作为一个长期运行的task跑在一个Executor上 Input DStream: 从外部数据源获得的流数据的抽象表示 (例如从文件中读取数据的文件流，套接字流，或者从Kafka中读取的输入流等) 每个Receiver都会负责一个Input DStream，从而与外部数据源进行连接，读取相关数据 工作流程 Reciever 实现从数据源读取数据的逻辑（接收到Spark系统中） 负责将读取到的数据存储到Spark的内存中 Input DStream 表示从数据源读取数据的DStream 关联一个Reciever 分发到executors上并长时间执行 DStream 状态（State） UpdateStateByKey: 允许用户维护任意的状态信息，并根据新数据更新状态 Transformation：与Spark一样 窗口 用户可以指定窗口时间间隔/大小 Dstream窗口操作优化 reduceByKeyAndWindow 通过提供的invFunc提供流数据“逆操作”，从而达到更高的效率 适用范围：滑动式窗口，slide 增量式窗口操作：上一时刻的值 - 没有的部分 + 新增的部分 时间 采用processing time 容错 worker： RDD可以复制到多个节点中的内存 所有的transform操作都是容错的，有exactly-once语义 master： 保存DStreams的状态到一个检查点文件（保存到HDFS），用来恢复 KEYS 流计算系统概述 流计算基本特征及应用 流计算的三大基本概念 流计算窗口类型 lwm 概念，计算 局限性 ingestion time 两种流计算中常见的优化 父子节点放在一起(Producer-Consumer Fusion) 兄弟节点放在一起(Siblings Fusion) Storm Storm里面的topology可以是环状图 spout是主动拉去数据/Bolt是被动的角色 tuple中的字段是否需要存储？ Continuous exectuin & continous query（SQL） Storm中的系统架构，与hadoop/spark对比，Worker是进程还是线程？ Storm的工作流程（zookeeper发挥的作用：充当了任务调度） 在Storm中，没有zookeeper无法运行 StreamGrouping的方式 Storm的消息传递方式，与批处理的不同点（2点），重要 Storm的容错机制以及at least once 在Storm中，每一个worker都执行executor，但在Spark中，Executor中的一部分用来执行task？ Spark Streaming Micro-Batch的time是什么？ Reciever和Input DStream的区别 DStream时间/窗口/状态 SparkStreaming是exactly once吗？ "},"recap/flink.html":{"url":"recap/flink.html","title":"批流融合系统","keywords":"","body":"批流融合系统 需求 某些场景可能既有大批量数据，又有快速产生数据，某些数据的实时要求性高（如广告投放） 离线训练/实时决策 Lambda架构 组成 主要分为Batch Layer/Serving Layer/Speed Layer Batch Layer 全量计算，存储MasterDataset，不变的持续增长的数据集 Serving Layer 对batch view进行操作，从而为最终的实时查询提供支撑 对Batch view的随机访问/更新batch view 这两个Layer缺点是：在运行预计算的数据不会马上到batch view中，不能保证实时性 Speed layer 增量计算：只处理最近的数据，更新 realtime view 当进行查询时，会同时落在两个View中（两边的最终结果应该一样），最后将查询结果进行merge batch view & realtime view,最终batch layer会覆盖speed layer 优缺点 优点 平衡了重新计算和延迟的矛盾 缺点 运维的复杂性：同时维护两套系统（容错/bug也需要两套） 开发复杂：需要将所有的算法实现两次，批处理系统和实时系统分开编程，还要求查询得到的是两个系统结果的合并 融合 一体化系统：Spark Structured Streaming/Flink 批处理优先/流处理优先 统一编程：Beam Flink 系统流程 当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Client Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。 任务 将用户程序翻译成逻辑执行计划(类似sparkDAG) 逻辑执行计划的优化 JobManager JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 任务 管理/调度/协调TaskManager TaskManager TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。 任务 与JobManager、TaskManager保持通信 启动Task线程，实际执行任务 相同节点或不同节点上Task线程进行数据交换 Graph 批处理:Program → BatchGraph → Optimized BatchGraph → JobGraph （基于规则/代价的优化）→ ExecutionGraph（考虑并行的问题） 流计算:Program → StreamGraph → JobGraph（chaining优化）→ExecutionGraph（考虑并行的问题） StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。 JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。 ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。 涵义 流计算 批处理 部件 用户程序 Stream API程序 Batch API程序 Client 程序的拓扑结构 StreamGraph DAG/Optimizer Client 逻辑执行计划 JobGraph JobGraph Client 物理执行计划 ExecutionGraph ExecutionGraph JobManager 物理实现 分布式执行 分布式执行 TaskManager 流水线机制 数据传输方式 批处理：一次一个Table Storm：一次一个Tuple Flink：一次一个Vector 尽可能采取流水线方式，一旦buffle满了（批处理）或超时（流计算），就向下游进行发送 在流计算模式中：完全采用流水线机制 批处理中：除了pipeline breaker外采用流水线，如排序，hash table（不是宽依赖的意思） 在Spark中，遇到Shuffle则阻塞（但并不一定需要阻塞，只是为了容错） Iteration 在Flink中，有三种迭代类型，一种对应DataStream，两种对应DataSet DataStream Iteration 这里的iterationBody接收X的输入，并根据最后的closeWith完成一次迭代，另外的数据操作Z退出迭代。 由于DataStream有可能永远不停止，因此不能设置最大的迭代次数，必须显式的指定哪些流需要迭代，哪些流需要输出。 这时候，一般使用split transformation 或者 filter来实现。 DataSet Bulk Iteration 可以发现与DataStream类似，但必须要迭代结束才能有输出。 同时，除了设置最大迭代次数，在closeWith中还可以添加第二个DataSet，当其为空时，则退出循环。 与流计算的区别： Input会有源源不断的来，且迭代过程中会有数据进入 Output随时都可以输出 DataSet Delta Iteration 由于在图计算中有很多算法在每次迭代中，不是所有TrainData都参与运算，所以定义Delta算子。 workset是每次迭代都需要进行更新的，而solution set不一定更新。 与批处理迭代的区别 在Hadoop/Spark中，迭代都是用户编写的程序，每次迭代时独立的job 在Flink中，是系统内置的，可以提供优化 Table&SQL Table API 与SQL基本相同，只是在SQL中，由于是字符串语句，在编译时无法查错。 批流等价转换 主要是想要让SQL在Stream上运行。 为了实现Contiuous Query，Flink设计了Dynamic Table对Stream进行相互转换。 Stream → Dynamic Table Append mode 很简单，直接添加一项即可 Upsert mode 需要将相同key的值更新或者插入 同样支持SQL和Window操作 Dynamic Table → Stream 为了容错，使用Undo/Redo日志 容错机制 JobManager 故障-> Zookeeper TaskManager 故障 -> 主要考虑Task的故障（由于迭代操作是内置的，因此需要系统考虑） 使用checkpoint的方式 非迭代过程的容错 对于批处理和流计算是一样的（主要考虑State） Pipelined Snapshots Synchronous Snapshots 会将数据切分成好几份，保存State 会造成不必要的延迟，且造成阻塞 Pipelined Snapshots Light checkpoint/Distributed snapshots/Exactly-once guarantee/异步的同步 不要求Task停下来，遇到标记，进行记录 迭代过程的容错 流式迭代 属于有环的Pipelined Snapshots 将迭代的数据看作虚拟的输入 批式迭代 特点：循环不结束，没有输出 两种CheckPointing方法： Tail checkpoint/Head checkpoint Spark Structured Streaming 流向表转换 Unbounded Table event time作为表中的列加入到window运算/引入流水线机制 依然没有定义session window 流与表的统一到DataSet/DataFrameAPI 底层引擎依然是批处理，继续使用micro-batch模型 Beam 需要格外注意什么？ 统一API（会不会造成性能差异） 统一编程（底层的两个系统如何统一） WWWH模型 What results are calculated? 计算什么结果? (read, map, reduce) 批处理系统可实现 Where in event time are results calculated? 在哪儿切分数据? (event time windowing) Windowed Batch When in processing time are results materialized? 什么时候计算数据? (triggers) Streaming How do refinements of results relate? 如何修正相关的数据（对于Delay的数据）?(Accumulation) Streaming + Accumulation BeamPipeline 数据处理流水线 表示抽象的流程 与“Flink流水线机制”不是一个概念 KEYS 批流融合系统 大数据的5V Lambda架构组成 Lambda优缺点 Flink 逻辑执行计算的优化在哪里完成？ 系统架构与MapReduce/Spark/Flink的比较，进程和线程的对比 Flink中的图，每一步在干什么 所有的图（JobGraph/BatchGraph/Optimized BatchGraph/SteamGraph）都是有向无环图 Flink流水线机制（批处理和流计算中的不同点），与Spark中选择Pipeline的标准不同 在Table&SQL中，若定义的环境为：StreamTableEnvironment，则输入的数据不一定非要是Streaming表，也可以是静态的，这是因为在Flink的定义中，batch是Streaming的特殊情况；反过来，在BatchTableEnvironment中，不能输入流式数据 SQL与Table API的区别：SQL语句在编译时无法查错 Flink非迭代式同步的主要方法（批处理和流计算一样吗？） Spark v2 主要改进 Deam 主要需要考虑的问题 WWWH模型 BeamPipeline "},"recap/graph.html":{"url":"recap/graph.html","title":"图处理系统","keywords":"","body":"图数据系统 使用单机的图算法库，再实现分布式算法 通用性不好 并行图计算系统 对大规模分布式系统容错没有很好的支持 现有分布式数据处理系统进行图计算 编程麻烦/未对迭代进行优化/编程模型不够直观 Pregel 利用MapReduce框架/不是基于MapReduce API计算 图算法共性：顶点给顶点给邻居传递消息，不断进行更新，此过程迭代，直到最终收敛 集中式算法:限制参与运算的顶点，例如 Dijkstra总是挑最近的顶点加入source 分布式算法:所有顶点同时参与运算 BSP Model 一系列全局超步（superstep） 局部计算：每个参与的处理器独立计算（不依赖其他节点） 通讯：处理器群相互交换数据（顶点间数据交换） 栅栏同步(Barrier Synchronization)：等到其他所有处理器完成计算，再继续下一个超步（可使用zookeeper） Vertex-centric计算模型 “边”并不是核心对象，在边上面不会运行计算，只有顶点才会执行用户自定义函数进行相应计算 顶点的状态 活跃active：该顶点参与当前的计算 非活跃inactive：该顶点不执行计算，除非被其他顶点发来的消息激活 当一个非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架根据条件判断是否将其显式唤醒进入活跃状态 什么时候结束 当所有顶点都是非活跃状态的时候，并且没有消息在传递 编程模型 Compute()：计算函数 SendMessageTo()：消息传递给哪些节点 Combiner()：可选，往同一个顶点的多个消息合并成一个，减少了传输和缓存的开销 Aggregator()：可选，一种全局通信/监控和数据查看的机制，该数据所有节点都可以见，例如PageRank中的N值/求图中的边数 VoteToHalt()：进入非活跃状态 体系结构 Master：协调各个Worker执行任务，对外服务，协调计算（同步控制） Master维护的数据信息大小，只与分区的数量有关，与顶点和边的数量无关 Worker：维护图的状态信息，负责计算（一个woker有多个节点）， BSP计算模型 计算：worker自身 通讯：worker之间 同步：master Worker内部 维护图顶点的描述信息（相邻顶点/边的长度），保存在内存中！ 执行图顶点上的计算：Compute() 管理图顶点的控制信息 需要存两份（接收的队列，发送的队列）！ 输入消息队列：接收到、发送给顶点的消息， S已经接收到的消息（来自于S-1），S中需要处理 S中接收到来自其他Worker的消息，S+1处理 标志位：用来标记顶点是否活跃状态 S中标记顶点是否活跃 S中记录顶点在S+1是否活跃 Worker之间 消息传输：SendMessageTo() 发送消息前会首先判断目标顶点U是否位于本地（根据内部描述信息），若在远程，使用partition function确定在哪个节点 本地：直接把消息放入到与目标顶点U对应的输入消息队列中 远程：暂时缓存消息到本地输出消息队列，当缓存中的消息数目达到阈值时，传输到目标顶点所在的Worker上 若存在用户自定义的Combiner操作，则消息被加入到输出队列或者到达输入队列时，就可以对消息执行合并操作 MapReduce与Giraph Giraph的主从结构在MapReduce中都是Map Task 利用zookeeper选主 只利用了MapReduce中的mapper节点，没有Reduce节点 只是利用了MapReduce框架Run函数将Giraph启动 数据加载 Master：只需要知道图的位置 将输入的图划分为多个部分，为每个Worker分配一部分数据 Worker：实际读取数据到内存 超步结束 在执行过程中，顶点可以对外发送消息，但必须在本超步结束之前完成 超步完成后，Worker把在下一个超步还处于“活跃”状态的顶点数量报告给Master 这里可以使用Aggregator函数执行 容错机制 不能使用MapReduce的容错机制 检查点机制 Master故障用zookeeper恢复 Worker发生故障：使用检查点机制 Pregel 重新启动一个Worker 局部恢复（confined recover） 将失效节点从检查点恢复到当前时刻 Giraph Master把失效Worker的分区分配到其他处于正常状态的Worker上 全局恢复，所有节点退回到检查点 乐观容错 不一定需要检查点 KEYS BSP Model Vertex-centric计算模型/什么时候算法结束？ Pregel不是基于MapReduce API计算 图算法共性 Aggregator（）的应用？（worker汇报活跃顶点数量） worker保存什么信息？保存在哪里？保存几分？ pregel图处理的数据结构更像临接链表 Master节点维护什么信息？大小与什么有关？ Pregel与MapReduce的关系 Hama是基于MapReduce API，与Giraph不同 Worker是线程执行 Giraph迭代结果保存在哪？（内存） Giraph的容错机制可以使用MapReduce吗？ Map Task仅用于启动Giraph Worker，并非执行map()函数 如果类似mapreduce那样在map()计算结束时 写入磁盘，那么Giraph的计算也已经结束了 如果每个superstep像map那样写磁盘（本地），那么得到的是本地的备份 会画Pregel图基本算法的流程图（单源最短路径/二分图匹配） "},"recap/ml.html":{"url":"recap/ml.html","title":"机器学习系统","keywords":"","body":"机器学习系统 机器学习三要素 模型 用参数来构造描述客观世界的数学模型 线性回归模型 对于参数很多的模型，还需要考虑参数容错（LDA） CNN/Tensorflow/Parameter Server -> 适用于从参数入手 策略 基于已有的观测数据进行经验风险最小化 选择什么参数在代价函数中是最好的？（例如，训练数据的误差平方和） 对于数据之间有关联的算法（PageRank）/GraphLab -> 适用于从数据入手 算法 优化问题的计算求解 适用于Model较为简单，数据没有关联性/Mahout -> 适用于从计算入手 梯度下降 要求必须每次迭代同时更新参数 与BSP模型很像 在某些算法中可以异步更新（系统：GraphLab） 机器学习系统设计思路 机器学习 强调机器学习方法的精确程度 评价标准:accuracy/precision/recall 机器学习系统 强调机器学习过程的运算速度 评价标准:performance 机器学习 机器学习系统 模型 参数 策略 数据 算法 计算 思考 应该从哪个角度来设计机器学习系统？ 考虑模型的复杂度/数据关联性/参数个数 分别从参数/数据/计算入手 哪些机器学习过程适合以计算为中心? 线性回归 哪些机器学习过程适合以数据为中心？ PageRank 哪些机器学习过程适合以参数为中心？ LDA/CNN Mahout 以计算为中心的机器学习系统 MapReduce框架不适合作为机器学习计算 迭代耗时，编程模型很难使用 缺乏声明式 使用Spark/Flink作为底层系统 R/Matlab-likeDSL for declarative implementation of algorithms 自动调优 矩阵平方 分别对行和列进行partition，再分别相乘（Partition），最后相加 对于slim矩阵，可以忽略最后的patition，因为最终的矩阵很小 GraphLab 以数据为中心的机器学习系统 我们之前已经有了基于BSP Model的Pregel，其主要特点是需要进行同步（双屏障），而同步是由最慢的节点决定，造成： 资源的浪费（大部分节点会等待少部分节点收敛） 某些算法可能并不需要同步更新 某些机器学习计算的特点 异步迭代（Asynchronous Iterative） 参数不一定需要同步更新 动态计算（Dynamic Computation） 某些参数可能快速收敛 在Pregel中可以通过设置inactive实现 可串行化（Serializability） 计算过程存在一定的顺序依赖关系，或者顺序计算的效果(收敛速度、精确度)更 BSP model中简化了这个问题，但可能影响迭代次数 因此，在GraphLab中，针对这三点分别进行了改进： 异步迭代 Pull model Update function 动态计算 Pull model 可串行化 Scheduler Consistency Model 计算模型 在Giraph中是节点主动发送消息，获取消息是一个被动的过程，Push model 而在GraphLab中，是主动拉去消息，Pull model 异步迭代 Pregel:不支持，BSP模型是同步迭代 GraphLab:通过pull model可以支持异步 动态计算 Pregel:根据判定条件让某些顶点votetohalt，但是后续可能还会收到消息 GraphLab:根据判定条件停止计算 updatefunction，不再pull消息 调度 与事务处理很像，如何实现？ Consistency Rules Full Consistency 最强的一致性，但并发度很低 Edge Consistency 只对边和中心点进行一致性保证 Vertex Consistency 只对顶点进行一致性保证 Parameter Server 设计思路 参数与训练数据分开存放 Server:负责参数 Worker:负责训练数据 取参数是按需Pull，再将更新后的参数放回去 提供同步计算与异步计算模式 灵活的consistency 用户选择 参数看作key-value pair进行备份 Consistent hashing 计算模式 Pull/Push/User defined functon 提供同步（Sequence）和异步（Eventual）的迭代 Trade-off between Algorithm Efficiency and System Performance 计算考虑 异步可能是错的 性能方面 异步更好 容错机制 使用一致性哈希和备份的方式 为什么不能用zookeeper？ 数据量太大 前提：这个model是key-value pair，才能够被hash KEY 机器学习系统 机器学习三要素及对应系统 应该从哪个角度来设计机器学习系统？ 哪些算法适合什么系统实现？ GraphLab 与Giraph的区别？（拉去消息/迭代计算/终止条件） 如何实现调度？Consistency的方式有哪些？ GraphLab vs. DB transactions Consistency类似事务可串行化中的什么概念? 隔离性 Pregel vs. GraphLab BSP模型是哪种consistency? Full consistency Pregel的vertex-centric编程模型服从vertex consistency model吗? 服从 PS PS中的参数和数据存在哪里？ 在PS中，如何得到想要的参数？ GraphLab和PS中sequential一样吗? GraphLab强调数据点之间的顺序计算关系 PS不考察训练数据点之间的关系，强调多次迭代之间的顺序关系 GraphLab中的consistency和PS中的 consistency是一样的吗? GraphLab中的consistency解决可串行问题 PS中的consistency解决同步/异步计算问题 "},"recap/summary.html":{"url":"recap/summary.html","title":"总结与对比","keywords":"","body":"系统之间的比较 MapReduce VS Spark Spark增加了 join 等更多复杂的函数，可以串联为DAG，编程模型比MapReduce更灵活 Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制，无依赖关系的任务之间可以同时进行运算 MapReduce实际执行任务的是 TaskTracker 中的 task（进程），而 Spark 实际执行任务的是 worker --> Executor 中的task（线程） 本质区别 迭代 Spark 提供了内存计算，整个迭代是作为一个 Job 运行的，可将迭代过程中的结果放到内存中，避免了从磁盘频繁读取数据 MapReduce 每一个迭代期都是一个 Job，迭代的中间结果要存入HDFS，效率远远低于Spark 容错 Spark 的容错通过 RDD 和 Lineage 实现 MapReduce 的容错通过 Map 端落盘实现 批处理系统 VS 流处理系统 输入的数据不同 批处理的输入数据是批量的、静态的、完整的、有界的 流处理的输入数据是无界的、动态的、乱序、延迟 数据 需求 批处理 静态、有限 非实时 流计算 动态、无限 实时 实现目标 批处理是对历史数据的挖掘 流处理是对实时数据价值的挖掘，要求的响应时间短 适用场景 批处理系统适合处理大批量数据、实时性要求不高的场景，应用数据量大volume 流计算系统适合处理快速产生的数据、实时性要求高的场景，但可能处理的数据量不大，应对数据产生速度快velocity Spark VS Storm 批处理与流计算的区别：Spark最终会完成计算并结束运行，而Storm会持续处理消息，运行的结果时时改变 Spark的RDD不知道schema，而Storm的tuple因为确定了topology是知道schema的 ZooKeeper的使用 Spark使用ZooKeeper选主，但也可以不用 Storm一定需要ZooKeeper，负责Nimbus和Supervisor之间的所有协调工作 消息传递机制 MapReduce和Spark的shuffle阶段存在阻塞，数据传输是成块进行的 Storm一次一记录，Tuple处理完即可发送，无阻塞，符合实时计算要求 Storm VS Spark Streaming Storm is continous executor，而Spark Streaming是将continous data转化为了batch进行操作 Flink VS Spark v2 Runtime engineer Flink 引擎处理 Streaming Dataflow，natively support streaming processing Spark v2 引擎处理 Batch Dataflow，强制将streaming转换为了batches 架构 相同之处：批流融合都采用了流向表转换的思路，Flink将流转变为Dynamic Table，Spark v2将流转变为Unbounded Table MapReduce vs Giraph 使用MapReduce等分布式数据处理系统进行图计算，编写程序复杂，且丢失了图结构信息和特性，大量迭代计算麻烦，没有将图看成是图，只知道迭代的过程，而非图计算，看待KMeans和PageRank是相同的 MapReduce系统没有针对迭代进行优化，每次迭代都要落盘，增加了I/O开销，虽然Spark/Flink没有这个开销 MapReduce每次都会更新所有的节点，造成资源的浪费，导致的原因：key-value转换之后将图看成了普通的k-v键值对，没有图信息，也就没有办法优化；Spark同样无法识别图，RDD全部参与运算；Flink可以做增量迭代 Giraph实现了迭代优化、增量迭代，并且其编程方便 Pregel将PageRank处理对象看成连通图，而MapReduce则将其看成是键值对 Pregel将计算细化到顶点，可在顶点内控制循环迭代次数，而MapReduce则将计算批量化处理，按作业进行循环迭代控制，一系列的MapReduce调用需要反复读写HDFS Giraph vs MapReduce Hama Giraph利用了MapReduce的框架来启动task，但是不是基于MapReduce API计算，即其没有使用Map/Reduce两个操作算子 MapReduce Hama利用了MapReduce的API实现计算，就是在MapReduce上实现了一套图算法程序 体系结构的异同 MapReduce Spark Storm Flink 应用名称 Job Job/DAG Topology Job JobTracker Master Nimbus JobManager 系统角色 TaskTracker Worker Supervisor TaskManager Task进程 Executor Worker Task线程 TaskSet（Task线程） Executor（Task线程） 组件接口 Map/Reduce RDD API Spout/Bolt Task 进程 VS 线程 线程好，更加轻量级，减少任务的启动开销 多核 => 多线程，充分利用硬件资源 工作流程的异同 数据传输方式 MapReduce、Spark批处理：阻塞式传递数据，数据从下往上传递 Storm流处理：火山式模型（open - next - close），一次一记录 Flink批流融合：火山式模型，从上往下拉取数据，但是一次一个vector，即多个tuple 引擎 Blocking Pipelined MapReduce Yes No Spark Stage之间 Stage内部 Storm No Yes Flink Pipeline Breaker Yes 迭代处理方式 MapReduce：由用户程序显式while\\for编写迭代，很多个不同的job，每次迭代结束写在HDFS上，容错方式和普通的算子一致 Spark：由用户程序显式while\\for编写迭代，相较于MapReduce改进，每一次迭代的中间结果存在内存。容错方式和普通的算子一致 Flink：迭代是系统内置的函数，系统可以优化迭代的过程，因此也需要系统来负责容错 容错机制 非迭代算子：批处理、流计算的容错机制一样 迭代算子：两者语义不同，批处理迭代没有完成前没有结果输出，而流计算每次迭代均有结果输出，因此容错机制有所差别 "}}