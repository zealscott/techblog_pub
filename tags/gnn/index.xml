<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>gnn on ZealScott</title>
    <link>https://tech.zealscott.com/tags/gnn/</link>
    <description>Recent content in gnn on ZealScott</description>
    <image>
      <url>https://tech.zealscott.com/papermod-cover.png</url>
      <link>https://tech.zealscott.com/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 19 Feb 2019 15:34:10 +0800</lastBuildDate><atom:link href="https://tech.zealscott.com/tags/gnn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>图卷积神经网络入门基础</title>
      <link>https://tech.zealscott.com/deeplearning/models/gnn/</link>
      <pubDate>Tue, 19 Feb 2019 15:34:10 +0800</pubDate>
      
      <guid>https://tech.zealscott.com/deeplearning/models/gnn/</guid>
      <description>卷积 定义 卷积是一种数学运算，称$(f*g)(n)$为$f,g$的卷积，
其连续的定义为：
$$(f*g)(n) = \int_{-\infty}^{+\infty} f(\tau)g(n-\tau)d\tau$$
离散的定义为：
$$(f*g)(n) = \sum\limits_{\tau = -\infty}^\infty f(\tau)g(n-\tau)$$
若令$x = \tau,y = n-\tau$，则$x+y = n$表示的是平行的直线。
对于图像来说，图像上的滑动窗口很好的解释了卷积的定义：
可以发现，我们对$f,g$进行卷积操作，保证$x,y$坐标的和都为1：
写成卷积公式为：
$$(f*g)(1,1) = \sum\limits_{k=0}^{2}\sum\limits_{h=0}^{2}f(h,k)g(1-h,1-k)$$
这样就实现了使用$g$这个算子在图像上的滑动。但注意，在数学中的卷积运算中，卷积核与原始的矩阵乘积，是围绕着中心元素进行180度旋转后，才是对应的元素。
而在实际的图像空间滤波中，我们是将设计的特定卷积核，然后将其与像素矩阵的对应元素（不进行上述的旋转）相乘得到。例如，在CV中常见的平滑滤波，高斯滤波。这些滤波被设计出来，以提取不同的特征。
..对于神经网络来讲，最大的不同是，这些滤波不需要我们自己去定义（也就是提取特征的过程），而是通过网络自身训练每一个卷积层的滤波器..。让这些滤波器组对特定的模式有高的激活，以达到CNN网络的分类/检测等目的。因此，在CNN中，由于这些卷积核都是未知参数，需要根据数据训练学习，那么翻不翻转已经没有关系了。
理解 对于离散卷积来说，本质上就是一种加权求和。CNN中的卷积本质上就是利用一个共享参数的过滤器（kernel），通过计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征的提取，当然加权系数就是卷积核的权重系数。
那么卷积核的系数如何确定的呢？是随机化初值，然后根据误差函数通过反向传播梯度下降进行迭代优化。这是一个关键点，卷积核的参数通过优化求出才能实现特征提取的作用，GCN的理论很大一部分工作就是为了引入可以优化的卷积参数。
Laplacian matrix 我们上离散数学都学过，图拉普拉斯矩阵的定义为：
$$L = D -W$$
其中，$D$ 是顶点的度矩阵（对角矩阵），$W$是图的邻接矩阵（带边权重）。其normalized形式为：
$$L^{nor} = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$$
但为什么是这样定义呢？我们先从拉普拉斯算子说起。
Laplacian 其数学定义为：
$$\Delta = \sum\limits_i \frac{\delta^2}{\delta x_i^2}$$
即为非混合二阶偏导数的和。
图像中的拉普拉斯算子 图像是一种离散数据，那么其拉普拉斯算子必然要进行离散化。
从导数定义：
$$f&#39;(x) = \frac{\delta f(x)}{\delta x} \approx f(x+1) - f(x)$$
因此可以得到二阶导为：
$$f&#39;&#39;(x) = \frac{\delta^2 f(x)}{\delta x^2} \approx f&#39;(x) - f&#39;(x-1) \approx f(x+1) + f(x-1) - 2f(x)$$</description>
    </item>
    
  </channel>
</rss>
